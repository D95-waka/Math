\input{../article_base.tex}
\title{תורת ההסתברות 1 --- סיכום}
\setcounter{secnumdepth}{2}
% chktex-file 9
% chktex-file 17

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}

\begin{document}
\maketitle
\maketitleprint{}

\tableofcontents

\section{שיעור 1 --- 29.10.2024}

\subsection{מבוא הקורס}
% אורי גורביץ' הוא המרצה, הקורס הוא מבוא להסתברות. הוא חצי חירש תתמודד עם זה במקרה הצורך.
נלמד לפי ספר שעוד לא יצא לאור שנכתב על־ידי אורי עצמו, הוא עוד לא סופי ויש בו בעיות ואי־דיוקים, תשיג את הספר הזה.
כן יש הבדל בין הקורס והספר אז לא לסמוך על הסדר שלו גם כשאתה משיג אותו, אבל זו תוספת מאוד נוחה.
יש סימון של כוכביות לחומר מוסף, כדאי לעבור עליו לקראת המבחן כי זה יתן לנו עוד אינטואיציה והעמקה של ההבנה.

נשים לב כי ענף ההסתברות הוא ענף חדש יחסית, שהתפתח הרבה אחרי שאר הענפים הקלאסיים של המתמטיקה, למעשה רק לפני 400 שנה נשאלה על־ידי נזיר במהלך חקר של משחק אקראי השאלה הראשית של העולם הזה, מה ההסתברות של הצלחה במשחק.

נעבור לדבר על פילוסופיה של ההסתברות.
מה המשמעות של הטלת מטבע מבחינת הסתברות?
ישנה הגישה של השכיחות, שמציגה הסתברות כתוצאה במקרה של חזרה על ניסוי כמות גדולה מאוד של פעמים.
יש כמה בעיות בזה, לרבות חוסר היכולת להגדיר במדויק אמירה כזו, הטיות שנובעות מפיזיקה, מטבעות הם לא מאוזנים לדוגמה.
הבעיה הראשית היא שלא לכל בעיה אפשר לפנות בצורה כזאת.
ישנה גישה נוספת, היא הגישה האוביקטיבית או המתמטית, הגישה הזו בעצם היא תרגום בעיה מהמציאות לבעיה מתמטית פורמלית.
לדוגמה נשאל את השאלה מה ההסתברות לקבל 6 בהגרלה של כל המספרים מ־1 עד מיליון.
השיטה ההסתברותית קובעת שאם אני רוצה להוכיח קיום של איזשהו אוביקט, לפעמים אפשר לעשות את זה על־ידי הגרלה של אוביקט כזה והוכחה שיש הסתברות חיובית שהוא יוגרל, וזו הוכחה שהוא קיים.
מה התחזיות שינבעו מתורת ההסתברות? לדוגמה אי־אפשר לחזות הטלת מטבע בודדת, אבל היא כן נותנת הבנה כללית של הטלת 1000 מטבעות, הסתברויות קטנות מספיק יכולות להיות זניחות ובמקרה זה נוכל להתעלם מהן.
לפחות בתחילת הקורס נדבר על תרגום של בעיות מהמציאות לבעיות מתמטיות, זה אומנם חלק פחות ריגורזי, אבל הוא כן חשוב ליצירת קישור בין המציאות לבין החומר הנלמד.

דבר אחרון, ישנה השאלה הפילוסופית של האם באמת יש הסתברות שכן לא בטוח שיש אקראיות בטבע, הגישה לנושא מבחינה פיזיקלית קצת השתנתה בעת האחרונה וקשה לענות על השאלה הזאת.
יש לנו תורות פיזיקליות שהן הסתברותיות בעיקרן, כמו תורת הקוונטים, תורה זו לא סתם הסתברותית, אנחנו לא מנסים לפתור בעיות הסתברותיות אלא ממש משתמשים במודלים סטטיסטיים כדי לתאר מצב בעולם.
לדוגמה נוכל להסיק ככה מסקנה פשוטה שאם מיכל גז נפתח בחדר, יהיה ערבוב של הגז הפנימי ושל אוויר החדר, זוהי מסקנה הסתברותית.
החלק המדהים הוא שתורת הקוונטים מניחה חוסר דטרמניזם כתכונה יסודית ועד כמה שאפשר לראות יש ניסויים שמוכיחים שבאמת יש חוסר ודאות בטבע.
דהינו שברמה העקרונית הפשוטה באמת אין תוצאה ודאית בכלל למצבים כאלה במציאות.

\subsection{מרחבי מדגם ופונקציית הסתברות}
\begin{definition}[מרחב מדגם]
	מרחב מדגם הוא קבוצה לא ריקה שמהווה העולם להסתברות. \\*
	נסמנה $\Omega$.
	איבר במרחב המדגם נסמן ב־$\omega \in \Omega$ על־פי רוב.
\end{definition}
נוכל להגיד שמרחב במדגם הוא הקבוצה של האיברים שעליה אנחנו שואלים בכלל שאלות, זהו הייצוג של האיברים או המצבים שמעניינים אותנו.
בהתאם נראה עכשיו מספר דוגמות שמקשרות בין אובייקטים שאנו דנים בהם בהסתברות ובהגדרה פורמלית של מרחבי מדגם עבורם.
\begin{example}[מרחבי הסתברות שונים]
	נראה מספר דוגמות למצבים כאלה:
	\begin{itemize}
		\item הטלת מטבע תוגדר על־ידי $\Omega = \{ H, T \}$.
		\item הטלת שלושה מטבעות תהיה באופן דומה $\Omega = {\{H, T\}}^3$.
		\item הטלת קוביה היא $\Omega = [6] = \{ 1, \dots, 6 \}$.
		\item הטלת מטבע ואז אם יוצא עץ (H) אז מטילים קוביה ואם יוצא פלי (T) אז מטילים קוביה עם 8 פאות. \\*
			במקרה זה נסמן $\Omega = \{ H1, H2, H3, \dots, H6, T1, \dots, T8 \} = \{H, T \} \times \{1, \dots, 8 \}$ כאשר הכוונה פה היא לזוג סדור $\langle H, 1 \rangle$.
		\item ערבוב חפיסת קלפים, במקרה זה מרחב המדגם שלנו יהיה סימון של הקלפים כרשימה מספרית בלבד, דהינו $\Omega = S_{52}$. \\*
			נוכל גם לסמן במקום את $\Omega = {\{1, \dots, 52 \}}^{52}$, זהו סימון זהה.
	\end{itemize}
\end{example}
בדוגמה זו קל במיוחד לראות שכל איבר בקבוצה מתאר מצב סופי כלשהו, ואנו יכולים לשאול שאלות הסתברותיות מהצורה מה הסיכוי שנקבל $\omega$ מסוים מתוך $\Omega$, זאת ללא התחשבות בבעיה שממנה אנו מגיעים.
נבחן עתה גם דוגמות למקרים שבהם אין לנו מספר סופי של אפשרויות, למעשה מקרים אלה דומים מאוד למקרים שראינו עד כה.
\begin{example}[מרחבי מדגם לא סופיים]
	מטילים מטבע עד שיוצא $H$, אז מרחב המדגם הוא $\Omega = \NN \cup \{ \infty \}$. \\*
	באופן דומה נוכל לבחון מדידת זמן התפרקות חלקיק, היא $\Omega = \RR_+ \cup \{ \infty \}$.
\end{example}
\begin{definition}[פונקציית הסתברות נקודתית]
	יהי מרחב מדגם $\Omega$ ותהי $p : \Omega \to [0, \infty)$ פונקציה כך שמתקיים
	\[
		\sum_{\omega \in \Omega} p(\omega) = 1
	\]
	אז פונקציה זו נקראת \textbf{פונקציית הסתברות}.
\end{definition}
למעשה פונקציית הסתברות היא מה שאנחנו נזהה עם הסתברות במובן הפשוט, פונקציה זו מגדירה לנו לכל סיטואציה ממרחב המדגם מה הסיכוי שנגיע אליה, כך לדוגמה אם נאמר שהטלת מטבע תגיע בחצי מהמקרים לעץ ובחצי השני לפלי,
אז זו היא פונקציית ההסתברות עצמה, פונקציה שמחזירה חצי עבור עץ וחצי עבור פלי, נראה מספר דוגמות.
\begin{example}[פונקציית הסתברות להטלת מטבע]
	נגדיר $\Omega = \{ H, T \}$ ויהי $0 \le \alpha \le 1$, נגדיר $p(H) = \alpha, p(T) = 1 - \alpha$.
\end{example}
\begin{example}[פונקציית הסתברות אינסופית]
	נגדיר $\Omega = \NN \cup \{ \infty \}$ ו־$p(\omega) = \begin{cases}
		2^{-\omega} & \omega \in \NN \\
		0 & \omega = \infty
	\end{cases}$.
	בדוגמה זו נקבל $\sum_{n = 1}^{\infty} 2^{-n} = 1$ ולכן זו אכן פונקציית הסתברות.
\end{example}
נבחין כי הדוגמה האחרונה מתארת לנו התפלגות של דעיכה, זאת אומרת שלדוגמה אם קיים חלקיק עם זמן מחצית חיים של יחידה אחת, פונקציית הסתברות זו תניב לנו את הסיכוי שהוא התפרק לאחר כמות יחידות זמן כלשהי.
\begin{example}
	נגדיר $\Omega = \NN$ ו־$p(\omega) = \frac{1}{\omega(\omega + 1)}$, נבחין כי אכן $\sum_{n = 1}^{\infty} \frac{1}{n(n + 1)} = 1$.
\end{example}
\begin{definition}[תומך]
	התומך של $p$ הוא $\supp(p) = \{ \omega \in \Omega \mid p(\omega) > 0 \}$. \\*
	נבחין כי התומך הוא למעשה קבוצת האיברים שאפשרי לקבל לפי פונקציית ההסתברות, כל שאר המצבים מקבלים 0, משמעו הוא שאין אפשרות להגיע אליו.
\end{definition}
\begin{remark}
	נבחין כי תמיד $\mathcal{F} \subseteq \mathcal{P}(\Omega)$.
\end{remark}
\begin{definition}[מאורע]
	מאורע הוא תת־קבוצה של מרחב המדגם, קבוצת כל המאורעות תסומן $\mathcal{F}$.
	עבור מאורע $A$ המאורע המשלים מסומן ב־$A^C = \Omega \setminus A$.
\end{definition}
\begin{definition}[פונקציית הסתברות]
	נגדיר עתה פונקציית הסתברות שאיננה נקודתית.
	יהי מרחב מדגם $\Omega$ וקבוצת מאורעות $\mathcal{F}$. \\*
	תהי $\PP : \mathcal{F} \to [0, \infty)$ המקיימת את התכונות הבאות:
	\begin{enumerate}
		\item $\PP(\Omega) = 1$
		\item לכל ${\{A_i\}}_{i = 1}^\infty \subseteq \mathcal{F}$ סדרת מאורעות שונים מתקיים
			\[
				\sum_{i \in \NN} \PP(A_i) = \PP(\bigcup_{i \in \NN} A_i)
			\]
			דהינו, הפונקציה סכימה בתת־קבוצות בנות מניה.
	\end{enumerate}
	לפונקציה כזו נקרא \textbf{פונקציית ההסתברות} על $(\Omega, \mathcal{F})$.
\end{definition}
\begin{proposition}
	תהי $p$ פונקציית הסתברות נקודתית על $\Omega$ אז נגדיר פונקציית הסתברות $\PP_p$ על־ידי
	\[
		\PP_p(A) = \sum_{\omega \in A} p(\omega)
	\]
	אז $\PP_p$ היא פונקציית הסתברות.
\end{proposition}
\begin{proof}
	נוכיח ששתי התכונות של פונקציית הסתברות מתקיימות.
	\[
		\PP_p(A) = \sum_{\omega \in A} p(\omega) \ge 0
	\]
	שכן זהו סכום אי־שלילי מהגדרת $p$,
	בנוסף נקבל מההגדרה של $p$ כי
	\[
		\PP_p(\Omega) = \sum_{\omega \in \Omega} p(\omega) = 1
	\]
	וקיבלנו כי התכונה הראשונה מתקיימת. \\*
	תהי ${\{A\}}_{i = 1}^\infty \in \mathcal{F}$, אז נקבל
	\[
		\sum_{i \in \NN} \PP_p(A_i) = \sum_{i \in \NN} \left( \sum_{\omega \in A_i} p(\omega) \right) = \sum_{\omega \in \bigcup_{i \in \NN} A_i} p(\omega) = \PP_p(\bigcup_{i \in \NN} A_i)
	\]
	ולכן גם התכונה השנייה מתקיימת וקיבלנו כי $\PP_p$ היא אכן פונקציית הסתברות.
\end{proof}
נשים לב כי בעוד פונקציית הסתברות נקודתית מאפשרת לנו לדון בהסתברות של איבר בודד בקבוצות בנות מניה, פונקציית הסתברות למעשה מאפשרת לנו לדון בהסתברות של מאורעות, הם קבוצות של כמה מצבים אפשריים, ובכך להגדיל את מושא הדיון שלנו.
מהטענה האחרונה גם נוכל להסיק שבין שתי ההגדרות קיים קשר הדוק, שכן פונקציית הסתברות נקודתית גוררת את קיומה של פונקציית הסתברות כללית.

\section{תרגול 1 --- 31.10.2024}
המתרגל הוא אמיר, amir.behar@mail.huji.ac.il

\subsection{מרחבי הסתברות סופיים ובני־מניה}
ניזכר בהגדרה למרחב הסתברות, המטרה של הגדרה זו היא לתאר תוצאות אפשריות של מצב נתון.
\begin{definition}[מרחב הסתברות]
	מרחב הסתברות הוא קבוצה $(\Omega, \mathcal{F}, \PP)$ כאשר $\PP : \mathcal{F} \to [0, 1]$, כך שמתקיים
	\begin{enumerate}
		\item חיוביות: $\forall A \in \mathcal{F}, \PP(A) \ge 0$
		\item נרמול: $\PP(\Omega) = 1$
		\item סיגמא־אדיטיביות: $\forall {\{ A_i \}}_{i = 1}^\infty \in \mathcal{F}, (\forall i, j \in \NN, i \ne j \implies A_i \cap A_j = \emptyset) \implies \sum_{i \in I} \PP(A_i) = \PP(\bigcup_{i \in I} A_i )$
	\end{enumerate}
\end{definition}
\begin{exercise}
	יהי $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, $A, B \in \mathcal{F}$, הוכיחו
	\[
		\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)
	\]
\end{exercise}
\begin{proof}
	נבחין כי $\PP(A) = \PP(A - (A \cap B)) + \PP(A \cap B)$ וגם $\PP(B) = \PP(B - (A \cap B)) + \PP(A \cap B)$. נוכל אם כן לסכום ולקבל
	\[
		\PP(A) + \PP(B) = \PP(A - (A \cap B)) + \PP(A \cap B) + \PP(B - (A \cap B)) + \PP(A \cap B)
		= \PP(A \cup B) + \PP(A \cap B)
	\]
	נבחין כי השוויון האחרון נובע מהזרות של קבוצות אלה.
\end{proof}
לאורך פרק זה נגדיר מעתה שמתקיים $\Omega$ סופית, $\mathcal{F} = 2^\Omega$ ואף נגדיר כי ההסתברות אחידה, דהינו $\forall A \enspace \PP(A) = \frac{|A|}{|\Omega|}$, זה כמובן שקול לטענה
\[
	\forall \omega, \omega' \in \Omega, \PP(\{\omega\}) = \PP(\{\omega'\})
\]
\begin{exercise}
	מטילים קוביה הוגנת, מה ההסתברות שיצא מספר זוגי?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = [6] = \{1, \dots, 6\}$, עם $\PP$ אחידה. \\*
	נרצה לחשב את $A = \{2, 4, 6\}$ ולכן נקבל $\PP(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = \frac{1}{2}$.
\end{solution}
\begin{exercise}
	מטילים מטבע הוגן שלוש פעמים, מה ההסתברות שיצא עץ בדיוק פעמיים, ומה ההסתברות שיצא עץ לפחות פעמיים?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = \{ TTT, TTP, TPT, PTT, \dots \}$. \\*
	עבור המקרה הראשון נגדיר $A = \{ TTP, TPT, PTT \}$, ולכן נקבל שההסתברות היא $\PP(A) = \frac{3}{8}$. \\*
	במקרה השני נקבל $B = A \cup \{ TTT \}$ ולכן $P(B) = \frac{1}{2}$.
\end{solution}
\begin{exercise}
	מטילים קוביה הוגנת $n$ פעמים.
	\begin{enumerate}
		\item מה ההסתברות שתוצאת ההטלה הראשונה קטנה מ־4?
		\item מה ההסתברות שתוצאת ההטלה הראשונה קטנה שווה מתוצאת ההטלה השנייה?
		\item מה ההסתברות שיצא 1 לפחות פעם אחת?
	\end{enumerate}
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {[6]}^n = \{ (x_1, \dots, x_n) \mid x_i \in [6] \}$.
	\begin{enumerate}
		\item נגדיר $A = \{ (x_1, \dots, x_n) \in \Omega \mid x_1 < 4 \}$ ולכן $\PP(A) = \frac{3 \cdot 6^{n - 1}}{6^n} = \frac{1}{2}$.
		\item נגדיר $B = \{ (x_1, \dots, x_n) \in \Omega \mid x_1 \le x_2 \} = \bigcup_{i = 1}^6 \{ (x_1, i, x_3, \dots, x_n) \in \Omega \mid x_i \le i \}$, ולכן נקבל
			\[
				\PP(B) = \sum \PP(B_i) = \sum \frac{i \cdot 6^{n - 2}}{6^n} = \frac{\sum_{i = 1}^{6} i}{6^2} = \frac{6 \cdot 7}{6^2 \cdot 2} = \frac{7}{12}
			\]
		\item הפעם $C = \{ (x_1, \dots, x_n) \in \Omega \mid \exists i, x_i = 1 \}$, בהתאם $C^C = \{ (x_1, \dots, x_n) \in \Omega \mid \forall i, x_1 \ne 1 \}$. \\*
			לכן נקבל $\PP(C^C) = \frac{5^n}{6^n} \implies \PP(C) = 1 - \frac{5^n}{6^n}$.
	\end{enumerate}
\end{solution}
\begin{exercise}
	חמישה אנשים בריאים וחמישה אנשים חולי שפעת עומדים בשורה. מה ההסתברות שחולי השפעת נמצאים משמאל לאנשים הבריאים?
\end{exercise}
\begin{solution}
	נגדיר $\Omega$ ככל הסידורים של $0, 1$ כשיש חמישה מכל סוג.
	לכן נקבל $|\Omega| = \binom{10}{5}$, שכן $\Omega = \{ X \subset [10] \mid |X| = 5 \}$. \\*
	המאורע הפעם הוא $A = \{ \{ 1, 2, 3, 4, 5 \} \}$ ובהתאם $\PP(A) = \frac{5! 5!}{10!}$.

	נוכל גם להגדיר $\Omega = S_{10}$ כאשר חמשת המספרים הראשונים מייצגים בריאים וחמשת האחרונים מייצגים חולים. \\*
	במקרה זה נקבל $A = \{ \pi \in \Omega \mid \pi(\{1, 2, 3, 4, 5\}) \subseteq \{1, 2, 3, 4, 5\} \}$ ולכן $|A| = 5! 5! $ וכך נקבל $\PP(A) = \frac{5! 5!}{10!}$.
\end{solution}

\section{שיעור 2 --- 31.10.2024}
\subsection{השלמה לטורים דו־מימדיים}
נגדיר הגדרה שדרושה לצורך ההרצאה הקודמת כדי להיות מסוגלים לדון בסכומים אינסופיים בני־מניה.
\begin{definition}[סכום קבוצת בת־מניה]
	אם ${\{a_i\}}_{i \in I}$ ו־$a_i \ge 0$ לכל $i \in I$ אז נגדיר
	\[
		\sum_{i \in I} a_i = \sup \left\{ \sum_{i \in J} \mid J \subseteq I, J \text{ is finite} \right\}
	\]
\end{definition}

\subsection{תכונות של פונקציות הסתברות}
נעבור עתה לבחון פונקציות הסתברות ואת תכונותיהן, נתחיל מתרגיל שיוצק תוכן לתומך של פונקציית הסתברות:
\begin{exercise}
	הוכיחו כי אם $\sum_{i \in I} a_i < \infty$ ו־$a_i \ge 0$ לכל $i \in I$ אז $|\{ i \in I \mid a_i < 0 \}| \le \aleph_0$.
	במילים אחרות הוכיחו כי התומך של $a$ הוא בן־מניה.
\end{exercise}
בשיעור הקודם ראינו את ההגדרה והטענה הבאות:
\begin{definition}[פונקציית הסתברות מתאימה לנקודתית]
	בהינתן פונקציית הסתברות נקודתית $p$ נגדיר
	\[
		\PP_p(A) = \sum_{\omega \in A} p(\omega)
	\]
\end{definition}
\begin{proposition}
	$\PP_p$ היא פונקציית הסתברות.
\end{proposition}
טענה זו בעצם יוצרת קשר בין פונקציות הסתברות לפונקציות הסתברות נקודתיות, ומאפשרת לנו לחקור את פונקציות ההסתברות לעומק באופן פשוט הרבה יותר. נשתמש עתה בכלי זה.
\begin{definition}[מרחב הסתברות בדיד]
	אם $\PP$ פונקציית הסתברות כך שקיימת פונקציית הסתברות נקודתית $p$ כך ש$\PP = \PP_p$, אז נאמר ש־$\PP$ היא בדידה ו־$(\Omega, \mathcal{F}, \PP)$ \textbf{מרחב הסתברות בדיד}.
\end{definition}
\begin{proposition}
	יש פונקציות הסתברות שאינן בדידות.
	בפרט, עבור מדגם ההסתברות $\Omega = [0, 1]$ קיימת פונקציית הסתברות $\PP$ המקיימת
	\[
		\forall a, b \in \RR, 0 \le a \le b \le 1 \implies \PP([a, b]) = b - a
	\]
\end{proposition}
\begin{example}
	ידוע כי $\sum_{n \in \NN} \frac{1}{n^2} = \frac{\pi^2}{6} < \infty$ ולכן נוכל להגדיר $\Omega = \NN$ ו־$p(n) = \frac{1}{\frac{\pi^2}{6} n^2}$, הגדרה זו תניב ש־$\sum_{n \in \NN} p(n) = 1$ ולכן זו פונקציית הסתברות.
	נחשב את $\PP_p(A)$ עבור $A = 2\NN$:
	\[
		\PP_p(A) = \sum_{n \in A} p(n) = \sum_{k \in \NN} p(2k) = \frac{1}{\frac{\pi^2}{6} {(2k)}^2} = \frac{6}{\pi^2} \frac{1}{4} \sum_{k \in \NN} \frac{1}{k^2} = \frac{1}{4}
	\]
	נסביר, הגדרנו פונקציית הסתברות של דעיכה, דהינו שככל שהמספר שאנו מבקשים גדול יותר כך הוא פחות סביר באופן מעריכי (לדוגמה זמן מחצית חיים), ואז שאלנו כמה סביר המאורע שבו נקבל מספר זוגי.
\end{example}
\begin{theorem}[תכונות פונקציית הסתברות]
	$\PP$ פונקציית הסתברות על $(\Omega, \mathcal{F})$, אז
	\begin{enumerate}
		\item $\PP(\emptyset) = 0$
		\item אם $I$ קבוצה סופית ו־${\{A_i\}}_{i \in I}$ מאורעות זרים בזוגות, אז $\PP(\bigcup_{i \in I} A_i) = \sum_{i \in I} \PP(A_i)$
		\item אם $A \subseteq B$ מאורעות אז $\PP(A) \le \PP(B)$
		\item $\PP(A) \le 1$ לכל מאורע $A$
		\item לכל מאורע $A$ מתקיים $\PP(A^C) = 1 - \PP(A)$
	\end{enumerate}
\end{theorem}
\begin{proof}
	נוכיח את התכונות
	\begin{enumerate}
		\item נראה כי $\PP(\emptyset) = \sum_{i = 1}^\infty \PP(\emptyset)$ שכן כל איחוד של קבוצות ריקות הוא זר, לכן אילו $\PP(\emptyset) \ne 0$ נקבל ישר סתירה, נסיק כי $\PP(\emptyset) = 0$ בלבד.
		\item נגדיר $A_i = \emptyset$ לכל $i > n$ ונשתמש בסיגמא־אדיטיביות ונקבל
			\[
				\PP(\bigcup_{i \in I} A_i)
				= \PP(\bigcup_{i \in \NN} A_i)
				= \sum_{i \in \NN} \PP(A_i)
				= \sum_{i \in I} \PP(A_i)
			\]
		\item נשתמש בתכונה 2 על $B, B \setminus A$, אלו הן קבוצות זרות כמובן, אם נגדיר $D = A \cup (B \setminus A)$ נקבל $\PP(D) = \PP(A) + \PP(B \setminus A) \ge \PP(A)$.
		\item נובע ישירות מתכונה 3 ומ־$A \subseteq \Omega$.
		\item ניזכר כי $A^C = \Omega \setminus A$ ולכן $\Omega = A \cup A^C$ ונקבל $1 = \PP(\Omega) = \PP(A) + \PP(A^C)$.
	\end{enumerate}
\end{proof}
נעבור עתה לאפיון של פונקציות הסתברות בדידות, נבין מתי הן כאלה ומתי לא.
\begin{theorem}[תנאים שקולים לפונקציית הסתברות בדידה]
	אם $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, התנאים הבאים שקולים:
	\begin{enumerate}
		\item $\PP$ היא פונקציית הסתברות בדידה
		\item $\PP$ נתמכת על קבוצות בנות־מניה, כלומר קיימת קבוצה $A \in \mathcal{F}$ בת־מניה כך ש־$\PP(A) = 1$
		\item $\sum_{\omega \in \Omega} \PP(\{\omega\}) = 1$
		\item לכל מאורע $A \in \mathcal{F}$ מתקיים $\PP(A) = \sum_{\omega \in A} \PP(\{\omega\})$
	\end{enumerate}
\end{theorem}
\begin{proof}
	$1 \implies 2$:
	נניח ש־$\PP = \PP_p$ עבור $p : \Omega \to [0, \infty)$ פונקציית הסתברות נקודתית.
	נסתכל על $\supp(p) = \{ \omega \in \Omega \mid p(\omega) > 0 \}$, לפי הגדרת הסכום והתרגיל נובע ש־$A = \supp(p)$ בת־מניה.
	נקבל
	\[
		\PP(A) = \sum_{\omega \in A} p(\omega) = \sum_{\omega \in \Omega} p(\omega) = \PP(\Omega) = 1
	\]

	$2 \implies 4$:
	נניח ש־$\PP(S) = 1$ עבור $S$ בת־מניה. לכן $\PP(S^C) = 0$.
	נראה כי $A$ הוא איחוד זר $A = (A \cap S) \cup (A \cap S^C)$ ולכן נקבל
	\[
		\PP(A) = \PP(A \cap S) + \PP(A \cap S^C) = \PP(A \cap S) + 0 = \sum_{\omega \in A \cap S} \PP(\{\omega\}) = \sum_{\omega \in A} \PP(\{\omega\})
	\]

	$4 \implies 3$:
	אם נבחר $A = \Omega$ נקבל את טענה 3.

	$3 \implies 1$:
	נגדיר $p : \Omega \to [0, \infty)$ על־ידי $p(\omega) = \PP(\{ \omega \})$, נקבל $\sum_{\omega \in \Omega} p(\omega) = 1$ ולכן $p$ היא פונקציית הסתברות נקודתית.
	מהתרגיל והגדרת הסכום נובע ש־$S = \supp(p)$ היא בת־מניה ומתקיים $\PP(S^C) = 0$, אז לכל $A \in \mathcal{F}$ מתקיים
	\[
		\PP(A) = \PP(A \cap S) + \PP(A \cap S^C) = \PP(A \cap S) = \sum_{\omega \in A \cap S} \PP(\{ \omega \}) = \sum_{\omega \in A} \PP(\{\omega\}) = \sum_{\omega \in A} p(\omega) = \PP_p(A)
	\]
\end{proof}

\subsection{פרדוקס יום ההולדת}
פרדוקס יום ההולדת הוא פרדוקס מוכר הגורס כי גם בקבוצות קטנות יחסית של אנשים, הסיכוי שלשני אנשים שונים יהיה תאריך יום הולדת זהה הוא גבוה במידה משונה.
הפרדוקס נקרא כך שכן לכאורה אין קשר בין מספר הימים בשנה לבין הסיכוי הכל־כך גבוה שמצב זה יקרה, נבחן עתה את הפרדוקס בהיבט הסתברותי.

נניח שכל תאריכי יום ההולדת הם סבירים באותה מידה ונבחן את הפרדוקס.
נגדיר $\Omega = {[365]}^k$ עבור $k$ מספר האנשים בקבוצה נתונה כלשהי.
$p(\omega) = \frac{1}{{365}^k}$ לכל $\omega \in \Omega$.
נקבל $\PP(A) = \PP_p(A) = \frac{|A|}{365^k}$.
נרצה לחשב את $A$ כמאורע שיש לפחות שני אנשים שיש להם יום הולדת באותו יום, דהינו שיש שני ערכים זהים ברשימת המספרים, נגדיר $A = \{ \omega \in \Omega \mid \exists 1 \le i \ne j \le k, \omega_i = \omega_j \}$.
בשל המורכבות נבחן את המשלים $A^C$, נקבל $|A^C| = 365 \cdot 364 \cdots (365 - (k - 1)) = \frac{365!}{(365 - (i - 1))!}$.
נציב ונחשב:
\[
	\PP(A^C) = \frac{|A^C|}{365^k} = \prod_{i = 1}^k \frac{365 - (i - 1)}{365} = \prod_{i = 1}^k (1 - \frac{i - 1}{365})
\]
מהנוסחה שקיבלנו נראה שמההצבה $k = 23$ נקבל שההסתברות היא בערך $\frac{1}{2}$, דהינו בקבוצה של 23 אנשים יש סבירות של חצי שלפחות שניים יחגגו יום הולדת באתו יום.

\section{שיעור 3 --- 5.11.2024}

\subsection{מכפלת מרחבי הסתברות בדידים}
ניזכר תחילה במרחבי הסתברות אחידים
\begin{definition}[מרחב הסתברות אחיד]
	מרחב הסתברות אחיד הוא $(\Omega, \mathcal{F}, \PP_p)$ המקיים $p(\omega_1) = p(\omega_2)$ לכל $\omega_1, \omega_2 \in \Omega$.
\end{definition}
\begin{conclusion}
	$\PP_p(A) = \frac{|A|}{|\Omega|}$
\end{conclusion}
נבחין כי במקרים מסוימים ההסתברות שלנו מורכבת משני מאורעות בלתי תלויים, במקרים אלה נרצה להגדיר מכפלה של מרחבי ההסתברות.
\begin{definition}[מרחב מכפלת הסתברויות]
	אם $(\Omega_1, \mathcal{F}_1, \PP_{p_1})$ ו־$(\Omega_2, \mathcal{F}_2, \PP_{p_2})$ מרחבי הסתברות בדידים
	נגדיר $q : \Omega_1 \times \Omega_2 \to [0, \infty)$ על־ידי $q(\omega_1, \omega_2) = p(\omega_1) \cdot p(\omega_2)$.
\end{definition}
\begin{proposition}
	$q$ פונקציית הסתברות נקודתית.
\end{proposition}
\begin{proof}
	נשתמש ישירות בהגדרה ונחשב
	\[
		\sum_{(\omega_1, \omega_2) \in \Omega_1 \times \Omega_2} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in \Omega_1, \omega_2 \in \Omega_2} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in \Omega_1} \left( \sum_{\omega_2 \in \Omega_2} p_1(\omega_1) p_2(\omega_2) \right)
		= \sum_{\omega_1 \in \Omega_1} p_1(\omega_1)
		= 1
	\]
\end{proof}
עתה כשהוכחנו טענה זו, יש לנו הצדקה אמיתית להגדיר את $(\Omega_1 \times \Omega_2, \mathcal{F}_{1,2}, \PP_q)$ כמרחב הסתברות, ונקרא לו מרחב מכפלה.
\begin{proposition}
	אם $(\Omega_1, \mathcal{F}_1, \PP_{p_1})$ ו־$(\Omega_2, \mathcal{F}_2, \PP_{p_2})$ מרחבי הסתברות אחידים,
	אז מרחב המכפלה $(\Omega_1 \times \Omega_2, \mathcal{F}_{1,2}, \PP_q)$ אחיד אף הוא.
\end{proposition}
\begin{proof}
	\[
		q(\omega_1, \omega_2) = p_1(\omega_1) p_2(\omega_2)
		= \frac{1}{|\Omega_1|} \cdot \frac{1}{|\Omega_2|}
		= \frac{1}{|\Omega_1 \times \Omega_2|}
	\]
\end{proof}
\begin{definition}[מאורע שוליים ומאורע מכפלה]
	במרחב מכפלה המאורעות מהצורה $A \times \Omega_2$ או $\Omega_1 \times A$ נקראים שוליים. \\*
	מאורע מהצורה $A \times B$ נקרא מאורע מכפלה.
\end{definition}
\begin{proposition}
	במרחב מכפלה $\PP_q(A \times B) = \PP_{p_1}(A) \cdot \PP_{p_2}(B)$.
	בפרט $\PP_q(A \times \Omega_2) = \PP_{p_1}(A)$.
\end{proposition}
\begin{proof}
	\[
		\sum_{(\omega_1, \omega_2) \in A \times B} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in A, \omega_2 \in B} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in A} \left( \sum_{\omega_2 \in B} p_1(\omega_1) p_2(\omega_2) \right)
		= \sum_{\omega_1 \in A} p_1(\omega_1) \PP_{p_2}(B)
		= \PP_{p_1}(A) \PP_{p_2}(B)
	\]
\end{proof}
\begin{example}
	בהינתן $n$ הטלות מטבע כלשהו, מה ההסתברות שיצאו $k$ עצים?

	עבור ההטלה הראשונה, $\Omega_1 = \{0, 1\}$. עוד נגדיר $p(1) = \alpha, p(0) = 1 - \alpha$ עבור $0 \le \alpha \le 1$ כלשהו. \\*
	בהתאם נקבל $\Omega = {\{0, 1\}}^n$, וכן
	\[
		q(\omega_1, \dots, \omega_n) = \prod_{i = 1}^n p(\omega_i)
		= \prod_{i = 1}^n \alpha^{\omega_i} \cdot {(1 - \alpha)}^{1 - {\omega_i}}
		= \alpha^{\sum_{i = 1}^n \omega_i} {(1 - \alpha)}^{n - \sum_{i = 1}^n \omega_i}
	\]

	נבחין כי היינו יכולים לתאר את המקרה הזה ממש על־ידי $q(\omega) = \alpha^\omega \cdot {(1 - \alpha)}^{1 - \omega}$.

	נעבור עתה לבחינת המאורע
	\[
		A = \{ (\omega_1, \dots, \omega_n) \in \Omega \mid \sum_{i = 1}^{n} \omega_i = k \}
	\]
	נקבל מהביטוי שמצאנו כי
	\[
		\PP_q(A)
		= \sum_{(\omega_1, \dots, \omega_n) \in A} q(\omega_1, \dots, \omega_n)
		\sum_{\sum_{i = 1}^n \omega_i = k} \alpha^{\sum_{i = 1}^n \omega_i} {(1 - \alpha)}^{n - \sum_{i = 1}^n \omega_i}
		= |A| \alpha^k {(1 - \alpha)}^{n - k}
		= \binom{n}{k} \alpha^k {(1 - \alpha)}^{n - k}
	\]
\end{example}
\begin{example}
	נבחן עתה את המקרה של הטלות הוגנות ובחינת המקרה שחצי מההטלות לפחות יצאו עץ,
	זאת־אומרת שנבחן את הדוגמה הקודמת כאשר $n = 2m, k = m$, ו־$\alpha = \frac{1}{2}$.
	מנוסחת סטרלינג שאנחנו לא מכירים $m! \simeq \sqrt{2\pi m} {(\frac{m}{e})}^m$ ואז נוכל להסיק
	\[
		\PP_q(A)
		= \binom{2m}{m} \frac{1}{2^m}
		\simeq \frac{\sqrt{4\pi m} {(\frac{2m}{e})}^{2m}}{{(\sqrt{2\pi m} {(\frac{k}{e})}^m)}^2 2^{2m}}
		= \frac{\sqrt{4\pi m}}{2\pi m}
		= \frac{1}{\sqrt{\pi m}}
	\]
\end{example}

\subsection{ניסויים דו־שלביים}
נניח $(\Omega_1, \mathcal{F}_1, \PP_{p_1})$ מרחב הסתברות בדידה עבור הניסוי הראשון, ונניח שיש מרחב הסתברות בדידה עבור הניסוי השני כך שלכל תוצאה בניסוי הראשון, פונקציית ההסתברות תשתנה בהתאם בניסוי השני.
לכל $\omega_1 \in \Omega_1$ יש פונקציית הסתברות נקודתית $p_{\omega_1} : \Omega_2 \to [0, \infty)$.
נגדיר את מרחב הניסוי הדו־שלבי $(\Omega_1 \times \Omega_2, \mathcal{F}_{1, 2}, \PP_q)$,
כאשר $q(\omega_1, \omega_2) = p_1(\omega_1) \cdot p_{\omega_1}(\omega_2)$.
\begin{proposition}
	$\PP_q$ פונקציית הסתברות.
\end{proposition}
\begin{proof}
	\[
		\sum_{(\omega_1, \omega_2) \in \Omega_1 \times \Omega_2} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in \Omega_1} \left( \sum_{\omega_2 \in \Omega_2} p_1(\omega_1) p_{\omega_1}(\omega_2) \right)
		= \sum_{\omega_1 \in \Omega_1} p_1(\omega_1) \left( \sum_{\omega_2 \in \Omega_2} p_{\omega_1}(\omega_2) \right)
		= \sum_{\omega_1 \in \Omega_1} p_1(\omega_1)
		= 1
	\]
\end{proof}
\begin{example}
	$\Omega_1 = \{H, T\}$ ו־$\Omega_2 = \{1, \dots, 8\}$, נגדיר $p_1(H) = p_1(T) = \frac{1}{2}$.
	עוד נגדיר
	\[
		p_H(\omega_2) = \begin{cases}
			\frac{1}{6} & 1 \le \omega_2 \le 6 \\
			0 & \text{else}
		\end{cases},
		\qquad
		p_T(\omega_2) = \frac{1}{8}
	\]
	מהגדרה זו נקבל
	\[
		q(\omega_1, \omega_2) = \begin{cases}
			\frac{1}{12} & \omega_1 = H, \omega_2 \in [6] \\
			0 & \omega_1 = H, \omega_2 \in \{7, 8\} \\
			\frac{1}{16} & \omega_1 = T, \omega_2 \in [8]
		\end{cases}
	\]
\end{example}
\begin{theorem}[חסם האיחוד]
	אם $A, B$ מאורעות אז $\PP(A \cup B) \le \PP(A) + \PP(B)$.
\end{theorem}
\begin{proof}
	\[
		\PP(A \cup B)
		= \PP(A \uplus (B \setminus A))
		= \PP(A) + \PP(B \setminus A)
		\le \PP(A) + \PP(B)
	\]
\end{proof}
נוכל להשתמש בחסם האיחוד כדי להוכיח גרסה כללית יותר של המשפט:
\begin{theorem}[אי־שוויון בול]
	אם $A_1, \dots, A_k$ מאורעות, אז $\PP(\bigcup_{i = 1}^k A_i) \le \sum_{i = 1}^k \PP(A_i)$.
\end{theorem}
\begin{example}
	נחזור לבחון את פרדוקס יום ההולדת, הפעם נבחן גרסה כללית יותר של הרעיון.
	נגדיר $\Omega = {[m]}^k$ עם הסתברות אחידה.
	נגדיר גם $A = \{ \omega \in \Omega \mid \exists 1 \le i < j \le k, \omega_i = \omega_j \}$.
	אנו רוצים את ההסתברות $\PP(A) = \frac{|A|}{|\Omega|}$, אז נבחן את המשלים
	\[
		A^C = \{ \omega \in \Omega \mid \forall 1 \le i, j \le k, i \ne j \implies \omega_i \ne \omega_j \}
	\]
	נחשב
	\[
		|A^C| = m (m - 1) \cdots (m - (k - 1))
	\]
	בהתאם
	\[
		\PP(A^C)
		= \frac{\prod_{i = 0}^{k - 1} (m - i)}{m^k}
		= \prod_{i = 0}^{k - 1} \frac{m - i}{m^k}
		= \prod_{i = 0}^{k - 1} (1 - \frac{i}{m})
	\]
	נזכור ש־$\forall x \in \RR, 1 + x \le e^x$, ונוכל לקבל
	\[
		\prod_{i = 0}^{k - 1} (1 - \frac{i}{m})
		\le \prod_{i = 0}^{k - 1} e^{-\frac{i}{m}}
		= \exp(- \frac{1}{m} \sum_{i = 0}^{k - 1} i)
		= e^{- \frac{k(k - 1)}{2m}}
	\]
	כאשר $k$ גדול ביחס ל־$\sqrt{2m}$ מקבלים חסם קרוב ל־$0$.

נגדיר הפעם $A = \bigcup_{\substack{i \ne j \\ i, j \in [k]}} A_{ij}$ עבור $A_{ij} = \{ \omega \in \Omega \mid \omega_i = \omega_j \}$.
	וגם
	\[
		i \ne j \implies \PP(A_{ij}) = \frac{|A_{ij}|}{m^k} = \frac{m \cdot m^{k - 2}}{m^k} = \frac{1}{m}
	\]
	ועתה
	\[
		\PP(A)
		\le \sum_{\substack{i \ne j \\ i, j \in [k]}} \PP(A_{ij})
		= \sum_{\substack{i \ne j \\ i, j \in [k]}} \frac{1}{m}
		= \binom{k}{2} \frac{1}{m}
		= \frac{k (k - 1)}{2m}
	\]
	לכן אם $k$ קטן ביחס ל־$\sqrt{2m}$ אז ההסתברות ליום־הולדת משותף קטנה.
\end{example}

\section{תרגול 2 --- 7.11.2024}

\subsection{פתרון שאלות הסתברותיות}
נתחיל בבחינת טענה שימושית לביצוע חישובי הסתברות:
\begin{proposition}[נוסחת ההסתברות השלמה]
	יהי $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, $\mathcal{A}$ חלוקה בת־מניה של $\Omega$, לכל $B \in \mathcal{F}$ מתקיים
	\[
		\PP(B) = \sum_{A \in \mathcal{A}} \PP(A \cap B)
	\]
	נניח שיש מרחב הסתברות ויש חלוקה בת מניה של המרחב, אז לכל מאורע ההסתברות שלו היא הסכום על החלוקה על החיתוך של החלוקה ו־$A$.
\end{proposition}
\begin{proof}
	נשים לב כי $B = \biguplus{A \in \mathcal{A}} B \cap A$ איחוד זר, ולכן הטענה נובעת מסיגמא־אדיטיביות.
\end{proof}
\begin{exercise}
	קוביה מוטה בעלת 6 פאות עם הסתברות נקודתית $p(i) = \frac{i}{21}$ מוטלת 5 פעמים. \\*
	מה ההסתברות שתוצאת ההטלה הראשונה התקבלה פעם אחת ויחידה?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {[6]}^5$ ונגדיר $\PP(x_1, \dots, x_5) = p(x_1) \cdots p(x_5)$.
	אנו רוצים לחשב את
	\[
		B = \{ (x_1, \dots, x_5) \in \Omega \mid \forall j \ne 1, x_j \ne x_1 \}
	\]
	נגדיר חלוקה $\mathcal{A} = \{ A_1, \dots, A_6 \}$ של $\Omega$ כך ש־$A_i = \{ (i, x_2, \dots, x_5) \in \Omega \mid 1 \le x_j \le 6 \}$. \\*
	נקבל
	\[
		\PP(B \cap A_i) = \frac{i}{21} \cdot {(1 - \frac{i}{21})}^4
	\]
	על־ידי שימוש בנוסחת ההסתברות השלמה נקבל
	\[
		\PP(B) = \sum_{i = 1}^{6} \PP(B \cap A_i) = \sum_{i = 1}^{6} \frac{i}{21} {(1 - \frac{i}{21})}^4
	\]
\end{solution}
נראה עתה דוגמה לשימוש בחסם האיחוד בן־המניה, אותו נראה בהרצאה הבאה
\begin{proposition}[חסם האיחוד הבן־מניה]
	אם $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות ו־${\{A_i\}}_{i = 1}^\infty \subseteq \mathcal{F}$ אז מתקיים
	\[
		\PP(\bigcup_{i = 1}^\infty A_i) \le \sum_{i \in \NN} \PP(A_i)
	\]
\end{proposition}
\begin{exercise}
	משלשלים $k$ פתקי הצבעה בין $n$ קלפיות. \\*
	מה ההסתברות שאין קלפי עם יותר מפתק אחד?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = \{ (x_1, \dots, x_n) \mid 0 \le x_i, x_1 + \cdots + x_n = k \}$.
	נחשב ונקבל $|\Omega| = \binom{n + k - 1}{k - 1}$. \\*
	נגדיר את המאורע, $A = \{ (x_1, \dots, x_n) \in \Omega \mid x_i \le 1 \}$. \\*
	ננסה לחסום את המשלים,
	\[
		\Omega \setminus A = \{ (x_1, \dots, x_n) \in \Omega \mid \exists i, x_i \ge 2 \}
	\]
	אם נגדיר $A_i = \{ (x_1, \dots, x_n) \in \Omega \mid x_i \ge 2 \}$ אז נוכל להגדיר
	\[
		\Omega \setminus A = \bigcup_{i \in [n]} A_i
	\]
	נחשב את ההסתברות של כל $A_i$, מתקבל $|A_i| = \binom{n + k - 3}{k - 3}$ מהשיקול של סכימת הפתרונות השלמים תוך התעלמות משני פתקים. \\*
	לכן
	\[
		\PP(A_i) = \frac{|A_i|}{|\Omega|} = \frac{\binom{n + k - 3}{k - 3}}{\binom{n + k - 1}{k - 1}} = \frac{k(k - 1)}{(k + n - 1)(k + n - 2)}
	\]
	מחסם האיחוד נובע
	\[
		\PP(\Omega - A) \le \sum_{i = 1}^{n} \frac{k(k - 1)}{(k + n - 1)(k + n - 2)} = n \cdot \frac{k(k  -1)}{(n + k - 1)(n + k - 2)}
	\]
	ועל־ידי מעבר למשלים שוב נוכל להסיק $\PP(A) \ge 1 - n \cdot \frac{k(k  -1)}{(n + k - 1)(n + k - 2)}$. \\*
	נזכור כי אנו מנסים להבין את המגמה כאשר המספרים מאוד גדולים, לכן נבחן את המקרה ש־$n \to \infty$, אז נובע $\PP(A) \xrightarrow[n \to \infty]{} 1$, \\*
	דהינו כאשר יש כמות קלפיות הולכת וגדלה הסיכוי שיהיה פתק יחיד בכל אחת (מספר הפתקים לא משתנה) הולך וגדל ומתקרב לסיכוי מלא.
\end{solution}
נראה עתה דוגמה לשימוש במרחבי ניסוי דו־שלביים:
\begin{exercise}
	מה ההסתברות שנגריל מספר $m$ בין $1$ ל־$n$,
	ואז נגריל עוד מספר והוא יהיה בין $1$ לבין $m$?
\end{exercise}
\begin{solution}
	נבנה פונקציית הסתברות עבור הניסוי השני, נניח שבניסוי השני קיבלנו $m$:
	\[
		p_m(k) = \begin{cases}
			\frac{1}{m} & k \le m \\
			0 & k > m
		\end{cases},
		\qquad
		q(m, k) = \begin{cases}
			\frac{1}{mn} & k \le m \\
			0 & k > m
		\end{cases}
	\]
	נגדיר $A_k$ המאורע שתוצאת ההגרלה השניה היא $k$, לכן
	\[
		\PP(A_k)
		= \PP(\{(m, k) \in \Omega \mid m \le k \})
		= \sum_{m = 1}^{n} q(m, k)
		= \sum_{m = k}^{n} \frac{1}{mn}
	\]
	נבחין כי המעבר האחרון אכן תקין, שכן קיבענו את המשתנה השני, זאת אומרת שעכשיו במקום להסתכל על מספר שיותר קטן ממספר אחר, אנו בוחנים את המספר החוסם מלמעלה, המספר הגדול יותר. \\*
	לדוגמה
	\[
		\PP(A_n) = \frac{1}{n^2},
		\qquad
		\PP(A_1) = \sum_{m = 1}^{n} \frac{1}{mn} = \frac{1}{n} \sum_{m = 1}^n \frac{1}{m} \approx \frac{\log n}{n}
	\]
\end{solution}
נבחן דוגמה ספציפית כהמשך של השאלה הזו, הפעם נגדיר $m = n / 2$:
\begin{example}
	נגדיר $B_{n / 2}$ להיות המאורע בהתחלה השניה ו־$B$ שבהגרלה השניה יצא מספר גדול מ־$n / 2$
	\[
		\PP(B_{n / 2}) = \PP(\bigcup_{k \ge n / 2}^n A_k)
		= \frac{1}{n} \sum_{k \ge \frac{n}{2}}^n \sum_{m = k}^n \frac{1}{m}
		= \frac{1}{n} \sum_{m = \lceil \frac{n}{2}\rceil}^{n} \frac{\frac{n}{2} + 1 - n + m}{m}
	\]
	כמו בשאלה הקודמת, גם הפעם נרצה להבין מגמה כללית, ולכן נבדוק את הביטוי כאשר $n$ שואף לאינסוף, דהינו שהמספרים שאפשר להגדיל הולכים וגדלים בכמותם:
	\[
		\lim_{n \to \infty} \PP(B_{n / 2})
		= \lim_{n \to \infty} \frac{1}{n} \sum_{m = \lceil \frac{n}{2}\rceil}^{n} \frac{1 + m - \frac{n}{2}}{m}
	\]
	נבחין כי $\sum_{m = 1}^n \frac{1}{m} = \log(n) + e + o(\frac{1}{m})$ ולכן
	\begin{align*}
		\lim_{n \to \infty} \frac{1}{n} \sum_{m = \lceil \frac{n}{2}\rceil}^{n} \frac{1 + m - \frac{n}{2}}{m}
		& = \lim_{n \to \infty} \frac{1}{2} + \frac{n}{2n} (\log(n) - \log(\frac{n}{2}) + o(\frac{1}{n})) + \frac{1}{n} (\log(n) - \log(\frac{n}{2}) + o(\frac{1}{n})) \\
		& = \frac{1}{2} + \frac{1}{n} \log 2
	\end{align*}
\end{example}

\section{שיעור 4 --- 7.11.2024}
בשיעור הקודם דיברנו על מרחבי מכפלה וניסויים דו־שלביים.
ברור לנו כי על־ידי שרשור דומה לתהליך של ניסוי דו־שלבי נוכל לבנות ניסוי רב־שלבי.
עוד דיברנו על חסם האיחוד, הטענה כי $\PP(\bigcup_{i = 1}^n A_i) = \sum_{i = 1}^n \PP(A_i)$.
השימוש של חסם האיחוד מאפשר לנו לפשט חישובים שבהם אנחנו רוצים הבנה כללית של ההתנהגות של מרחב ההסתברות.

\subsection{חסמי איחוד ורציפות}
\begin{definition}[סדרת מאורעות עולה]
	סדרת מאורעות ${\{A_n\}}_{n = 1}^\infty$ נקראת עולה אם $A_n \subseteq A_{n + 1}$ לכל $n \in \NN$.
\end{definition}
\begin{notation}
	נסמן $A_\infty = \bigcup_{n \in \NN} A_n$.
\end{notation}
\begin{theorem}[משפט רציפות פונקציית ההסתברות]\label{probability_continuous_probability_function_theorem}
	אם ${\{A_n\}}_{n = 1}^\infty$ סדרת מאורעות עולה אז
	\[
		\PP(A_\infty) = \lim_{n \to \infty} \PP(A_n)
	\]
\end{theorem}
המשפט נקרא כך בשל ההקבלה שלו לקונספט של רציפות בפונקציות רגילות, עבור $f : \RR \to \RR$ היא רציפה ב־$a$ אם ורק אם לכל סדרה $x_n \to a$ מתקיים $f(x_n) \to f(a)$.
\begin{proof}
	נגדיר $B_n = A_n \setminus A_{n - 1}$ כאשר $B_1 = A_1 \setminus \emptyset = A_1$. \\*
	נראה כי מתקיים $\biguplus_{n = 1}^m B_n = A_m$ איחוד זר: \\*
	$\bigcup_{n = 1}^m B_n = A_m$ כי לכל $\omega \in A_m$ יש $n$ מינימלי כך ש־$\omega \in A_n$, אבל $\omega \notin A_{n - 1}$, לכן נוכל להסיק כי $\omega \in A_n \setminus A_{n - 1} = B_n$. \\*
	אם $\omega \in B_n = A_n \setminus A_{n - 1}$ אז $\omega \notin A_{n - 1}$ ולכן $\omega \notin A_k$ לכל $k < n$.
	מסיגמא־אדיטיביות נסיק
	\[
		\sum_{n = 1}^{m} \PP(B_n) = \PP(A_m)
	\]
	וגם
	\[
		\sum_{n = 1}^{\infty} \PP(B_n)
		= \PP(\biguplus_{n = 1}^\infty B_n)
		= \PP(\bigcup_{m = 1}^\infty \left(\biguplus_{n = 1}^\infty B_n\right))
		= \PP(\bigcup_{m = 1}^\infty A_m)
	\]
	מצד שני מהגדרת הגבול
	\[
		\sum_{n = 1}^{\infty} \PP(B_n)
		= \lim_{m \to \infty} \sum_{n = 1}^{m}  \PP(B_n)
		= \lim_{m \to \infty} \PP(A_m)
	\]
\end{proof}
\begin{definition}[סדרת מאורעות יורדת]
	נגדיר סדרת מאורעות ${\{A_n\}}_{n = 1}^\infty$ כך שמתקיים $A_{n + 1} \subseteq A_n$ לכל $n \in \NN$.
\end{definition}
נוכל להסיק מהעובדה שמשלים של סדרה עולה הוא סדרה יורדת ונקבל
\begin{proposition}
	\[
		\PP(\bigcap_{n \in \NN} A_n) = \lim_{n \to \infty} \PP(A_n)
	\]
\end{proposition}
\begin{proposition}[חסם האיחוד הבן־מניה]
	אם ${\{A_n\}}_{n = 1}^\infty$ סדרת מאורעות אז מתקיים
	\[
		\PP(\bigcup_{n = 1}^\infty A_n) \le \sum_{n \in \NN} \PP(A_n)
	\]
\end{proposition}
\begin{proof}
	נגדיר $B_m = \bigcup_{n = 1}^m A_n$, זוהי סדרה עולה ולכן
	\[
		\PP(\bigcup_{n = 1}^\infty A_n)
		= \PP(\bigcup_{m = 1}^\infty B_m)
		= \lim_{m \to \infty} \PP(B_m)
		\le \lim_{m \to \infty} \sum_{n = 1}^{m} \PP(A_n)
		= \sum_{n = 1}^{\infty} \PP(A_n)
	\]
\end{proof}

\subsection{עיקרון ההכלה וההדחה}
\begin{proposition}
	אם $A, B$ מאורעות אז
	\[
		\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)
	\]
\end{proposition}
\begin{proof}
	נגדיר $C = A \setminus B, D = A \cap B, E = B \setminus A$, נקבל
	\[
		A = C \uplus D,
		\quad
		B = D \uplus E,
		\quad
		A \cup B = C \uplus D \uplus E
	\]
	ונקבל
	\[
		\PP(A) = \PP(C) + \PP(D),
		\quad
		\PP(D \cup B) = \PP(D) + \PP(E)
	\]
	ולכן
	\[
		\PP(A \cup B) = \PP(C) + \PP(D) + \PP(E)
	\]
\end{proof}
\begin{theorem}[הכלה והפרדה לשלושה מאורעות]
	עבור שלושה מאורעות $A, B, C$:
	\[
		\PP(A \cup B \cup C) = \PP(A) + \PP(B) + \PP(C) - (\PP(A \cap B) + \PP(A \cap C) + \PP(B \cap C)) + \PP(A \cap B \cap C)
	\]
\end{theorem}
\begin{theorem}[הכלה והפרדה ל־n מאורעות]
	יהיו $A_1, \dots, A_n$ מאורעות, אז
	\[
		\PP(\bigcup_{i = 1}^n A_i)
		= \sum_{i = 1}^{n} \PP(A_i) - \sum_{i = 1}^{n} \sum_{j = 1}^{i - 1} \PP(A_i \cap A_j) + \sum_{i = 1}^{n} \sum_{j = 1}^{i - 1} \sum_{k = 1}^{j - 1} \PP(A_i \cap A_j \cap A_k) + \dots
	\]
	אם נגדיר $A_I = \bigcap_{i \in I} A_i$ לכל $I \subseteq [n]$ אז נקבל
	\[
		\PP(\bigcup_{n = 1}^n A_i)
		= \sum_{k = 1}^{n} {(-1)}^{k + 1} \sum_{\substack{I \subseteq [i] \\ |I| = k}} \PP(A_I)
		= \sum_{\emptyset \ne I \subseteq [n]} {(-1)}^{|I| + 1} \PP(A_I)
	\]
\end{theorem}
את משפט זה נוכיח בהמשך הקורס. \\*
נראה דוגמה לבעיה קלאסית במקרים אלה.
\begin{exercise}[בעיית ההתאמה]
	מחלקים $n$ מעטפות ל־$n$ תיבות דואר, אחת לכל תיבה, מה ההסתברות שאף מכתב לא הגיע ליעדו?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = S_n$ מרחב אחיד.
	$A = \{ \omega \in \Omega \mid \forall i, \omega(i) \ne i \}$. \\*
	נבחן את המשלים, $A^C = \{ \omega \in \Omega \mid \exists i, \omega(i) = i \} = \bigcup_{i = 1}^n A_i$ עבור $A_i = \{ \omega \in \Omega \mid \omega(i) = i \}$.
	נחשב
	\[
		\PP(A_i) = \frac{|A_i|}{|\Omega|} = \frac{(n - 1)!}{n!} = \frac{1}{n}
	\]
	במקרה של חיתוך $\PP(A_i \cap A_j)$ עבור $j < i$ נקבל
	\[
		\PP(A_i \cap A_j) = \frac{|A_i \cap A_j|}{|\Omega|} = \frac{(n - 2)!}{n!} = \frac{1}{n(n - 1)}
	\]
	נוכל להמשיך את התהליך הזה, ונקבל
	\[
		\PP(A_I) = \frac{|\bigcap_{i \in I} A_i|}{|\Omega|}
		= \frac{(n - |I|)!}{n!}
		= \frac{1}{n (n - 1) (n - 2) \cdots (n - (I + 1))}
	\]
	כעת נותר להשתמש בנוסחה להכלה והדחה, ונקבל
	\[
		\PP(\bigcup_{i = 1}^n A_i)
		= \sum_{k = 1}^{n} {(-1)}^{k + 1} \sum_{\substack{I \subseteq [n] \\ |I| = k}} \frac{(n - k)!}{n!}
		= \sum_{k = 1}^{n} {(-1)}^{k + 1} \binom{n}{k} \frac{(n - k)!}{n!}
		= \sum_{k = 1}^{n} \frac{{(-1)}^{k + 1}}{k!}
	\]
	נשים לב כי רצינו לחשב את המשלים למאורע, לכן
	\[
		\PP(A)
		= 1 - \PP(\bigcup_{i = 1}^n A_i)
		= 1 + \sum_{k = 1}^{n} \frac{{(-1)}^k}{k!}
		= \sum_{k = 0}^{n} \frac{{(-1)}^k}{k!}
		\xrightarrow[n \to \infty]{} e^{-1}
	\]
	נקבל שאוסף התמורות ללא נקודת שבת הוא
	\[
		|A^n| = n! \sum_{l = 0}^{n} \frac{{(-1)}^l}{l!}
	\]
	נגדיר קבוצה חדשה
	\[
		D_k = \{ \omega \in S_n \mid \exists i, \omega(i) = i \}
		= \biguplus_{\substack{I \subseteq [n] \\ |I| = k }} D_I
	\]
	ונבחין כי
	\[
		D_I = \{ \omega \in S_n \mid \forall i \in I, \omega(i) = i, \forall i \notin I, \omega(i) \ne i \}
	\]
	ולכן
	\begin{align*}
		\PP(D_k)
		& = \sum_{\substack{I \subseteq [n] \\ |I| = k }} \PP(D_I) \\
		& = \sum_{\substack{I \subseteq [n] \\ |I| = k }} \frac{|D_I|}{n!} \\
		& = \sum_{\substack{I \subseteq [n] \\ |I| = k }} \frac{(n - k)! \sum_{l = 0}^n \frac{{(-1)}^l}{l!}}{n!} \\
		& = \binom{n}{k} \frac{(n - k)!}{n!} \sum_{l = 0}^n \frac{{(-1)}^l}{l!} \\
		& = \frac{1}{k!} \sum_{l = 0}^n \frac{{(-1)}^l}{l!} \\
		& \xrightarrow[n \to \infty]{} \frac{e^{-1}}{k!}
	\end{align*}
\end{solution}

\section{שיעור 5 --- 12.11.2024}

\subsection{הסתברות מותנית}
\begin{definition}[הסתברות מותנית]
	$A, B$ מאורעות, ההסתברות המותנית של $A$ בהינתן $B$ תוגדר להיות
	\[
		\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
	\]
\end{definition}
\begin{example}
	אם מטילים שתי קוביות מאוזנות, מה ההסתברות שיצא 3 בקוביה הראשונה בהינתן שהסכום הוא 8?

	נגדיר כמובן $\Omega = {[6]}^2$, וכן נגדיר $A = \{ (3, i) \in \omega \mid 1 \le i \le 6 \}$ וכן $B = \{ (2, 6), (3, 5), (4, 4), (5, 3), (6, 2) \}$.
	\[
		\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
		= \frac{\frac{1}{36}}{\frac{5}{36}}
		= \frac{1}{5}
	\]
\end{example}
\begin{proposition}
	נקבע מאורע $B$ עם הסתברות $\PP(B) > 0$, נגדיר $\PP_B(A) = \PP(A \mid B)$, דהינו $\PP_B : \mathcal{F} \to [0, \infty)$. \\*
	אז $\PP_B$ היא פונקציית הסתברות.
\end{proposition}
\begin{proof}
	$\PP_B(A)$ היא אי־שלילית. \\*
	נראה גם
	\[
		\PP_B(\Omega) = \frac{\PP(\Omega \cap B)}{\PP(B)} = 1
	\]
	ולבסוף
	\[
		\PP_B(\biguplus_{i \in I} A_i)
		= \frac{(\PP_B(\biguplus_{i \in I} A_i )) \cap B}{\PP(B)}
		= \frac{\PP_B(\biguplus_{i \in I} A_i \cap B)}{\PP(B)}
		= \sum_{i \in I} \frac{\PP(A_i \cap B)}{\PP(B)}
		= \sum_{i \in I} \PP_B(A_i)
	\]
\end{proof}
\begin{proposition}
	יהיו $C, B$ מאורעות המקיימים $\PP(B \cap C) > 0$, נסמן $\PP' = \PP_B$ ו־$\PP'' = \PP_C'$. \\*
	אז לכל מאורע $A$ מתקיים $\PP''(A) = \PP(A \mid B \cap C)$ או בחילוף סימונים $\PP'' = \PP_{B \cap C}$.
\end{proposition}
\begin{proof}
	\[
		\PP''(A)
		= \PP_C'(A)
		= \frac{\PP'(A \cap C)}{\PP'(C)}
		= \frac{\PP_B(A \cap C)}{\PP_B(C)}
		= \frac{\frac{\PP(B \cap (A \cap C))}{\PP(B)}}{\frac{\PP(B \cap C)}{\PP(B)}}
		= \PP_{B \cap C}(A)
	\]
\end{proof}
מצאנו כי התניה חוזרת היא אסוציאטיבית ולכן נוכל לדבר על הסתברות מותנית בכמה מאורעות ללא התייחסות לסדר שלהם, למעשה התנייה מותנית היא קומוטטיבית כפי שאפשר לראות בהוכחה.
\begin{conclusion}[נוסחת ההסתברות השלמה בהסתברות מותנית]
	נניח ש־${\{A_i\}}_{i \in \NN}$ חלוקה בת־מניה של $\Omega$ ו־$B$ מאורע כלשהו, אז
	\[
		\PP(B) = \sum_{i \in \NN} \PP(A_i) \PP(B \mid A_i)
	\]
\end{conclusion}
\begin{proof}
	\[
		\PP(A_i) \PP(B \mid A_i) = \PP(A_i) \frac{\PP(B \cap A_i)}{\PP(A_i)} = \PP(B \cap A_i)
	\]
	ולכן
	\[
		\biguplus_{i \in \NN} (B \cap A_i) = B
		\implies \PP(B) = \sum_{i \in \NN} \PP(B \cap A_i)
	\]
\end{proof}
\begin{lemma}[כלל בייס]
	אם $A, B$ מאורעות עם הסתברות חיובית אז
	\[
		\PP_A(B) = \frac{\PP(B)}{\PP(A)} \PP_B(A)
	\]
\end{lemma}
\begin{proof}
	ישירות מהגדרה נסיק
	\[
		\PP_A(B)
		= \frac{\PP(A \cap B)}{\PP(A)}
		= \frac{\PP(B)}{\PP(A)} \cdot \frac{\PP(A \cap B)}{\PP(B)}
		= \frac{\PP(B)}{\PP(A)} \PP_B(A)
	\]
\end{proof}
\begin{conclusion}[כלל השרשרת]
	\[
		\PP(A \cap B) = \PP(A) \PP(B \mid A)
	\]
\end{conclusion}
\begin{exercise}
	מטילים מטבע הוגן. אם יוצא עץ נוסעים לתל־אביב ואם יוצא פלי אז ונסעים לחיפה.
	כשנוסעים לתל־אביב יש הסתברות של אחוז אחד לפנצ'ר, ובנסיעה לחיפה יש הסתברות של 2 אחוז לפנצ'ר. \\*
	מה ההסתברות לפנצ'ר ומה ההסתברות שנסעו לתל־אביב?
\end{exercise}
\begin{solution}
	נגדיר $A$ הוא עץ או לנסוע לתל־אביב ו־$B$ ההסתרות שיהיה פנצ'ר, בהתאם
	\[
		\PP(A^C) = \PP(A) = \frac{1}{2},
		\qquad
		\PP(B \mid A) = 0.01,
		\PP(B \mid A^C) = 0.02
	\]
	בהתאם
	\[
		\PP(B) = \PP(A) \PP(B \mid A) + \PP(A^C) + \PP(B \mid A^C)
		= \frac{1}{2} 0.01 + \frac{1}{2} 0.02 = 0.015
	\]
	באשר לשאלה השנייה נקבל
	\[
		\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)} = \frac{\PP(A)}{\PP(B)} \PP(B \mid A) = \frac{\frac{1}{2}}{0.015} \cdot 0.01 = \frac{1}{3}
	\]
\end{solution}
נבחין כי התוצאה יצאה מאוד אלגנטית כתוצאה מהמטבע ההוגן, אילו הוא היה לא הוגן היינו מקבלים חישוב שונה במקצת, אך תקף באותה המידה.
\begin{example}[מונטי הול]
	יש שלוש דלתות, בוחרים אחת, מנחה פותח דלת שלא נבחרה ומאחוריה אין כלום, מה שאומר שמאחורי אחת הדלתות הסגורות יש אוצר ובאחרות יש עז.
	המנחה מציע לכם להחליף את הדלת שבחרתם.

	קשה למדל את הבעיה הזו, שכן חסר תיאור והגדרה, אז נאמר שהגרלנו מספר ב־$[3]$, נניח שבחרנו $1$, נניח שהמנחה גם במכוון תמיד בוחר דלת ריקה.
	נוסיף את ההנחה שאם האוצר מאחורי דלת 1 אז המנחה פותח את 2 או 3, וההסתברויות שוות.
	
	נעבור להגדרה, $A_i$ המאורע שהאוצר ב־$i$ ו־$B_i$ היא שהמנחה פותח את דלת $i$.
	מההנחות שלנו נובע $\PP(B_3 \mid A_2) = 1, \PP(B_2 \mid A_3) = 1, \PP(B_3 \mid A_1) = \PP(B_2 \mid A_1) = \frac{1}{2}$.
	לבסוף נניח כי $\PP(A_i) = \frac{1}{3}$ לכל שלוש הדלתות. \\*
	נרצה לחשב את $\PP(A_1 \mid B_2)$:
	\[
		\PP(A_1 \mid B_2)
		= \frac{\PP(A_1)}{\PP(B_2)} \cdot \PP(B_2 \mid A_1)
		= \frac{\frac{1}{6}}{\PP(B_2)}
	\]
	וגם
	\[
		\PP(B_2) = \PP(A_1) \PP(B_2 \mid A_1) + \PP(A_2) \PP(B_2 \mid A_2) + \PP(A_3) \PP(B_2 \mid A_3) = \frac{1}{3} \cdot \frac{1}{2} + \frac{1}{3} \cdot 0 + \frac{1}{3} \cdot 1 = \frac{1}{2}
	\]
\end{example}

\section{תרגול 3 --- 14.11.2024}

\subsection{הסתברות מותנית}
\begin{exercise}
	מטילים זוג קוביות הוגנות ושונות. נתון שסכום תוצאותיהן גדול מעשר,
	מה ההסתברות שבהטלה השנייה יצא 6?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {[6]}^2$ עם $\PP$ אחידה. \\*
	עוד נגדיר $A = \{ (x, y) \in \Omega \mid x + y > 10 \}$ וכן $B = \{ (x, 6) \in \Omega \}$, לכן
	\[
		\PP(B \mid A)
		= \frac{\PP(A \cap B)}{\PP(A)}
		= \frac{\frac{|A \cap B|}{|\Omega|}}{\frac{|A|}{|\Omega|}}
		= \frac{|A \cap B|}{|A|} = \frac{2}{3}
	\]
\end{solution}
\begin{exercise}
	אדם מחפש מכתב, זכור לו במעורפל בהסתברות $0 \le p \le 1$ שהניח אותו באחת ממגירות שולחן העבודה. \\*
	בשולחן $n$ מגירות והאדם חיפש ב־$k$ המגירות הראשונות ולא מצא את המכתב. \\*
	מה ההסתברות שהמכתב בשולחן?
\end{exercise}
\begin{solution}
	נגדיר $A$ להיות המאורע שהמכתב בשולחן ו־$B_k$ המכתב לא באף אחת מ־$k$ המגירות הראשונות. אנו מחפשים את $\PP(A \mid B_k)$. \\*
	לכן
	\[
		\PP(A \mid B_k)
		= \frac{\PP(A \cap B_k)}{\PP(B_k)}
	\]
	עוד אנו יודעים כי
	\[
		\PP(A) = p,
		\PP(B_k) = 1 - \frac{kp}{n}
	\]
	אזי
	\[
		\frac{\PP(A \cap B_k)}{\PP(B_k)}
		= \frac{\frac{(n - k)p}{n}}{\frac{n - kp}{n}}
		= \frac{(n - k) p}{n - kp}
	\]
\end{solution}
\begin{exercise}
	האדם הוא מתודי והחליט להפסיק את החיפוש אם ההסתברות שהמכתב בשולחן קטנה מ־$\frac{1}{4}$. \\*
	נניח ש־$p = \frac{3}{4}$ ושיש 10 מגירות, כמה מגירות תיבדקנה לכל היותר עד שהאדם יפסיק את החיפוש?
\end{exercise}
\begin{solution}
	\[
		\frac{1}{4} > \PP(A \mid B_k) = \frac{(10 - k) \frac{3}{4}}{10 - \frac{3k}{4}} \iff k > \frac{89}{11}
	\]
	נבדוק לכל היותר 8 מגירות.
\end{solution}

\subsection{ניסוי דו־שלבי על־ידי הסתברות מותנית}
\begin{proposition}
	נניח שנתון ניסוי דו־שלבי על $\Omega_1 \times \Omega_2$ עם פונקציית הסתברות נקודתית $p$ על $\Omega_1$ ולכל $\omega \in \Omega_1$ גם $p_\omega$ היא פונקציית הסתברות נקודתית על $\Omega_2$. \\*
	אם $\PP$ היא פונקציה על $\Omega_1 \times \Omega_2$ המקיימת
	\[
		\PP(\{a, x\}) = p(a),
		\qquad
		\PP(\{ x, b \} \mid \{ (a, x) \}) = p_a(b)
	\]
	אז $\PP$ היא פונקציית הסתברות יחידה המתאימה לניסוי הדו־שלבי.
\end{proposition}
\begin{proof}
	יהי $(a, b) \in \Omega_1 \times \Omega_2$, מכלל השרשרת נובע
	\[
		\PP(\{(a, b)\})
		= \PP(\{(a, x)\}) \cdot \PP(\{(x, b)\} \mid \{(a, x)\})
		= p(a) \cdot p_a(b)
		= q(a, b)
	\]
	עבור $q$ פונקציית הסתברות נקודתית של $\PP$.
\end{proof}
נבחין שוב כי בעוד כל ניסוי דו־שלבי, ניתן לבחון אותו כניסוי מותנה, הכיוון ההפוך לא בהכרח מתקיים; לא כל ניסוי מותנה הוא ניסוי דו־שלבי. \\*
נבחן דוגמות לשימוש בקשר זה.
\begin{exercise}
	בשוק ישנם שלושה סוגי מחשבים. חצי מסוג ראשון, 30\% מסוג שני ו־20\% מסוג שלישי. \\*
	הסיכוי שמחשב מסוג ראשון יתקלקל בשנתו הראשונה הוא עשירית, הסיכוי לסוג שני הוא חמישית והסיכוי למחשב מהסוג השלישי הוא $\frac{1}{20}$. \\*
	קונים מחשב באקראי מבין מחשבי השוק, מה ההסתברות שהוא יתקלקל בשנתו הראשונה?
\end{exercise}
\begin{solution}
	נסמן $C_j$ הוא המאורע שקנינו מחשב מסוג $j$ ו־$B$ המאורע שהמחשב התקלקל בשנתו הראשונה. \\*
	עוד נתון $\PP(C_1) = \frac{l}{2}, \PP(C_2) = \frac{3}{10}, \PP(C_3) = \frac{1}{5}$. \\*
	נתונים לנו גם $\PP(B \mid C_1) = \frac{1}{10}, \PP(B \mid C_2) = \frac{1}{5}, \PP(B \mid C_3) = \frac{1}{20}$.
	מנוסחת ההסתברות השלמה נובע
	\[
		\PP(B) = \PP(B \mid C_1) \PP(C_1) + \PP(B \mid C_2) \PP(C_2) + \PP(B \mid C_3) \PP(C_3)
	\]
\end{solution}
\begin{exercise}
	במבחן אמריקאי לכל שאלה 4 אפשרויות ובדיוק 1 נכונה.
	סטודנטית ניגשת למבחן עם האסטרטגיה הבאה:
	\begin{itemize}
		\item אם היא יודעת את התשובה היא עונה נכונה.
		\item אם היא לא יודעת את התשובה אז היא בוחרת תשובה אקראית.
	\end{itemize}
	נתון כי הסטודנטית יודעת את התשובה ל־90\% משאלות הבחינה. \\*
	בוחרים שאלה באקראי, ונתון שהסטודנטית ענתה עליה נכון, מה ההסתברות שהיא ידעה את התשובה.
\end{exercise}
\begin{solution}
	נסמן ב־$A$ את המאורע שהסטודנטית ידעה את התשובה, וב־$B$ את המאורע שהסטודנטית ענתה נכון. \\*
	אנו יודעים כי $\PP(A) = \frac{9}{10}$ וגם כי $\PP(B \mid A) = 1, \PP(B \mid A^C) = \frac{1}{4}$.
	\[
		\PP(A \mid B)
		= \frac{\PP(A)}{\PP(B)} \PP(B \mid A)
		= \frac{\frac{9}{10} \cdot 1}{\PP(B \mid A) \cdot \PP(A) + \PP(B \mid A^C) \cdot \PP(A^C)}
		= \frac{\frac{9}{10}}{\frac{9}{10} + \frac{1}{4} \cdot \frac{1}{10}}
		= \frac{\frac{9}{10}}{\frac{37}{40}}
		\approx 0.973
	\]
\end{solution}

\section{שיעור 6 --- 14.11.2024}

\subsection{אי־תלות}
\begin{definition}[מאורעות בלתי־תלויים]
	מאורעות $A, B$ המקיימים $\PP(A \cap B) = \PP(A) \PP(B)$ יקראו \textbf{בלתי־תלויים}.
\end{definition}
\begin{remark}
	נובע שמתקיים
	\[
		\PP(A \mid B) = \PP(A),
		\qquad
		\PP(B \mid A) = \PP(B)
	\]
\end{remark}
\begin{remark}[תזכורת]
	אם $A \subseteq \Omega_1$ ו־$B \subseteq \Omega_2$ ועובדים עם $\PP$ פונקציית הסתברות של מרחב המכפלה $\Omega_1 \times \Omega_2$. \\*
	אז ראינו שמתקיים $\PP(A \times B) = \PP_1(A) \cdot \PP_2(B) = \PP(A \times \Omega_2) \cdot \PP(\Omega_1 \times B)$
\end{remark}
\begin{example}
	מטילים שתי קוביות, אז $\Omega = {[6]}^2$. \\*
	נגדיר $A = \{ 4 \} \times [6]$ המאורע שיצא 4 בקוביה הראשונה ו־$B$ המאורע שסכום הקוביות הוא 7.
	\[
		\PP(A) = \frac{|A|}{|\Omega|} = \frac{1}{6},
		\qquad
		\PP(B) = \frac{|B|}{|\Omega|} = \frac{1}{6}
	\]
	וחישוב חיתוך המאורעות יניב
	\[
		\PP(A \cap B) = \frac{|A \cap B|}{|\Omega|} = \frac{|\{(4, 3)\}|}{36} = \frac{1}{36} = \PP(A) \PP(B)
	\]
	אז המאורעות בלתי־תלויים.
\end{example}
\begin{proposition}
	\begin{enumerate}
		\item לכל מאורע $A$, $A$ ו־$\emptyset$ בלתי־תלויים וכן $A$ ו־$\Omega$ בלתי־תלויים.
		\item אם $A$ ו־$B$ בלתי־תלויים ו־$\PP(B) > 0$ אז $\PP(A \mid B) = \PP(A)$.
		\item אם $A$ ו־$B$ בלתי־תלויים אז גם $A^C$ ו־$B$ בלתי תלויים.
	\end{enumerate}
\end{proposition}
\begin{proof}
	נוכיח את הטענה השלישית
	\[
		\PP(B \cap A^C)
		= \PP(B) - \PP(A \cap B)
		= \PP(B) - \PP(B) \PP(A)
		= \PP(B) (1 - \PP(A))
		= \PP(B) \PP(A^C)
	\]
	במעבר הראשון השתמשנו בנוסחת ההסתברות השלמה על החלוקה $A, A^C$.
\end{proof}
נראה הגדרה לאי־תלות במספר מאורעות, אך לא ההגדרה שאנו רוצים לעבוד איתה.
\begin{definition}[אי־תלות בזוגות]
	אם $A_1, \dots, A_n$ נקראים בלתי תלויים בזוגות אם
	\[
		\forall 1 \le i < j \le n, \PP(A_i \cap A_j) = \PP(A_i) \PP(A_j)
	\]
\end{definition}
\begin{definition}[קבוצה בלתי־תלויה]
	מאורע $A$ נקרא בלתי־תלוי בקבוצת המאורעות $B_1, \dots, B_n$ אם לכל $I \subseteq [n]$ מתקיים
	\[
		\PP(A \mid \bigcap_{i \in I} B_i) = \PP(A)
	\]
	דהינו $A$ ו־$\bigcap_{i \in I} B_i$ בלתי־תלוי.
\end{definition}
\begin{exercise}
	הביאו דוגמה למאורעות $A, B_1, B_2$ כך ש־$A$ ו־$B_1$ בלתי־תלויים וכך גם $A$ ו־$B_2$ אבל $A$ לא בלתי־תלוי בקבוצה $\{B_1, B_2\}$. \\*
	הראו כי אם $A, B_1$ בלתי־תלויים וגם $A, B_2$ בלתי־תלויים וגם $B_1, B_2$ זרים, אז $A$ בלתי־תלוי ב־$\{B_1, B_2\}$.
\end{exercise}
\begin{proposition}
	$A$ בלתי־תלוי ב־$\{B_1, \dots, B_n\}$ אם ורק אם $A$ בלתי תלוי ב־$\{B_1, \dots, B_n, B_1^C, \dots, B_n^C\}$.
\end{proposition}
\begin{proof}
	הכיוון השני הוא טריוויאלי, לכן נוכיח את הכיוון הראשון בלבד. \\*
	נראה ש־$A$ בלתי־תלוי בקבוצה ${\{B_1, \dots, B_n, B_1\}}^C$.
	נרצה להראות שלכל $I \subseteq [n + 1]$ מתקיים ש־$A$ ו־$\bigcap_{i \in I} B_i$ בלתי־תלויים. \\*
	אם $n + 1 \notin I$ אז לפי ההנחה חוסר התלות כבר מתקיים. \\*
	אחרת נגדיר $J = I \setminus \{ n + 1 \}$ ולכן $I = J \uplus \{ n + 1 \}$, ומכאן נובע
	\begin{align*}
		\PP((\bigcap_{i \in I} B_i) \cap A)
		& = \PP((\bigcap_{i \in J} B_i) \cap B_1^C \cap A) \\
		& = \PP(\bigcap_{i \in J} B_i \cap A) - \PP(\bigcap_{i \in J} B_i \cap B_1 \cap A) \\
		& = \PP(\bigcap_{i \in J} B_i) \PP(A) - \PP(\bigcap_{i \in J} B_i \cap B_1) \PP(A) \\
		& = \PP(\bigcap_{i \in J} B_i \cap B_1^C) \PP(A) \\
		& = \PP(\bigcap_{i \in I} B_i) \PP(A)
	\end{align*}
	ומצאנו כי ניתן להוסיף איבר, בשל כך נוכל לבצע את התהליך איטרטיבית ולקבל את המבוקש.
\end{proof}
\begin{definition}[אי־תלות קבוצת מאורעות]
	קבוצת מאורעות $\{A_1, \dots, A_n\}$ נקראת בלתי־תלויה אם לכל $I \subseteq [n]$ מתקיים
	\[
		\PP(\bigcap_{i \in I} A_i) = \prod_{i \in I} \PP(A_i)
	\]
\end{definition}
\begin{conclusion}
	אם $A_1, \dots, A_n$ בלתי־תלויים, אז גם כל תת־קבוצה של מאורעות היא בלתי־תלויה. \\*
	בפרט $A_1, \dots, A_n$ בלתי־תלויים גורר ש־$A_1, \dots, A_n$ בלתי־תלויים בזוגות.
\end{conclusion}
\begin{proposition}
	קבוצת מאורעות $\{A_1, \dots, A_n\}$ בלתי־תלויה אם ורק אם לכל $i \in [n]$ מתקיים ש־$A_i$ בלתי־תלויה ב־$\{A_1, \dots, A_n\} \setminus \{A_i\}$.
\end{proposition}
\begin{proof}
	לכיוון הראשון, נרצה להראות ש־$A_1$ לא תלוי ב־$\{A_2, \dots, A_n\}$, כלומר לכל $I \subseteq \{2, \dots, n\}$ רוצים להראות ש־$\PP(\bigcap_{i \in I} A_i \cap A_1) = \PP(\bigcap_{i \in I} A_i) \PP(A_1)$ על־ידי
	\[
		\PP(\bigcap_{i \in I} A_i \cap A_1)
		= (\prod_{i \in I} \PP(A_i)) \PP(A_1)
		= \PP(\bigcap_{i \in I} A_i) \PP(A_1)
	\]

	נעבור לכיוון השני.
	צריך להראות שלכל $I \subseteq [n]$ מתקיים $\PP(\bigcap_{i \in I} A_i) = \prod_{i \in I} \PP(A_i)$.
	תהי $I \subseteq [n] = \{ i_1, \dots, i_k \}$ כאשר $|I| = k$. \\*
	לפי ההנחה $A_{i_1}$ בלתי־תלוי ב־$\{ A_j \mid j \in [n] \setminus \{ i_1 \}\}$, לכן נקבל באינדוקציה
	\[
		\PP(\bigcap_{l = 1}^k A_{i_l})
		= \PP(A_{i_1} \cap (\bigcap_{l = 2}^k A_{i_l}))
		= \PP(A_{i_1}) \cdot \PP(\bigcap_{l = 2}^k A_{i_l})
		= \PP(A_{i_1}) \PP(A_{i_2}) \PP(\bigcap_{l = 3}^k A_{i_l})
		= \cdots = \PP(A_{i_1}) \cdots \PP(A_{i_k})
	\]
\end{proof}

\section{שיעור 7 --- 19.11.2024}

\subsection{אי־תלות}
נראה הגדרה שקולה לאי־תלות
\begin{definition}[שקולה לאי־תלות]
	$A_1, \dots, A_n$ בלתי־תלויים אם ורק אם
	\[
		\forall I \subseteq [n], \PP((\bigcap_{i \in I} A_i) \cap (\bigcap_{i \in [n] \setminus I} A_i^C))
		= \prod_{i \in I} \PP(A_i) \prod_{i \in [n] \setminus I} \PP(A_i^C)
	\]
\end{definition}
את השקילות של ההגדרות נראה בתרגיל. \\*
מאורעות $A_1, \dots, A_n$ בלתי־תלויים בהינתן $B$ אם הם בלתי־תלויים לפי פונקציית ההסתברות המותנית ב־$B$, $\PP_B$.
\begin{example}
	בוחרים מטבע באקראי משק ומטילים אותו $n$ פעמים. \\*
	$A_i$ יצא עץ בהטלה ה־$i$ בלתי־תלוי בהינתן בחירת המטבע, $B$ המאורע שנבחר מטבע מסוים.
\end{example}
נרצה לנסות לתת הגדרה חדשה עבור מקרים אינסופיים, נראה שיתקיים
\[
	\forall I \subseteq \NN, \PP(\bigcap_{i \in I} A_i = \prod_{i \in I} \PP(A_i))
\]
אבל היא לא מועילה לנו, נגדיר במקום זאת
\begin{definition}[קבוצה בת־מניה בלתי־תלויה]
	$A_1, A_2, \dots$ מאורעות הם בלתי־תלויים אם ורק אם \\*
	לכל קבוצה סופית $I$ מתקיים ${\{A_i\}}_{i \in I}$ קבוצה בלתי־תלויה.
\end{definition}
\begin{remark}[מכפלה אינסופית]
	נגדיר מכפלה אינסופית על־ידי
	\[
		\prod_{i \in \NN} a_i
		= \prod_{i = 1}^\infty a_i
		= \lim_{N \to \infty} \prod_{i = 1}^N a_i
	\]
\end{remark}
\begin{proposition}
	אם $A_1, A_2, \dots$ סדרת מאורעות בלתי־תלויים אז
	\[
		\PP(\bigcap_{i \in \NN} A_i) = \prod_{i \in \NN} \PP(A_i)
	\]
\end{proposition}
\begin{proof}
	נגדיר $B_n = \bigcap_{i = 1}^n A_i$ סדרה יורדת ולכן מרציפות פונקציית ההסתברות נובע
	\[
		\PP(\bigcap_{i \in \NN} A_i)
		= \PP(\bigcap_{n \in \NN} B_n)
		= \lim_{n \to \infty} \PP(B_n)
		= \lim_{N \to \infty} \prod_{i = 1}^N \PP(A_i)
		= \prod_{i = 1}^\infty \PP(A_i)
	\]
\end{proof}
\begin{example}
	אם $A_1, \dots$ בלתי־תלויים ו־$\PP(A_i) = p < 1$ אז $\PP(\bigcap_{i \in \NN} A_i) = 0$. \\*
	לדוגמה בהטלה אינסוף פעמים של מטבע הסיכוי שייצא עץ הוא אפס.
	דוגמה זו קצת בעייתית שכן כלל לא הראינו כי מרחב זה קיים ומוגדר, אבל המשמעות היא שעבור מרחבי מדגם הולכים וגדלים, אז ההסתברות המבוקשת שואפת להיות אפס.
\end{example}

\subsection{משתנים מקריים}
עד כה היינו צריכים לבצע ניתוח מלא של הסיטואציה כדי להגיע למסקנה, גם אם בהרבה מקרים שונים הגענו לבדיוק אותה המסקנה, המטרה של משתנים מקריים הוא לבודד את הרעיון הזה ולתקוף אותו.
\begin{definition}[משתנה מקרי]
	יהי $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, פונצקיה מ־$\Omega$ ל־$\RR$ נקראת \textbf{משתנה מקרי}.
\end{definition}
\begin{notation}
	על־אף שזו פונקציה, נהוג לסמן משתנים מקריים בסימונים שאנו רגילים שמשמשים למשתנים, לדוגמה $X, Y, Z$.
\end{notation}
\begin{remark}
	השם קצת מטעה, אלו הם לא משתנים, ושווה לחשוב עליהם בתור מצבים מקריים יותר.
\end{remark}
\begin{example}
	נניח $\Omega = \{H, T\}$ הטלת מטבע, ונגדיר את הפונקציה $f : \Omega \to \RR$ על־ידי $f(H) = 2, f(T) = -3$,
	זאת במטרה לייצג שאם יוצא עץ נפסיד שלושה מטבעות ואם מתקבל פלי אז נקבל שני מטבעות.
\end{example}
\begin{example}
	נרצה להטיל שתי קוביות ונרצה לדבר על תוצאת אחת ההטלות, נתחיל ונגדיר $\Omega = {[6]}^2$. \\*
	נגדיר $X_1 : \Omega \to \RR$ על־ידי $X_1(a, b) = a$, ובאופן דומה נגדיר $X_2(a, b) = b$.
	יצרנו פונקציות שמהוות משתנה מקרי עבור ההטלה הראשונה ועבור ההטלה השנייה, נגדיר גם עבור הסכום, $Y(a, b) = a + b$. \\*
	נקבל עתה $Y = X_1 + X_2$, ונבחין בכוח האמיתי של הגדרה זו, יש לנו איזשהו קישור מורכב במרחב ההסתברות ללא עבודה ישירות מול המרחב.
\end{example}
\begin{definition}[משתנה מקרי מושרה ממאורע]
	אם $A$ מאורע אז נגדיר $1_A$ משתנה מקרי על־ידי
	\[
		1_A(\omega) = \begin{cases}
			1, & \omega \in A \\
			0, & \omega \notin A
		\end{cases}
	\]
\end{definition}
\begin{proposition}[תכונות של משתנים מקריים מושרים]
	\begin{enumerate}
		\item $1_{A^C} = 1 - 1_A$
		\item $1_{A \cap B} = 1_A \cdot 1_B$
		\item $1_{A \cup B} = \max\{1_A, 1_B\}$
	\end{enumerate}
\end{proposition}
\begin{example}
	$\Omega = S_n$, $A_i$ המאורע שיש $i$ נקודות שבת. \\*
	נסמן $X_i = 1_{A_i}$ ו־$X = \sum_{i = 1}^n X_i$.
\end{example}
בדוגמות הקודמות ההטלה הראשונה זוגית $\{ (a, b) \in {[6]}^2 \mid a \in \{2, 4, 6\}\}$. נכתוב במקום זאת $X_1 \in \{2, 4, 6\}$.
\begin{definition}[מאורע מושרה ממשתנה מקרי]
	אם $X$ משתנה מקרי ו־$S \in \mathcal{F}_\RR$, המאורע $X \in S$ מוגדר להיות
	\[
		X^{-1}(S) = \{ \omega \in \Omega \mid X(\omega) \in S \}
	\]
	בהתאם נכתוב $\PP(\{x \in S\})$ על־ידי $\PP(X \in S)$, ובאופן דומה נוכל לציין גם את $\PP(X = s), \PP(X \le s)$ ודומים.
\end{definition}
\begin{definition}[פונקציית הסתברות מושרית ממשתנה מקרי]
	$(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, ויהי $X$ משתנה מקרי. \\*
	נגדיר פונקציה $\PP_X : \mathcal{F}_\RR \to [0, \infty)$ על־ידי
	\[
		\PP_X(S) = \PP(X \in S) = \PP(\{\omega \in \Omega \mid X(\omega) \in S \})
	\]
	$\PP_X$ מכונה ה\textbf{ההתפלגות} של $X$. \\*
	אם $\PP_X$ נתמכת על $S$ (כלומר $\PP_X(S) = 1$) אז אומרים ש־$X$ נתמך על $S$.
\end{definition}
\begin{proposition}
	$\PP_X$ היא פונקציית הסתברות על $(\RR, \mathcal{F}_\RR)$.
\end{proposition}
\begin{proof}
	\[
		\forall S, \PP_X(S) = \PP(X \in S) \ge 0
	\]
	וכן
	\[
		\PP_X(\RR) = \PP(X \in \RR) = \PP(\Omega) = 1
	\]
	ולבסוף סיגמא־אדיטיביות:
	\begin{align*}
		\forall S_1, S_2, \dots, \PP_X(\biguplus_{n \in \NN} S_n)
		& = \PP(X \in \biguplus_{n \in \NN} S_n) \\
		& = \PP(\{\omega \in \Omega \mid X(\omega) \in \biguplus_{n \in \NN} S_n \}) \\
		& = \PP(\biguplus_{n \in \NN} \{ X \in S_n \}) \\
		& = \sum_{n \in \NN} \PP(X \in S_n) \\
		& = \sum_{n \in \NN} \PP_X(S_n)
	\end{align*}
\end{proof}

\section{תרגול 4 --- 21.11.2024}

\subsection{אי־תלות}
נניח מרחב הסתברות $(\Omega, \mathcal{F}, \PP)$.
\begin{exercise}
	בכד שלושה מטבעות, שניים הוגנים ואחד שמוטבע עץ על שני צדדיו. \\*
	שולפים מטבע באקראי ואז מטילים אותו פעמיים. \\*
	האם תוצאת ההטלה הראשונה תלויה בתוצאת ההטלה השנייה?
\end{exercise}
\begin{solution}
	נסמן ב־$A_i$ את המאורע שבהטלה ה־$i$ יצא עץ. \\*
	אנו שואלים אם $A_1, A_2$ הם תלויים, נסמן גם $F$ המאורע ששלפנו מטבע הוגן.
	\[
		\PP(A_1)
		= \PP(A_1 \mid F) \PP(F) + \PP(A_1 \mid F^C) \PP(F^C)
		= \frac{1}{2} \cdot \frac{2}{3} + 1 \cdot \frac{1}{3} = \frac{2}{3}
	\]
	אנו רוצים לבדוק את התלות ולכן נחשב
	\[
		\PP(A_1 \cap A_2)
		= \PP(A_1 \cap A_2 \mid F) \PP(F) + \PP(A_1 \cap A_2 \mid F^C) \PP(F^C)
		= \frac{1}{4} \cdot \frac{2}{3} + 1 \cdot \frac{1}{3} = \frac{1}{2} \ne \frac{4}{9} = \PP(A_1) \cdot \PP(A_2)
	\]
	ולכן הם תלויים.
\end{solution}
\begin{exercise}
	נגדיר $\Omega = \NN$ ו־$\PP(\{n\}) = \frac{1}{c \cdot n^2}$, כאשר $c = \sum_{n \in \NN} \frac{1}{n^2} = \frac{\pi^2}{6}$. \\*
	נגדיר $\forall k \in \NN, A_k = k \NN = \{ kn \mid n \in \NN \}$. \\*
	האם ${\{A_k\}}_{k \in \NN}$ היא תלויה?
\end{exercise}
\begin{solution}
	\[
		\PP(A_k) = \sum_{n \in \NN} \PP(\{k_n\})
		= \sum_{n \in \NN} \frac{1}{c k^2 n^2}
		= \frac{1}{c k^2} \sum_{n \in \NN} \frac{1}{n^2}
		= \frac{1}{k^2}
	\]
	ולכן
	\[
		\PP(A_2 \mid A_4) = 1 \ne \frac{1}{4} = \PP(A_2)
	\]
	ולכן המאורעות תלויים ובכלל הקבוצה לא בלתי־תלויה.
\end{solution}
\begin{exercise}
	נגדיר $P$ קבוצת המספרים הראשוניים, האם ${\{A_p\}}_{p \in P}$ בלתי־תלויה?
\end{exercise}
\begin{solution}
	יהיו $p_1, \dots, p_m \in P$ ראשוניים, אז מהמשפט היסודי של האריתמטיקה (או פירוק לגורמים ראשוניים)
	\[
		A_{p_1} \cap \cdots \cap A_{p_m} = A_{p_1 \cdots p_m}
	\]
	ולכן
	\[
		\PP(A_{p_1} \cap \cdots \cap A_{p_m}) = \PP(A_{p_1 \cdots p_m}) = \frac{1}{{(p_1 \cdots p_m)}^2}
		= \frac{1}{p_1^2} \cdots \frac{1}{p_m^2} = \PP(A_{p_1}) \cdots \PP(A_{p_m})
	\]
	נגדיר גם $B = \bigcap_{p \in P} A_p^C = \{ 1 \}$, ונחשב
	\[
		\frac{6}{\pi^2} = \frac{1}{c} = \PP(B) = \prod_{p \in P} (1 - \frac{1}{p^2})
	\]
\end{solution}
\begin{conclusion}
	נוכל להסיק מסקנה משמעותית נוספת, לכל $s > 1$ מתקיים
	\[
		\sum_{n = 1}^\infty \frac{1}{n^2} = \prod_{p \in P} {(1 - \frac{1}{ps})}^{-1} = \zeta(s)
	\]
	הערך ה־$s$ של פונקציית זטא של רימן, וזו זהות אוילר לפונקציית זטא.
\end{conclusion}
\begin{conclusion}
	מסקנה נוספת היא שבשל אי־רציונליות $\pi$ נוכל להסיק כי הטור הוא לא טור סופי, לכן יש אינסוף ראשוניים.
\end{conclusion}

\subsection{משתנים מקריים}
אנו רוצים להסתכל על משתנה מקרי כדרך להסתכל מחדש על מרחב ההסתברות ובפרט פונקציית ההסתברות באופן נוסף, זה בתורו יאפשר לנו לפתור בעיות בדרך חדשה ואולי אף פשוטה יותר, כפי שנראה בהמשך.
\begin{example}
	נגדיר $X$ להיות משתנה מקרי שמתאר סכום הטלת שתי קוביות הוגנות, דהינו נוכל להגדיר $\PP = {[6]}^2$ ו־$\PP$ אחידה. \\*
	בהתאם נגדיר $X : \Omega \to \RR$ על־ידי $(a, b) \mapsto a + b$. \\*
	לכן
	\[
		\rng(X) = \{2, \dots, 12 \}
	\]
	נעבור לחישוב הסתברויות
	\begin{align*}
		& \PP(X = 2) = \frac{1}{36}, \\
		& \PP(X = 3) = \frac{2}{36}, \\
		& \PP(X = 4) = \frac{3}{36}, \\
		& \PP(X = 5) = \frac{4}{36}, \\
		& \PP(X = 6) = \frac{5}{36}, \\
		& \PP(X = 7) = \frac{6}{36}, \\
		& \PP(X = 8) = \frac{5}{36}
	\end{align*}
	וכן הלאה, בהתאם נוכל להסיק
	\[
		\forall E \subseteq \RR,
		\PP(X \in E)
		= \PP_X(E \cap \rng(X))
		= \sum_{i \in E \cap \rng(X)} \PP(X = i)
	\]
	נסמן $X_i$ תוצאת ההטלה ה־$i$, ולכן $X = X_1 + X_2$, נחשב את $X$ ביחס ל־$X_i$.
	\begin{align*}
		\forall n \in \{2, \dots, 12\},
		\PP(X = n) 
		& = \sum_{i = 1}^{6} \PP(X_1 = i) \PP(X_2 = n - i) \\
		& = \sum_{i = 1}^{6} \frac{1}{6} \min\{6 - i, 0\} \\
		& = \frac{1}{36} |\{\{1, \dots, 6\} \cap \{n - 1, \dots, n - 6 \}\}|
	\end{align*}
	אם נגדיר $Y = X (\mod 6)$ אז $\rng(Y) = \{0, \dots, 5\}$ ונחשב
	\begin{align*}
		\forall n \in \{0, \dots, 5\},
		\PP(Y = n)
		& = \PP(X = n \lor X = n + 6 \lor X = n + 12) \\
		& = \frac{1}{36} \cdot |\{1, \dots, 6\} \cap \{n + 12, \dots, n - 6\}| \\
		& = \frac{6}{36} = \frac{1}{6}
	\end{align*}
\end{example}

\section{שיעור 8 --- 21.11.2024}

\subsection{משתנים מקריים --- המשך}
\begin{definition}[משתנה מקרי בדיד]\label{def_discrete_random_variable}
	$X$ משתנה מקרי נקרא \textbf{בדיד} אם $\PP_X$ הוא פונקציית הסתברות בדידה. \\*
	במקרה זה יש ל־$X$ התפלגות נקודתית $p_X : \RR \to [0, \infty)$.
\end{definition}
\begin{remark}
	נבחין כי גם אם מרחב ההסתברות הוא לא בדיד, נוכל להגדיר משתנה מקרי בדיד עליו.
\end{remark}
\begin{example}
	נגדיר $A \in \mathcal{F}$ ו־$X = 1^A$ ונניח $\PP(A) = p$. \\*
	אם $S \subseteq \mathcal{F}_\RR$ אז אם $\{0, 1 \} \in S$ אז $\Omega = X^{-1}(S)$ ואז $\PP_X(S) = \PP(\Omega) = 1$. \\*
	אם $1 \in S$ אבל $0 \notin S$ אז $A = X^{-1}(S)$ ואז $\PP_X(S) = \PP(A) = p$. \\*
	לבסוף אם $0 \in S$ ו־$1 \notin S$ אז $A^C = X^{-1}(S)$ אז $\emptyset = X^{-1}(S)$ ואז $\PP_X(S) = \PP(\emptyset) = 0$.

	אם נגדיר $p_X : \RR \to [0, \infty)$ על־ידי
	\[
		p_X(s) = \begin{cases}
			p & s = 1 \\
			1 - p & s = 0 \\
			0 & \text{else}
		\end{cases}
	\]
	אז מתקיים
	\[
		\PP_X(S) = \sum_{s \in S} p_X(s)
	\]
\end{example}
\begin{definition}[התפלגות ברנולי]
	משתנה מקרי $X$ מתפלג ברנולי עם פרמטר $p$ אם יש לו פונקציית התפלגות נקודתית
	\[
		p_X(s) = \begin{cases}
			p & s = 1 \\
			1 - p & s = 0 \\
			0 & \text{else}
		\end{cases}
	\]
	במקרה זה נסמן $X \sim \text{Ber}(p)$, סימון לא מאוד מועיל או מתכתב עם השימוש הסטנדרטי, אבל אלה הם החיים.
\end{definition}
נשאל את עצמנו את השאלה האם כל משתנה מקרי מתפלג ברנולי הוא מציין של מאורע.
אילו נגדיר $A = X^{-1}(1)$ אז מתקבל $X = 1_A$, אנו אומרים ש־$X$ שווה למציין של $1_A$ כמעט תמיד, נראה זאת בהמשך הפרק. \\*
נמשיך לעוד מקרים.
\begin{definition}[משתנה מקרי קבוע]
	משתנה מקרי $X$ הוא קבוע אם
	\[
		p_X(s) = \begin{cases}
			1 & s = c \\
			0 & \text{else}
		\end{cases}
	\]
	עבור $c$ קבוע כלשהו.
\end{definition}
\begin{definition}[משתנה מקרי אחיד]
	משתנה מקרי $X$ נקרא אחיד על $S$ תת־קבוצה סופית של $\RR$ אם
	\[
		p_X(s) = \begin{cases}
			\frac{1}{|S|} & s \in S \\
			0 & \text{else}
		\end{cases}
	\]
	במקרה זה נסמן $X \sim U(S)$.
\end{definition}
\begin{definition}[התפלגות גאומטרית]\label{geometric_distribution}
	$X$ מתפלג גאומטרית עם פרמטר $p$ אם
	\[
		p_X(s) = \begin{cases}
			{(1 - p)}^{s - 1} p & s \in \{1, 2, \dots \} \\
			0 & \text{else}
		\end{cases}
	\]
	ונסמן $X \sim \text{Geo}(p)$. \\*
	לפעמים הגדרה זו תסומן אחרת על־ידי מדידת המקרים שבהם יצאה ההסתברות למאורע הראשון בלבד. \\*
	התפלגות זו מתארת את המקרה שניסינו לקבל תוצאה בהסתברות בין שני מקרים וקיבלנו אותה בפעם ה־$s$.
\end{definition}
\begin{definition}[התפלגות בינומית]\label{binominal_distribution}
	$X$ מתפלג בינומית עם פרמטרים $n$ ו־$p$ אם
	\[
		p_X(s) = \begin{cases}
			\binom{n}{s} p^s {(1 - p)}^{n - s} & s \in \{1, 2, \dots\} \\
			0 & \text{else}
		\end{cases}
	\]
	ונסמן $X \sim \text{Bin}(n, p)$.
\end{definition}
מאפשר לנו לחשב את מספר המטבעות המוטים שיצאו על צד מסוים.
ולבסוף
\begin{definition}[התפלגות פואסונית]\label{poasson_distribution}
	$X$ מתפלג פואסונית עם פרמטר $\lambda$ אם
	\[
		p_X(s) = \begin{cases}
			e^{-\lambda} \frac{\lambda^s}{s!} & s \in \{0, 1, 2, \dots\} \\
			0 & \text{else}
		\end{cases}
	\]
	ונסמן $X \sim \text{Po}(\lambda)$.
\end{definition}
בפעם הראשונה ההתפלגות הזו הופיעה בהקשר של מספר החיילים שנהרגו מבעיטה מהסוס שלהם, התפלגות שהייתה מהותית עד מלחמת העולם הראשונה.

\subsection{קשרים בין משתנים־מקריים}
\begin{example}
	$\Omega = {[6]}^2$ מרחב אחיד להטלת שתי קוביות, ונגדיר שוב $Y = X_1 + X_2$ סכום הטלות שתי הקוביות, דהינו
	\[
		X_1(a, b) = a,
		X_2(a, b) = b,
		Y(a, b) = a + b
	\]
	בתרגול מצאנו את הערכים של $p_Y$ לכל ערך אפשרי. \\*
	נגדיר גם $Z = Y \mod 6$ (ומנוחות נגדיר $Z \in [6]$), ונשאל מה ההתפלגות של $Z$.
	\[
		p_Z(1) = \PP(Z = 1) = \PP(Y = 7) = \frac{1}{6}
	\]
	באופן דומה
	\[
		p_Z(2) = \PP(Z = 2) = \PP(Y = 2) + \PP(Y = 8) = \frac{1}{36} + \frac{5}{36} = \frac{1}{6}
	\]
	באופן כללי מתקיים מחישוב כזה ש־$p_Z(n) = \frac{1}{6}$ לכל $n \in [6]$, נסיק כי $Z \sim U([6])$.
\end{example}
\begin{definition}[הסתברות כמעט תמיד]
	אם $A$ מאורע עם הסתברות $1$ אז אומרים שהוא מתרחש כמעט תמיד.
\end{definition}
\begin{definition}[משתנים שווים שמעט תמיד]
	אם $X$ ו־$Y$ המקיימים ש־$X = Y$ כמעט תמיד אז נסמן $X \overset{a.s.}{=} Y$. \\*
	זה כמובן שקול להגדרה כי $\PP(\{ \omega \in \Omega \mid X(\omega) = Y(\omega) \}) = 1$ וזה נכון אם ורק אם $\PP(\{\omega \in \Omega \mid X(\omega) \ne Y(\omega) \}) = 0$.
\end{definition}
\begin{exercise}
	הוכיחו כי אם $X \overset{a.s.}{=} Y$ וגם $Y \overset{a.s.}{=} Z$ אז גם $X \overset{a.s.}{=} Z$, דהינו זהו יחס טרנזיטיבי.
\end{exercise}
\begin{proof}
	נשים לב לעובדה הבאה, אם $X(\omega) = Y(\omega)$ ו־$Y(\omega) = Z(\omega)$ עבור $\omega \in \Omega$ כלשהו, אז $X(\omega) = Z(\omega)$,
	כלומר
	\[
		\{ \omega \in \Omega \mid X(\omega) = Y(\omega) \} \cap \{ \omega \in \Omega \mid Y(\omega) = Z(\omega) \} \subseteq \{ \omega \in \Omega \mid X(\omega) = Z(\omega) \}
	\]
	ובהתאם גם
	\[
		\{ \omega \in \Omega \mid X(\omega) \ne Y(\omega) \} \cup \{ \omega \in \Omega \mid Y(\omega) \ne Z(\omega) \} \supseteq \{ \omega \in \Omega \mid X(\omega) \ne Z(\omega) \}
	\]
	אז מחסם האיחוד נקבל
	\[
		0 \le \{ \omega \in \Omega \mid X(\omega) \ne Z(\omega) \} \le 0 + 0
	\]
\end{proof}
\begin{proposition}
	$\overset{a.s.}{=}$ הוא יחס שקילות על מרחב כל המשתנים־המקריים על $\Omega$.
\end{proposition}
\begin{proof}
	ראינו עתה טרנזיטיביות, וסימטריה ורפלקסיביות נובעות ישירות מההגדרה.
\end{proof}
\begin{exercise}
	האם בדוגמה קודם מתקיים $X_1 \overset{a.s.}{=} X_2$?
\end{exercise}
\begin{solution}
	מחישוב מתקיים $\PP(X_1 = X_2) = \frac{1}{6}$ ולכן התשובה היא שלא. \\*
	נבחין כי גם $\PP(X_1 \ne Z) \ge \PP(X_1 = 2, Z = 3) = \PP(\{(2, 1)\}) = \frac{1}{36}$.
\end{solution}
באופן יותר כללי גם אם יש מאורעות שיש להם אותה ההסתברות, אין הכרח שיהיה קשר לשוויון שלהם כמעט תמיד.
\begin{definition}[משתנים  מקריים שווי התפלגות]
	אם למשתנים מקריים $X, Y$ יש אותה פונקציית התפלגות, דהינו $\PP_Y = \PP_X$, \\*
	דהינו מתקיים $\forall S \in \mathcal{F}_\RR, \PP_X(S) = \PP_Y(S) \iff \PP(X \in S) = \PP(Y \in S) \iff \PP(X^{-1}(S)) = \PP(Y^{-1}(S))$, \\*
	אז נאמר שהם שווי התפלגות ונסמן $X \overset{d}{=} Y$.
\end{definition}
ראינו שיש משתנים מקריים $X$ ו־$Y$ כך ש־$X \overset{a.s.}{\ne} Y$ אבל $X \overset{d}{=} Y$,
האם $X \overset{a.s.}{=} Y$ גורר $X \overset{d}{=} Y$?
התשובה היא שכן!
\begin{proposition}
	אם $X \overset{a.s.}{=} Y$ אז גם $X \overset{d}{=}$.
\end{proposition}
\begin{proof}
	נניח ש־$X \overset{a.s.}{=} Y$ ונרצה להוכיח ש־$\forall S \in \mathcal{F}_\RR, \PP(X \in S) = \PP(Y \in S)$. \\*
	לכל $S \in \mathcal{F}_\RR$ מתקיים
	\[
		0 \ne \PP(X \ne Y) \ge \PP(X \in S, Y \notin S) = 0
	\]
	ובהתאם
	\[
		\PP(X \in S) = \PP(X \in S, Y \in S) + \overbrace{\PP(X \in S, Y \notin S)}^{= 0} = \PP(X \in S, Y \in S)
	\]
	כמו־כן גם $\PP(Y \in S) = \PP(X \in S, Y \in S)$.
\end{proof}

\section{שיעור 9 --- 25.11.2024}

\subsection{וקטורים מקריים}
ניזכר בהגדרה\ \ref{def_discrete_random_variable}:
\begin{definition*}[משתנה מקרי בדיד]
	משתנה מקרי נקרא בדיד אם $\PP_X$ פונקציית הסתברות בדידה, כלומר
	\[
		\forall S \in \mathcal{F}_X, \PP_X(S) = \sum_{s \in S} p_X(s)
	\]
	כאשר $p_X(s) = \PP(X = s) = \PP(X^{-1}(s)) = \PP(\{\omega \in \Omega \mid X(\omega) = s \})$.
\end{definition*}
גם דיברנו על סוגים שונים של התפלגות, לדוגמה
\[
	\forall i \in [6], p_X(i) = \frac{1}{6} \iff X \sim U([6])
\]
או באופן דומה
\[
	\forall k \in \{0, \dots, n\}, p_X(k) = \binom{n}{k} p^k {(1 - p)}^{n - k}
\]
$A$ מתרחש כמעט תמיד אם $\PP(A) = 1$, לכן אם $\PP(X = Y) = 1$ אז נאמר ש־$X = Y$ כמעט תמיד, או נסמן $X \overset{a.s.}{=} Y$. \\*
באופן דומה אם $\PP_X = \PP_Y$ אז נסמן $X \overset{d}{=} Y$ או נאמר ש־$X$ ו־$Y$ שווי התפלגות, וראינו קשר בין שתי ההגדרות.
\begin{example}
	נגדיר הטלת מטבע, $\Omega = \{H, T\}$ ו־$X = 1_{\{H\}}, Y = 1_{\{T\}}$, אז $X \overset{d}{=} Y$ שכן
	\[
		p_X(s) = p_Y(s) = \begin{cases}
			\frac{1}{2} & s = 0 \\
			\frac{1}{2} & s = 1 \\
			0 & \text{else}
		\end{cases}
	\]
	אבל גם $\PP(X = Y) = 0$ ולכן $X \overset{a.s.}{\ne} Y$.
\end{example}
\begin{proposition}
	אם $X \overset{d}{=} Y$ ו־$f \in \mathcal{F}_{\RR \to \RR}$ אז $f(X) \overset{d}{=} f(Y)$.
\end{proposition}
\begin{proof}
	נגדיר $W = f(Y), Z = f(X)$,
	צריך להוכיח ש־$\forall S \in \mathcal{F}_\RR, \PP_Z(S) = \PP_W(S)$.
	\begin{align*}
		\PP_Z(S)
		& = \PP(Z \in S) \\
		& = \PP(\{\omega \in \omega \mid Z(\omega) \in S\}) \\
		& = \PP(\{\omega \in \omega \mid f(X(\omega)) \in S\}) \\
		& = \PP(\{\omega \in \omega \mid X(\omega) \in f^{-1}(S)\}) \\
		& = \PP(X \in f^{-1}(S)) \\
		& = \PP_X(f^{-1}(S)) \\
		& = \PP_Y(f^{-1}(S)) \\
		& = \PP(\{\omega \in \omega \mid Y(\omega) \in f^{-1}(S)\}) \\
		& = \PP(\{\omega \in \omega \mid f(Y(\omega)) \in S\}) \\
		& = \PP(W \in S) \\
		& = \PP_W(S)
	\end{align*}
\end{proof}
\begin{example}
	נניח ש־$X \sim Ber(\frac{1}{2})$ וגם $Y \sim Ber(\frac{1}{2})$, ונרצה לחשב את $\PP(X = Y)$. \\*
	אין לנו את היכולת לעשות זאת כי אין מספיק מידע.
\end{example}
\begin{definition}[וקטור מקרי]
	וקטור מקרי הוא משתנה מקרי לתוך $\RR^n$ במקום ל־$\RR$, $X : \Omega \to \RR^n$.
\end{definition}
כלל ההגדרות נשארות זהות פרט להגדרה זו, לדוגמה $\PP_X(S) = \PP(X \in S)$. \\*
המוטיב שלנו הוא היכולת לבנות כמה משתנים מקריים ולעבוד איתם כיציר בודד, לדוגמה $X = (X_1, X_2)$ עבור $X_1, X_2 : \Omega \to \RR$ משתנים מקריים. \\*
\begin{definition}[התפלגות משותפת והתפלגויות שוליות]
	אם $X_1, \dots, X_n$ משתנים מקריים המוגדרים על $\Omega$ יחיד אז $X = (X_1, \dots, X_n)$ הוא וקטור מקרי המוגדר על $\Omega$ וההתפלגות שלו נקראת ההתפלגות המשותפת של $X_1, \dots, X_n$. \\*
	ההתפלגויות של כל אחד מ־$X_1, \dots, X_n$ נקראות ההתפלגויות השוליות.
\end{definition}
השם הזה נובע מהגישה שבה נוכל להבין את ההסתברות של משתנה מקרי בודד מתוך הווקטור על־ידי, אם $X_1, X_2$ מרחבים מקריים אז
\[
	\PP_{X_1}(S) = \PP_{(X_1, X_2)}(S \times \RR) = \PP(\{\omega \in \in \Omega \mid (X_1(\omega), X_2(\omega)) \in S\})
\]
\begin{example}
	אם $\Omega = {[6]}^2$ ו־$X_1(a, b) = a, X_2(a, b) = b$ אז $X = (X_1, X_2)$ כאשר $X : \Omega \to \RR^2$ פונקציית הזהות.
\end{example}
\begin{example}
	נבחן עבור $E = \{(s, y) \in \RR^2 \mid s \le t\}$ את
	\[
		\PP_{(X, Y)}(E)
		= \PP(X \le Y)
	\]
\end{example}
\begin{definition}[התפלגות משותפת בדידה]
	אם לווקטור המקרי התפלגות בדידה, כלומר $\PP_{(X_1, \dots, X_n)}$ פונקציית הסתברות בדידה, \\*
	אז נאמר שההתפלגות המשותפת של $X_1, \dots, X_n$ בדידה.
\end{definition}
\begin{proposition}
	ההתפלגות המשותפת של $X_1, \dots, X_n$ בדידה אם ורק אם ההתפלגות של כל אחד מ־$X_1, \dots, X_n$ בדידה.
\end{proposition}
\begin{proof}
	נוכיח את הכיוון הראשון. \\*
	נניח $\PP_{X_1, \dots, X_n}$ בדידה, אך זה נכון אם ורק אם היא נתמכת על־ידי קבוצה בת־מניה, נבחר קבוצה $S \in \mathcal{F}_\RR$ כזו. \\*
	נסמן ב־$S_1$ את ההטלה של $S$ על הקורדינטה הראשונה, לכן $\PP_{X_1}(S_1) = \PP_{(X_1, X_2)}(S_1 \times \RR)$ אבל $S \subseteq S_1 \times \RR$. \\*
	לכן $X_1$ נתמך על־ידי קבוצה בת־מניה, $S_1$, ולכן הוא בדיד.

	נעבור לכיוון השני. \\*
	נניח ש־$X_1, X_2$ בדידים, לכן קיימות $S_1, S_2 \in \mathcal{F}_\RR$ בנות־מניה, \\*
	כך ש־$\PP(X_1 \in S_1) = \PP(X_2 \in S_2) = 1$. \\*
	לכן $\PP((X_1, X_2) \in S_1 \times S_2) = \PP(X_1 \in S_2, X_2 \in S_2) = 1$. \\*
	$S_1, S_2$ בנות־מניה ולכן נובע ש־$S_1 \times S_2$ בת־מניה.

	כמובן לווקטורים בגודל $n > 2$ ההוכחה דומה.
\end{proof}

\section{תרגול 5 --- 28.11.2024}

\subsection{משתנים מקריים}
בהרצאה זו נניח שכל המשתנים המקריים הם בדידים.
\begin{definition}[התניה במשתנים מקריים בדידים]
	אם $X : \Omega \to \RR^d$ ו־$A \subseteq \Omega$ כך ש־$\PP(A) > 0$ אז
	\[
		\forall S \subseteq \RR^d, \PP_{X \mid A}(S) = \PP(X \in S \mid A) = \PP_A(X \in S)
	\]
\end{definition}
\begin{definition}[אי־תלות במשתנים מקריים בדידים]
	אם $X, Y : \Omega \to \RR^d$ בלתי־תלויים אם לכל $S, T \subseteq \RR^d$ מתקיים
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \cdot \PP(Y \in T)
	\]
\end{definition}
\begin{exercise}
	יהיו $X_1, X_2 Geo(p)$ בלתי־תלויים ונגדיר גם $Z = X_1 + X_2$.
	\begin{enumerate}
		\item חשבו את ההתפלגות המשותפת של $X_1$ ו־$Z$.
		\item הראו ש־$X_1 \mid \{ Z = 1 \}$ מתפלג אחיד על $\{1, \dots, n - 1\}$.
	\end{enumerate}
\end{exercise}
\begin{solution}
	תחילה ניזכר שאם $W \sim Geo(p)$ אז $\supp(W) = \NN$ ו־$\PP(W = k) = {(1 - p)}^{k - 1} p$, שכן זוהי ההסתברות שלא הצלחנו $k - 1$ פעמים ובניסיון ה־$k$ הצלחנו, עבור איזושהי פעולה.
	\begin{enumerate}
		\item אנו מגדירים $X = (X_1, Z)$ וקטור מקרי ואנו רוצים לחשב את ההתפלגות שלו, נחשב את התומך
			\[
				\supp(X_1, Z) \subseteq \NN^2
			\]
			ישירות מההגדרה של הווקטור, אבל אנו יודעים כי תמיד $X_1 < Z$, וכן גם אם $m < n$
			\begin{align*}
				P_{(X_1, Z)}(m, n)
				& = \PP(X_1 = m, Z = n) \\
				& = \PP(X_1 = m, X_2 = n - m) \\
				& = \PP(X_1 = m) \cdot \PP(X_2 = m - n) \\
				& = {(1 - p)}^{m - 1} p {(1 - p)}^{n - m - 1} p \\
				& = p^2 {(1 - p)}^{n - 2}
			\end{align*}
			ולכן נסיק
			\[
				P_{(X_1, Z)}(n, n) = \begin{cases}
					0 & m \ge n \\
					p^2 {(1 - p)}^{n - 2} & m < n
				\end{cases}
			\]
		\item נבחן את $X_1 \mid \{ Z = n\}$ ונבין מה התומך.
			\[
				\supp(X_1 \mid \{Z = 1\})
				= \{1, \dots, n - 1\}
			\]
			שכן $Z$ מייצג סכום ולכן מהווה חסם ל־$X_1$ יחד עם $\supp(X_1) = \NN$. נעבור לחישוב ההתפלגות
			\[
				\PP(X = m \mid Z = n)
				= \frac{\PP(X = m, Z = n)}{\PP(Z = n)}
				= \frac{p^2 {(1 - p)}^{n - 2}}{\PP(Z = n)}
			\]
			אבל
			\[
				\PP(Z = n)
				= \PP(X_1 + X_2 = n)
				= \sum_{i = 1}^{n - 1}  \PP(X_1 = i, X_2 = n - i)
				= \sum_{i = 1}^{n - 1}  p^2 {(1 - p)}^{n - 2}
				= (n - 1) p^2 {(1 - p)}^{n - 2}
			\]
			זוהי קונבולוציה, לכן נוכל להסיק
			\[
				\PP(X_1 = m \mid Z = n)
				= \frac{p^2 {(1 - p)}^{n - 2}}{(n - 1) p^2 {(1 - p)}^{n - 2}}
				= \frac{1}{n - 1}
			\]
	\end{enumerate}
\end{solution}
\begin{exercise}
	מטילים מטבע הוגן, אם יצא 0 מטילים שוב מטבע הוגן ואם יצא 1 מטילים מטבע מוטה עם הסתברות $p$ ל־1. \\*
	נתחו את התפלגות ההטלה השנייה.
\end{exercise}
\begin{solution}
	נסמן $X$ תוצאת ההטלה הראשונה ו־$Y$ תוצאת ההטלה השנייה. \\*
	מהנתונים נסיק כי $X \sim Ber(\frac{1}{2})$. אנו גם יודעים כי גם $Y$ הוא בהתפלגות ברנולי כלשהי. \\*
	לבסוף אנו גם יודעים שמתקיים $Y \mid \{X = 0\} \sim Ber(\frac{1}{2})$ וגם ש־$Y \mid \{X = 1\} \sim Ber(p)$, לכן
	\[
		\PP(Y = 1)
		= \PP(Y = 1 \mid X = 0) \PP(X = 0) + \PP(Y = 1 \mid X = 1) \PP(X = 1)
		= \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot p
		= \frac{1}{4} + \frac{p}{2}
	\]
\end{solution}
\begin{exercise}
	יהיו $X \sim Ber(p)$ ו־$Y \sim Ber(q)$ בלתי־תלויים. \\*
	חשבו את ההתפלגות של $X \cdot Y$.
\end{exercise}
\begin{solution}
	נתחיל ונראה כי
	\[
		\supp(X Y) = \{0, 1\},
	\]
	וכן גם $XY$ בהתפלגות ברנולי כלשהי, אך
	\[
		\PP(XY = 1)
		= \PP(X = 1, Y = 1)
		= \PP(X = 1) \PP(Y = 1)
		= pq
	\]
	ולכן $XY \sim Ber(pq)$.
\end{solution}

\section{שיעור 10 --- 28.11.2024}
\subsection{התפלגות תחת התניה}
\begin{definition}[התפלגות משתנה מקרי בהינתן מאורע]
	יהי $X$ משתנה מקרי ו־$A$ מאורע כך ש־$\PP(A) > 0$ אז אפשר לדבר על התפלגות $X$ בהינתן $A$.
	זוהי ההתפלגות של $X$ תחת $\PP_A$ במקום $\PP$.
	במקרה זה
	\[
		\PP_{X \mid A}(S) = \PP_A(X \in S) = \PP(\{X \in S\} \mid A)
	\]
\end{definition}
\begin{proposition}
	אם $X \overset{d}{=} Y$ ו־$S \in \mathcal{F}_\RR$ כך ש־$\PP(X \in S) > 0$ וכן $\PP(Y \in S) > 0$, \\*
	אז $X \mid X \in S \overset{d}{=} Y \mid Y \in S$.
\end{proposition}
\begin{example}
	נניח ש־$X, Y \sim U([6])$ ו־$S = [3, \infty)$ אז
	\[
		X \mid X \in S \sim U(\{3, 4, 5, 6\}),
		\qquad
		Y \mid Y \in S \sim U(\{3, 4, 5, 6\})
	\]
\end{example}
\begin{definition}[אי־תלות משתנים מקריים]
	$X$ ו־$Y$ נקראים בלתי־תלויים אם לכל $S, T \in \mathcal{F}_\RR$ המאורעות $X \in S, Y \in T$ בלתי־תלויים. \\*
	הגדרה זו שקולה להגדרה שמתקיים
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \cdot \PP(Y \in T)
	\]
\end{definition}
\begin{proposition}
	אם $X$ ו־$Y$ משתנים מקריים בדידים אז $X$ ו־$Y$ בלתי־תלויים אם ורק אם לכל $s, t \in \RR$ מתקיים ש־$X = s$ ו־$Y = t$ בלתי־תלויים. \\*
	טענה זו שקולה לטענה שמתקיים
	\[
		\PP(X = s, Y = t) = \PP(X = s) \cdot \PP(Y = t)
	\]
\end{proposition}
\begin{proof}
	הכיוון הראשון הוא טריוויאלי מבחירת יחידונים ושימשו בהגדרה, לכן נניח את הכיוון השני ונראה כי מתקיים לכל $S, T \in \mathcal{F}_\RR$ מתקיים
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \PP(Y \in T)
	\]
	נבחין כי
	\begin{align*}
		\PP(X \in S, Y \in T)
		& = \PP(X \in S \cap \supp(X), Y \in T \cap \supp(Y)) \\
		& = \sum_{\substack{s \in S \cap \supp(X) \\ t \in T \cap \supp(Y)}} \PP(X = s, Y = t) \\
		& = \sum_{\substack{s \in S \cap \supp(X) \\ t \in T \cap \supp(Y)}} \PP(X = s) \PP(Y = t) \\
		& = \sum_{s \in S \cap \supp(X)} \left( \sum_{t \in T \cap \supp(Y)} \PP(X = s) \PP(Y = t) \right) \\
		& = \left( \sum_{s \in S \cap \supp(X)} \PP(X = s) \right) \left( \sum_{t \in T \cap \supp(Y)} \PP(Y = t) \right) \\
		& = \PP(X \in S) \PP(Y \in T)
	\end{align*}
\end{proof}
\begin{proposition}
	התפלגות $X$ ו־$X + Y$ ו־$Y$ בלתי־תלויים קובע ביחידות את ההתפלגות המשותפת.
\end{proposition}
\begin{proof}[הוכחה עבור בדידים]
	$p_X$ ו־$p_Y$ בלתי־תלויים קובע את $p_{(X, Y)}(s, t) = p_X(s) p_Y(t)$.
\end{proof}
\begin{proposition}
	$X, Y, Z$ משתנים מקריים בדידים ונניח שלכל $s \in \supp(X)$ מתקיים $Y \mid X = s \overset{d}{=} Z$,
	אז $Y \overset{d}{=} Z$ ו־$X, Y$ בלתי־תלויים.
\end{proposition}
\begin{proof}
	מנוסחת ההסתברות השלמה נובע
	\begin{align*}
		\PP(Y = t)
		& = \sum_{s \in \supp(X)} \PP(X = s) \PP(Y = t \mid X = s) \\
		& = \sum_{s \in \supp(X)} \PP(X = s) \PP(Z = t) \\
		& = \PP(Z = t)
	\end{align*}
	עבור החלק השני נבחין כי
	\[
		\PP(X = s, Y = t)
		= \PP(X = s) \PP(Y = t \mid X = s)
		= \PP(X = s) \PP(Z = t)
		= \PP(X = s) \PP(Y = t)
	\]
\end{proof}
\begin{proposition}
	אם $X, Y$ משתנים מקריים בלתי־תלויים ו־$f, g \in \mathcal{F}_{\RR \to \RR}$ אז $f(X), g(Y)$ בלתי־תלויים.
\end{proposition}
\begin{proof}
	צריך להראות שלכל $S, T \in \mathcal{F}_{\RR}$ מתקיים
	\[
		\PP(f(X) \in S, g(Y) \in T) = \PP(f(X) \in S) \PP(g(Y) \in T)
	\]
	אבל ראינו כבר כי
	\[
		\PP(f(X) \in S, g(Y) \in T) = \PP(X \in f^{-1}(S), Y \in g^{-1}(T))
	\]
	אבל גם
	\[
		\PP(X \in f^{-1}(S), Y \in g^{-1}(T)) = \PP(f(X) \in S) \PP(g(Y) \in T)
	\]
\end{proof}
\begin{example}
	$X$ ו־$Y$ בלתי־תלויים אז $X^2, \frac{1}{Y}$ בלתי־תלויים, זאת שכן $\PP(X = 1, Y = 1) = \frac{1}{2} \ne \frac{1}{2} \cdot \frac{1}{2}$. \\*
	בכיוון ההפוך אם $X = Y \sim Ber(\frac{1}{2})$ לא בלתי־תלויים אבל אם $f(x) = g(y) = 6$ גוררים ש־$f(X)$ ו־$g(Y)$ הם כמעט תמיד 6 ולכן בלתי־תלויים.
\end{example}
\begin{definition}[קבוצת משתנים מקריים בלתי־תלויה]
	יהיו $X_1, \dots, X_n$ משתנים מקריים, אז הם יקראו בלתי־תלויים אם לכל $S_1, \dots, S_n \in \mathcal{F}_\RR$ המאורעות ${\{ X_i \in S_i \}}_{i \in [n]}$ הם בלתי־תלויים.
\end{definition}
\begin{example}
	אם $X, Y, Z$ הם בלתי־תלויים, האם גם $X + Y$ ו־$Z$ בלתי־תלויים?
	אנו צריכים להראות ש־$\PP(X + Y = s, Z = t) = \PP(X + Y = s) \PP(Z = t)$ עבור כל $s \in \{0, 1, 2\}, t \in \{0, 1\}$. \\*
	נבחר לדוגמה את $\PP(X + Y = 1, Z = 1) = \PP(X = 0, Y = 1, Z = 1) + \PP(X = 1, Y = 0, Z = 1) = \frac{1}{8} + \frac{1}{8}$ ונוכל להמשיך כך ולראות שהטענה אכן מתקיימת.
\end{example}
\begin{exercise}
	מאורעות $A_1, \dots, A_n$ הם בלתי־תלויים אם ורק אם $1_{A_1}, \dots, 1_{A_n}$ הם בלתי־תלויים.
\end{exercise}
\begin{proposition}
	$X_1, \dots, X_n$ משתנים מקריים בלתי־תלויים ונניח שיש אינדקסים $1 = i_0 < i_1 < \cdots < i_k = n$. \\*
	נגדיר $Y_0 = (X_{i_0}, \dots, X_{i - 1}), \dots. Y_k = (X_{i_{k - 1}}, \dots, X_{i_k})$.
\end{proposition}

\section{שיעור 11 --- 3.12.2024}
\subsection{אי־תלות משתנים מקריים}
נמשיך עם מהלך ההרצאה הקודמת.
\begin{proposition}
	יהיו $X_1, \dots, X_n$ משתנים מקריים (יכולים להיות גם וקטורים מקריים ללא השפעה על ההוכחה) בלתי־תלויים,
	ויהיו $1 = b_0 < b_1 < \cdots < b_k = n$ ונגדיר $Y_1 = (X_{b_0 + 1}, \dots, X_{b_1}), \dots, Y_k = (X_{b_{k - 1} + 1}, \dots, X_{b_k})$. \\*
	אז $Y_1, \dots, Y_k$ בלתי־תלויים. \\*
	כדוגמה, $X_1, \dots, X_7 \rightarrow \overbrace{(X_1, X_2, X_3)}^{Y_1}, \overbrace{(X_4, X_5)}^{Y_2}, \overbrace{(X_6, X_7)}^{Y_3}$.
\end{proposition}
\begin{proof}[הוכחה במקרה הבדיד]
	$Y_i$ הוא וקטור מקרי ממימד $b_i - b_{i - 1}$, צריך להוכיח שלכל $s_1, \dots, s_k$ כך ש$s_1 \in \RR^{b_i - b_{i - 1}}$ מתקיים
	\[
		\PP(\forall i \in k, Y_i = s_i) = \prod_{i = 1}^k \PP(Y_i = s_i)
	\]
	נניח ש־$s_i = (a_{i 1}, \dots, a_{i d_i})$ ולכן נסיק מחוסר התלות של $X_i$
	\[
		\prod_{i = 1}^k \PP(Y_i = s_i)
		= \prod_{i = 1}^k \PP(\forall 1 \le j \le d_i, X_{b_{i - 1} + j} = a_{i j})
		= \prod_{i = 1}^k \prod_{j = 1}^{d_i} \PP(X_{b_{i - 1} + j} = a_{i j})
	\]
	אבל
	\[
		PP(\forall i \in k, Y_i = s_i)
		= \PP(\forall j = X_j = c_j)
		= \prod_{j = 1}^h \PP(X_j = c_j)
		= \prod_{i = 1}^k \prod_{j = 1}^{d_i} \PP(X_{b_{i - 1} + j} = a_{i j})
	\]
	עבור
	\[
		c = (\overbrace{a_{11}, \dots, a_{1d_1}}^{s_1}, \dots, \overbrace{a_{k1}, \dots, a_{kd_1}}^{s_k})
	\]
	ומצאנו כי השוויון אכן מתקיים ו־$Y_1, \dots, Y_k$ בלתי־תלויים.
\end{proof}
\begin{conclusion}
	$X_1, \dots, X_n$ בלתי־תלויים ו־$0 = b_0 < b_1 < \cdots < b_k = n - 1$ ו־$d_i = b_i - b_{i - 1}$ כך ש־$Y_i = (X_{b_{i - 1} + 1}, \dots, X_{b_i})$ ו־$f_1, \dots, f_k$ כאשר $f_i : \RR^{d_i} \to \RR$,
	אז ${\{f_i(Y_i)\}}_{i = 1}^k$ בלתי־תלויים.
\end{conclusion}
\begin{example}
	אם $X_1, \dots, X_n$ בלתי־תלויים אז גם $X_1 + X_2, \dots, X_3 + X_4, \dots, X_{n - 1} + X_{n}$ כנביעה מהמסקנה, \\*
	באופן דומה גם $X_1 + X_2 + X_3, \dots$ בלתי־תלויים. \\*
	כרעיון אנו יכולים לחלק משתנים מקריים לווקטורים בלתי־תלויים, ואז להפעיל פונקציה, שלא משנה את חוסר התלות, על כל הקבוצה.
\end{example}
\begin{example}
	נניח ש־$A_1, \dots, A_5$ מאורעות בלתי־תלויים, אז המאורעות $(A_5 \cap A_4) \cup (A_1 \cap A_2) \cup A_3^C, A_4 \setminus A_5$ בלתי־תלויים,
	זאת אנו עושים על־ידי שימוש במשתנים המקריים האופייניים של $A_i$ ושימוש במסקנה.
\end{example}
נבחין כי דרישת סופיות קבוצת המשתנים המקריים היא לא תנאי הכרחי
\begin{definition}[קבוצה בת־מניה של משתנים מקריים בלתי־תלויים]
	$X_1, X_2, \dots$ משתנים מקריים הם בלתי־תלויים אם לכל $n \in \NN$ מתקיים $X_1, \dots, X_n$ בלתי־תלויים.
\end{definition}
\begin{proposition}
	אם ${\{X_n\}}_{n \in \NN}$ בלתי־תלויים ו־$S_n \in \mathcal{F}_\RR$ לכל $n \in \NN$ אז
	\[
		\PP(\forall n \in \NN, X_n \in S_n)
		= \prod_{n \in \NN} \PP(X_n \in S_n)
	\]
\end{proposition}
נשאל את עצמנו אם מצב זה בכלל אפשרי, נראה טענה ללא הוכחה שעונה על שאלה זו.
\begin{proposition}
	קיימת סדרת משתנים מקריים כזאת שכולם $Ber(\frac{1}{2})$.
\end{proposition}
\begin{proposition}
	סדרה כזו בהכרח לא מוגדרת על מרחב בדיד.
\end{proposition}
\begin{proof}
	נניח ש־$X_1, \dots$ סדרה כזו ונניח ש־$(\Omega, \mathcal{F}, \PP)$ בדיד. \\*
	נניח ש־$\omega_0 \in \Omega$ ו־$\PP(\{\omega_0\}) > 0$, נסמן $s_i = X_i(\omega_0)$, אז
	\[
		0 \xleftarrow[n \to 0]{} {\left(\frac{1}{2}\right)}^n = \PP(\forall i \le n, X_i = s_i) \ge \PP(\forall i \in \NN, X_i = s_i) \ge \PP(\{\omega_0\}) > 0
	\]
	וקיבלנו סתירה לקיום $\omega_0$ כזה.
\end{proof}

\subsection{התפלגות גאומטרית}
ניזכר בהגדרה\ \ref{geometric_distribution}, אשר מדברת על ניסוי שאנו עושים שוב ושוב עד שאנו מצליחים.
\begin{proposition}
	אם $X_1, X_2, \dots$ משתנים מקריים בלתי־תלויים המתפלגים $Ber(p)$ עבור $0 < p < 1$, \\*
	ונסמן $Y = \min \{ k \mid X_k = 1 \}$, אז $Y \sim Geo(p)$.
\end{proposition}
נבחין כי $Y$ מייצג בחירת המופע הראשון של $1$ בהתפלגות ברנולי, נזכיר כי היא מייצגת הגרלה יחידה, לדוגמה הטלת מטבע בודד. נעבור להוכחה.
\begin{proof}
	המאורע $Y = l$ הוא המאורע $X_1 = X_2 = \cdots = X_{l - 1} = 0$ ו־$X_l = 1$, אבל שלו הם משתנים בלתי־תלויים, לכן
	\[
		\PP(X_1 = \cdots = X_{l - 1} = 0, X_l = 1)
		= \PP(X_1 = 0) \cdots \PP(X_{l - 1}) \PP(X_l = 1)
		= {(1 - p)}^{l - 1} p
	\]
	זוהי התפלגות גאומטרית.
\end{proof}
\begin{remark}
	הסכום הוא
	\[
		\sum_{l = 1}^{\infty} {(1 - p)}^{l - 1} p = 1
	\]
	ולכן המקרה שבו אין מינימום כפי שהגדרנו לא רלוונטי להגדרה, וניתן להתעלם ממנו.
\end{remark}
מה יקרה אם נגדיר ככה $Y_1 = Y$ ו־$Y_2$ המשתנה המקרי שעבורו קיבלנו 1 בפעם השנייה וכן הלאה, אז $Y_2 - Y_1$ וסדרת החיסורים היא בלתי תלויה־אף היא, תוצאה אינטואיטיבית אך לא מובנת מאליו.

\section{תרגול 6 --- 5.12.2024}
\subsection{שאלות בנושאי משתנים מקריים בלתי־תלויים}
\begin{exercise}
	שתיים מטילות $n$ מטבעות הוגנים כל אחת באופן בלתי־תלוי. \\*
	מה ההסתברות שהן קיבלו אותו מספר תוצאות עץ?
\end{exercise}
\begin{solution}
	נגדיר $X_i \sim Ber(\frac{1}{2})$ ההטלה ה־$i$ של הראשונה, ו־$Y \sim Ber(\frac{1}{2})$ ההטלה ה־$i$ של השנייה, ונגדיר
	\[
		X = \sum_{i = 1}^{n} X_i,
		\qquad
		Y = \sum_{i = 1}^{n} Y_i
	\]
	משתנים המייצגים את מספר הטלות העץ של כל אחת מהשתיים. \\*
	אבל זאת דרך מורכבת לפתור את השאלה הזאת, נגדיר במקום זה $X, Y \sim Bin(n, \frac{1}{n})$, את ההוכחה לשקילות נראה בהרצאה הקרובה. \\*
	מהגדרה\ \ref{binominal_distribution} נוכל להסיק $\supp X = \{0, \dots, n\}$, ואנו מבקשים לחשב את $\PP(X = Y)$, נחשב על־ידי
	\[
		\PP(X = Y)
		= \sum_{k = 0}^{n} \PP(X = k = Y)
		= \sum_{k = 0}^{n} \PP(X = k) \PP(Y = k)
		= \sum_{k = 0}^{n} \binom{n}{k} \frac{1}{2^n} \binom{n}{k} \frac{1}{2^n}
		= \frac{1}{2^{2n}} \sum_{k = 0}^{n} {\binom{n}{k}}^2
	\]
\end{solution}
\begin{proposition}\label{binominal_distributions_summation_proposition}
	יהיו $X \sim Bin(n, p)$ ו־$Y \sim Bin(m, p)$ בלתי־תלויים, אז $X + Y \sim Bin(n + m, p)$.
\end{proposition}
\begin{proof}
	נבחין תחילה ש־$\supp X + Y = \{0, \dots, n + m\}$ וכן
	\begin{align*}
		\PP(X + Y = k)
		& = \sum_{i = 0}^{n + m} \PP(X = i, Y = k - i) \\
		& = \sum_{i = 0}^{n + m} \PP(X = i) \PP(Y = k - i) \\
		& = \sum_{i = 0}^{n + m} \binom{n}{i} p^i {(1 - p)}^{n - i} \binom{m}{k - i} p^{k - i} {(1 - p)}^{m - k + i} \\
		& = p^k {(1 - p)}^{n + m - k} \sum_{i = 0}^{n + m} \binom{n}{i} \binom{m}{k - i} \\
		& = p^k {(1 - p)}^{n + m - k} \binom{n + m}{k}
	\end{align*}
	כאשר עלינו להוכיח את השוויון האחרון, זאת נעשה בתרגיל הבא.
\end{proof}
\begin{exercise}[זהות ונדרמונדה]
	מתקיים
	\[
		\sum_{i = 0}^{n + m} \binom{n}{i} \binom{m}{k - i}
		= \binom{n + m}{k}
	\]
\end{exercise}
\begin{solution}
	נבחין כי $\binom{n + k}{k}$ הוא קומבינטורית שקול לבחירה $k$ ערכים מתוך קבוצה בגודל $m$ וקבוצה בגודל $n$. \\*
	במקביל $\binom{n}{i} \binom{m}{k - i}$ הוא היכולת לבחור $i$ מתוך $m$ ועוד השארית מ־$k$ מהקבוצה השנייה. \\*
	נבחין כי זוהי הוכחה קומבינטורית ואפשרי להוכיח גם אלגברית את השוויון הנתון.
\end{solution}
\begin{proposition}
	ניזכר בהגדרה $\ref{poasson_distribution}$ יהיו $X \sim Pois(\lambda)$ ו־$Y \sim Pois(\lambda + \eta)$ בלתי־תלויים, אז $X + Y \sim Pois(\lambda + \eta)$.
\end{proposition}
ניזכר בהתפלגות פואסון עם דוגמה.
אם מגיעים לבית־חולים בממוצע $\lambda = 10$ אנשים ביום, אז התפלגות פואסון היא השאלה כמה אנשים הגיעו ביום ספציפי לבית־החולים. \\*
בהתאם השאלה שאנו שואלים מדברת על מקרה שבו יש שני בתי־חולים ואנו שואלים על כמה אנשים הגיעו ביום. \\*
ברור לנו אם כן שטענה זו הגיונית, נעבור להוכחה.
\begin{proof}
	תחילה $\supp(X + Y) = \NN \cup \{0\}$ ונניח $k \in \supp(X + Y)$, ונחשב
	\begin{align*}
		\PP(X + Y = k)
		& = \sum_{n = 0}^{\infty} \PP(X = n) \PP(Y = k - n) \\
		& = \sum_{n = 0}^{k} \frac{e^{-\lambda} \lambda^n}{n!} \frac{e^{-\eta} \eta^{k - n}}{(k - n)!} \\
		& = e^{-(\lambda + \eta)} \sum_{n = 0}^{k} \frac{\lambda^n \eta^{k - n}}{n! (k - n)!} \\
		& = \frac{e^{-(\lambda + \eta)}}{k!} \sum_{n = 0}^{k} \frac{k!}{n! (k - n)!} \lambda^n \eta^{k - n} \\
		& = \frac{e^{-(\lambda + \eta)}}{k!} \sum_{n = 0}^{k} \binom{k}{n} \lambda^n \eta^{k - n} \\
		& = \frac{e^{-(\lambda + \eta)}}{k!} {(\lambda + \eta)}^k
	\end{align*}
\end{proof}
\begin{proposition}
	נניח $X \sim Pois(\lambda)$ ו־$Y$ משתנה מקרי המקיים
	\[
		\forall n \in \NN \cup \{0\},\ Y \mid \{X = n\} \sim Bin(n, p)
	\]
	אז $Y \sim Pois(\lambda p)$.
\end{proposition}
\begin{proof}
	הפעם $\supp Y = \NN \cup \{0\}$. \\*
	נניח ש־$k \in \NN \cup \{0\}$ ונחשב את $\PP(Y = k)$ על־ידי שימוש בנוסחת ההסתברות השלמה.
	\begin{align*}
		\PP(Y = k)
		& = \sum_{n = 0}^{\infty} \PP(X = n) \PP(Y = k \mid X = n) \\
		& = \sum_{n = 0}^{\infty} \frac{e^{-\lambda} \lambda^n}{n!} \binom{n}{k} p^k {(1 - p)}^{n - k} \\
		& = \sum_{n = k}^{\infty} \frac{e^{-\lambda} \lambda^n}{n!} \frac{n!}{k! (n - k)!} p^k {(1 - p)}^{n - k} \\
		& = \frac{e^{-\lambda} p^k}{k!} \sum_{n = k}^{\infty} \frac{\lambda^n {(1 - p)}^{n - k}}{(n - k)!} \\
		& = \frac{e^{-\lambda} p^k}{k!} \sum_{m = 0}^{\infty} \frac{\lambda^{m + k} {(1 - p)}^m}{m!} \\
		& = \frac{e^{-\lambda} p^k \lambda^k}{k!} \sum_{m = 0}^{\infty} \frac{\lambda^m {(1 - p)}^m}{m!} \\
		& = \frac{e^{-\lambda} p^k \lambda^k}{k!} e^{\lambda(1 - p)} \\
		& = \frac{e^{-\lambda p} p^k {(p \lambda)}^k}{k!}
	\end{align*}
\end{proof}

\section{שיעור 12 --- 5.12.2024}

\subsection{התפלגות גאומטרית}
\begin{proposition}
	אם $X_i \sim Ber(p)$ אז $Y = \min{k \mid X_k = 1}$ אז $Y \sim Geo(p)$.
\end{proposition}
\begin{theorem}[תכונת חוסר הזיכרון]
	$X$ משתנה מקרי הנתמך על $\NN$, ב־$\PP(X > 1) > 0$.
	אז התנאים הבאים שקולים:
	\begin{enumerate}
		\item $X \sim Geo(p)$ עבור $0 < p < 1$ כלשהו.
		\item לכל $l \in \NN$ מתקיים $X \overset{d}{=} X - l \mid X > l$. כלומר $\PP(X > l \mid X - l > 0) = \PP(X = k)$ לכל $l \in \NN$.
		\item $X \overset{d}{=} X - 1 \mid X > 1$. כלומר לכל $S \in \mathcal{F}_\RR$ מתקיים $\PP(X \in S) = \PP(X - 1 \in S \mid X > 1)$.
	\end{enumerate}
\end{theorem}
נראה טענה קודמת שתעזור לנו בהוכחת המשפט
\begin{proposition}
	אם $X$ משתנה מקרי שנתמך על $\NN$ אז התנאים הבאים שקולים:
	\begin{enumerate}
		\item $X \sim Geo(p)$
		\item $\PP(X > n) = {(1 - p)}^n$
	\end{enumerate}
\end{proposition}
\begin{proof}
	$1 \implies 2$:
	\[
		\PP(X > n)
		= \sum_{k = n + 1}^{\infty} \PP(X = k)
		= \sum_{k = n + 1}^{\infty} {(1 - p)}^{k - 1} p
		= {(1 - p)}^n \sum_{l = 1}^{\infty} {(1 - p)}^{l - 1} p
		= {(1 - p)}^n
	\]

	$2 \implies 1$:
	\[
		\forall n \in \NN,\ \PP(X = n)
		= \PP(X > n - 1) - \PP(X > n)
		= {(1 - p)}^{n - 1} - {(1 - p)}^n
		= {(1 - p)}^{n - 1} (1 - (1 - p))
	\]
\end{proof}
\begin{proof}[הוכחת המשפט]
	$1 \implies 2$:
	נניח $l, k \in \NN$
	\[
		{(1 - p)}^{k - 1} p
		= \PP(X = k)
		= \PP(X - l = k \mid X - l > 0)
		= \frac{\PP(X = l + k)}{\PP(X > l)}
		= \frac{{(1 - p)}^{l + k - 1} p}{{(1 - p)}^l}
		= {(1 - p)}^{l - 1} p
	\]

	$2 \implies 3$ מיידי.

	$3 \implies 1$:
	נסמן $p = \PP(X = 1)$ ונראה ש־$X \sim Geo(p)$ על־ידי כך שנראה $\forall n \in \NN,\ \PP(X < n) = {(1 - p)}^n$ והטענה. \\*
	נוכיח באינדוקציה. עבור $n = 1$ נובע $\PP(X > 1) = 1 - \PP(X = 1) = 1 - p$. \\*
	נניח שהטענה נכונה ל־$n$ ונראה
	\[
		\PP(X > n + 1)
		= \PP(X > 1) \PP(X > n + 1 \mid X > 1)
		= (1 - p) \PP(X - 1 > n \mid X > 1)
		= (1 - p) \PP(X > n)
		= (1 - p) {(1 - p)}^n
	\]
	והשלמנו את מהלך האינדוקציה.
\end{proof}

\subsection{התפלגות בינומית}
נעבור לדבר על התפלגויות בינומיות כפי שהגדרנו בהגדרה\ \ref{binominal_distribution}.
\begin{proposition}
	אם $X_1, \dots, X_n$ משתנים מקריים בלתי תלויים מתפלגים $Ber(p)$, אז $Y = \sum_{i = 1}^n X_i$ מתפלג $Bin(n, p)$.
\end{proposition}
\begin{proof}
	\begin{align*}
		\PP(Y = k)
		& = \sum_{\substack{v \in {\{0, 1\}}^n \\ \sum v_i = k}} \PP(X_1 = v_1, \dots, X_n = v_n) \\
		& = \sum_{\substack{v \in {\{0, 1\}}^n \\ \sum v_i = k}} \prod_{i = 1}^k \PP(X_i = v_i) \\
		& = \sum_{\substack{v \in {\{0, 1\}}^n \\ \sum v_i = k}} p^k {(1 - p)}^{n - k} \\
		& = \binom{n}{k} p^k {(1 - p)}^{n - k}
	\end{align*}
\end{proof}
ניזכר בטענה\ \ref{binominal_distributions_summation_proposition} ונוכיח אותה הפעם בדרך פורמלית ולא על־ידי אינטואיציה.
\begin{proof}
	נניח שיש $Z_1, \dots, Z_{n + m}$ משתנים מקריים בלתי־תלויים $Ber(p)$ כך שמתקיים
	\[
		X' = \sum_{i = 1}^n Z_i,
		\qquad
		Y' = \sum_{i = n + 1}^{n + m} Z_i
	\]
	אז $X' + Y' = \sum_{i = 1}^{n + m} Z_i$, לפי הטענה
	\[
		X' \sim Bin(n, p),
		\qquad
		Y' \sim Bin(m, p)
	\]
	וכן
	\[
		X' + Y' \sim Bin(n + m, p)
	\]
	לפי הטענה מההרצאה הקודמת $X'$ ו־$Y'$ בלתי־תלויים. \\*
	אבל $X \overset{d}{=} X'$ ו־$Y \overset{d}{=} Y'$ ו־$X$ ו־$Y$ בלתי־תלויים וגם $X', Y'$ בלתי־תלויים, אז $(X', Y') \overset{d}{=} (X, Y)$.
	לבסוף נובע $X + Y \overset{d}{=} X' + Y'$.
\end{proof}

\subsection{התפלגות פואסון}
נעבור להתפלגות\ \ref{poasson_distribution} ונבחן אותה
\begin{proposition}
	נניח $\lambda > 0$ ונגדיר $X_n \sim Bin(n, \frac{\lambda}{n})$ עבור $n > \lambda$, \\*
	אז לכל $k \in \NN$ מתקיים
	\[
		\PP(X_n = k) \xrightarrow[n \to \infty]{} \PP(Y = k)
	\]
	עבור $Y \sim Pois(\lambda)$.
\end{proposition}
\begin{proof}
	\[
		\PP(X_n = 0)
		= \binom{n}{0} {\left(\frac{\lambda}{n}\right)}^0 {\left(1 - \frac{\lambda}{n}\right)}^n \xrightarrow[n \to \infty]{} e^{-\lambda}
	\]
	באופן דומה
	\[
		\PP(X_n = 1)
		= \binom{n}{1} {\left(\frac{\lambda}{n}\right)}^1 {\left(1 - \frac{\lambda}{n}\right)}^n \cdot {\left(1 - \frac{\lambda}{n}\right)}^{-1} \xrightarrow[n \to \infty]{} e^{-\lambda} \cdot \lambda
	\]
	ונעבור למקרה הכללי
	\[
		\PP(X_n = k)
		= \overbrace{\binom{n}{k} {\left(\frac{\lambda}{n}\right)}^k}^{\frac{n!}{k! (n - k)!} \frac{\lambda^k}{n^k} \to \frac{\lambda^k}{k!}} {\left(1 - \frac{\lambda}{n}\right)}^n \cdot {\left(1 - \frac{\lambda}{n}\right)}^{-k}
		\xrightarrow[n \to \infty]{} \frac{\lambda^k}{k!} e^{-\lambda} \cdot 1
	\]
\end{proof}
נחזור לטענה שראינו בתרגיל הבית:
\begin{proposition}
	אם $X \sim Pois(\lambda_1)$ ו־$Y \sim Pois(\lambda_2)$ בלתי־תלויים אז $X + Y \sim Pois(\lambda_1 + \lambda_2)$.
\end{proposition}
הפעם אפשר יהיה להוכיחה על־ידי הטענה החדשה שראינו.
\begin{example}
	נניח ${\{X_n\}}_{n \in \NN}$ משתנים מקריים בלתי־תלויים $U([26])$ המייצגים את קבלת כל אחת מהאותיות באנגלית, אז
	\[
		\PP(X_1 = 1, \dots, X_{1000} = 1) = \frac{1}{26^{1000}} = \PP(X_{1001} = 1, \dots, X_{2000} = 1)
	\]
	ההסתברות שיצא טקסט שמורכב מהאות $a$ 1000 פעמים.
	הרעיון הוא שאין קשר בין המיקום שבו אנו שואלים עם הטקסט הופיע, אלא רק מהו אורך הטקסט, בהתאם
	\[
		\PP(\lnot \exists k,\ X_{1000 k + 1} = 1, \dots, X_{1000k + 1000} = 1)
		= {(1 - \frac{1}{26^{1000}})}^n
		\to 0
	\]
	ולכן בסופו של דבר הטקסט הזה בהכרח יופיע.
\end{example}

\section{שיעור 13 --- 10.12.2024}
\subsection{תוחלת}
\begin{definition}[תוחלת במשתנים מקריים בדידים]
	$X$ משתנה מקרי בדיד.
	ה\textbf{תוחלת} של $X$ היא
	\[
		\EE(X) = \sum_{s \in \RR} s \PP(X = s)
	\]
\end{definition}
\begin{remark}
	אם הטור $\sum_{s \in \RR} s \PP(X = s)$ לא מתכנס בהחלט, אז נאמר שאין תוחלת ל־$X$.
\end{remark}
\begin{example}
	נניח ש־$\Omega = [100]$ מרחב הסתברות אחיד, מייצג קבוצת תלמידים. \\*
	נגדיר $X(\omega)$ הציון של תלמיד $\omega$ במבחן. \\*
	אז $\EE(X)$ יהיה ממוצע הציונים בכיתה.
\end{example}
\begin{proposition}
	אם $X$ מוגדר על מרחב הסתברות בדידה, אז $\EE(X) = \sum_{\omega \in \Omega} X(\omega) \PP(\{\omega\})$.
\end{proposition}
\begin{proof}
	מההגדרה של מרחב הסתברות בדידה נוכל להשתמש בתומך ואז נובע
	\[
		\EE(X)
		= \sum_{s \in \RR} s \PP(X = s)
		= \sum_{s \in \RR} s \sum_{\substack{\omega \in \Omega \\ X(\omega) = s}} \PP(\{\omega\})
		= \sum_{s \in \RR} \sum_{\substack{\omega \in \Omega \\ X(\omega) = s}} X(\omega) \PP(\{\omega\})
		= \sum_{\omega \in \Omega} X(\omega) \PP(\{\omega\})
	\]
\end{proof}
\begin{proposition}
	אם $X_1, \dots, X_n$ משתנים מקריים בדידים ו־$f \in \mathcal{F}_{\RR^n \to \RR}$, $Y = f(X_1, \dots, X_n)$, אז
	\[
		\EE(Y) = \sum_{(s_1, \dots, s_n) \in \RR^n} f(s_1, \dots, s_n) \PP(X_1 = s_1, \dots, X_n = s_n)
	\]
\end{proposition}
\begin{proof}
	כמקודם נשתמש בתומך וכך נראה את השוויון.
	\begin{align*}
		\EE(Y)
		& = \sum_{s \in \RR} s \PP(Y = s) \\
		& = \sum_{s \in \RR} s \sum_{\substack{(s_1, \dots, s_n) \in \RR^n \\ f(s_1, \dots, s_n) = s}} \PP(X_1 = s_1, \dots, X_n = s_n) \\
		& = \sum_{s \in \RR} \sum_{\substack{(s_1, \dots, s_n) \in \RR^n \\ f(s_1, \dots, s_n) = s}} f(s_1, \dots, s_n) \PP(X_1 = s_1, \dots, X_n = s_n) \\
		& = \sum_{(s_1, \dots, s_n) \in \RR^n} f(s_1, \dots, s_n) \PP(X_1 = s_1, \dots, X_n = s_n)
	\end{align*}
\end{proof}
\begin{example}
	נניח $X \sim Ber(p)$ אז
	\[
		\EE(X) = 0 \cdot \PP(X = 0) + 1 \cdot \PP(X = 1) = p
	\]
\end{example}
\begin{example}
	אם $X \sim U([n])$ אז
	\[
		\EE(X) = \sum_{k = 1}^n k \cdot \PP(X = k)
		= \frac{1}{n} \sum_{k = 1}^{n} k
		= \frac{1}{n} \cdot \frac{n(n + 1)}{2}
		= \frac{n + 1}{2}
	\]
\end{example}
\begin{example}
	אם $X \sim Bin(n, p)$ אז
	\[
		\EE(X)
		= \sum_{k = 0}^{n} k \binom{n}{k} p^k {(1 - p)}^{n - k}
		= np
	\]
\end{example}
\begin{example}
	נניח $X \sim Poi(\lambda)$, אז
	\[
		\EE(X)
		= \sum_{k = 0}^\infty k e^{-\lambda} \frac{\lambda^k}{k!}
		= \sum_{k = 1}^\infty e^{-\lambda} \frac{\lambda^k}{(k - 1)!}
		= \lambda e^{-\lambda} \sum_{k = 1}^\infty \frac{\lambda^{k - 1}}{(k - 1)!}
		= \lambda e^{-\lambda} \sum_{k = 0}^\infty \frac{\lambda^k}{k!}
		= \lambda e^{-\lambda} e^\lambda
		= \lambda
	\]
\end{example}
ולבסוף גם
\begin{example}
	נניח $X \sim Geo(p)$ ולכן
	\[
		\EE(X)
		= \sum_{k = 1}^{\infty} k {(1 - p)}^{k - 1}
		= \frac{1}{p}
	\]
	את החישוב עצמו שמוכיח את הטענה הזאת נעשה בהמשך.
\end{example}
\begin{example}
	נניח $X \sim Geo(\frac{1}{2})$ ונגדיר $Y = 2^X$. \\*
	בהתאם $\PP(Y = 2) = \frac{1}{2}$ וכן $\PP(Y = 4) = \frac{1}{4}$ וכן הלאה, כך ש־$\PP(Y = 2^k) = \frac{1}{2^k}$.
	נחשב את התוחלת:
	\[
		\EE(Y)
		= \sum_{s \in \{2, 4, 8, \dots\}} 2^k \PP(Y = 2^k)
		= \sum_{k = 1}^{\infty} 2^k \frac{1}{2^k}
		= \sum_{k = 1}^{\infty} 1
		= \infty
	\]
	ולכן אין תוחלת.

	יכולנו להחליף את הגדרת $Y$ להיות $Y = {(-2)}^X$ והיינו מקבלים טור שלא מתכנס בכלל.
\end{example}
\begin{remark}
	אפשר להרחיב את התוחלת ותכונותיה למקרים אינסופיים, אנו לא נעשה זאת.
\end{remark}

\subsection{תכונות של תוחלת}
\begin{proposition}[תכונות של תוחלת]
	אם $X, Y$ משתנים מקריים בעלי תוחלת, אז:
	\begin{enumerate}
		\item אם $X \ge 0$ כמעט תמיד אז $\EE(X) \ge 0$.
			אם בנוסף $\PP(X > 0) > 0$ אז $\EE(X) > 0$.
		\item לינאריות: אם $a, b \in \RR$ אז אם $Z = aX + bY$ יש תוחלת והיא $\EE(Z) = a \EE(X) + b \EE(Y)$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item אם $X \ge 0$ כמעט תמיד אז כל המחוברים אי־שליליים ולכן $\EE(X) \ge 0$. \\*
			אם בנוסף $\PP(X > 0) > 0$ אז קיים $s > 0$ כך ש־$\PP(S = s) > 0$ ולכן $s \PP(X = s) > 0$ ולכן הסכום חיובי.
		\item נגדיר $f(x, y) = ax + by$ עבור $f \in \mathcal{F}_{\RR^2 \to \RR}$. אז
			\begin{align*}
				\EE(Z)
				& = \EE(f(x, y)) \\
				& = \sum_{(s, t) \in \RR^2} f(s, t) \PP(X = s, Y = t) \\
				& = \sum_{(s, t) \in \RR^2} (as + bt) \PP(X = s, Y = t) \\
				& = a \left(\sum_{(s, t) \in \RR^2} s \PP(X = s, Y = t)\right) + b \left(\sum_{(s, t) \in \RR^2} t \PP(X = s, Y = t)\right) \\
				& = a \left(\sum_{s \in \RR} \sum_{t \in \RR} s \PP(X = s, Y = t)\right) + b \left(\sum_{t \in \RR} \sum_{s \in \RR} t \PP(X = s, Y = t)\right) \\
				& = a \sum_{s \in \RR} s \PP(X = s) + b \sum_{t \in \RR} t \PP(Y = t) \\
				& = a \EE(X) + b \EE(Y)
			\end{align*}
	\end{enumerate}
\end{proof}
\begin{conclusion}
	אם $X$ ו־$Y$ משתנים מקריים בעלי תוחלת ו־$Y \le X$ כמעט תמיד, אז $\EE(Y) \le \EE(X)$.
\end{conclusion}
\begin{proof}
	נגדיר $Z = X - Y$ ואז $Z \ge 0$ כמעט תמיד ולכן $\EE(Z) \ge 0$ ואז $X = Y + Z$ ולכן $\EE(X) = \EE(Y) + \EE(Z) \ge \EE(Y)$.
\end{proof}
\begin{example}
	נעבור לראות את החישוב של תוחלת להתפלגות בינומית:
	נגדיר $X \sim Bin(n, p)$ ונגדיר $X_1, \dots, X_n$ משתנים מקריים $Ber(p)$, ויהי $X = \sum_{i = 1}^n X_i$, לכן $X \sim Bin(n, p)$ וכן
	\[
		\EE(X) = \sum_{i = 1}^n \EE(X_i) = np
	\]
\end{example}
\begin{definition}[תוחלת מותנית]
	$X$ משתנה מקרי ו־$A$ מאורע, כך ש־$\PP(A) > 0$, אז
	\[ 
		\EE(X \mid A)
		= \sum_{s \in \RR} s \PP(X = s \mid A)
	\]
\end{definition}
\begin{proposition}
	\[
		\EE(X \mid A) = \frac{\EE(X \cdot 1_A)}{\PP(A)}
	\]
\end{proposition}
\begin{proof}
	\[
		\EE(X \mid A)
		= \sum_{s \in \RR} s \PP(X = s \mid A)
		= \sum_{s \in \RR} s \frac{\PP(X = s, A)}{\PP(A)}
		= \sum_{s \in \RR} s \frac{\PP(X \cdot 1_A = s)}{\PP(A)}
	\]
	כאשר המעבר האחרון נובע מהגדרת המציין ובדיקה ידנית של המקרים בהם $s \in A, s \notin A$, כלומר
	\[
		\PP(X = s, A)
		= \PP(\{\omega \in \Omega \mid X(\omega) = s, \omega \in A\})
		= \PP(\{\omega \in \Omega \mid X(\omega) = s, 1_A(\omega) = 1\})
	\]
\end{proof}
\begin{proposition}
	אם $A_1, \dots, A_n$ חלוקה של $X$ משתנה מקרי בעל תוחלת, אז
	\[
		\EE(X) = \sum_{k = 1}^n \EE(X \cdot q_{A_k})
	\]
\end{proposition}
\begin{proof}
	מאותו מעבר כמו בהוכחה הקודמת נסיק
	\[
		X = \sum_{k = 1}^n X \cdot 1_{A_k}
	\]
	ואז משתמש בתכונת הלינאריות של תוחלות ונקבל את המבוקש.
\end{proof}
\begin{proposition}[נוסחת התוחלת השלמה]
	אם $A_1, \dots, A_n$ חלוקה ו־$\PP(A_k) > 0$ לכל $k$ אז
	\[
		\EE(X) = \sum_{k = 1}^n \PP(A_k) \EE(X \mid A_k)
	\]
\end{proposition}
\begin{proof}
	על־ידי הטענות הקודמות נובע
	\[
		\EE(X)
		= \sum_{k = 1}^n \EE(X \cdot 1_{A_k})
		= \sum_{k = 1}^n \PP(A_k) \EE(X \mid A_k)
	\]
\end{proof}
\begin{example}
	נניח $X \sim Geo(p)$ ו־$A_1 = \{X = 1\}$ וכן $A_2 = \{X > 1\}$, אז $\{A_1, A_2\}$ הם אכן חלוקה של $\Omega$. \\*
	נחשב את התוחלת:
	\[
		\EE(X)
		= \PP(A_1) \EE(X \mid A_1) + \PP(A_2) \EE(X \mid A_2)
		= p \cdot 1 + (1 - p) \cdot \EE(X \mid X > 1)
	\]
	אבל אז מתכונת חוסר הזיכרון
	\[
		p \cdot 1 + (1 - p) \cdot \EE(X \mid X > 1)
		= p + (1 - p) \cdot (\EE(X - 1 \mid X > 1) + \EE(1 \mid X > 1))
		= p + (1 - p) \cdot (\EE(X) + 1)
	\]
	ולכן קיבלנו את השוויון $\EE(X) = p + (1 - p)(\EE(X) + 1)$, ממנו נובע $0 = p + (1 - p) - p \EE(X)$, לכן $\EE(X) = \frac{1}{p}$.
\end{example}

\section{תרגול 7 --- 12.12.2024}
\subsection{שאלות ותכונות של תוחלות}
באנגלית תוחלת היא Expectancy, מילה שמתארת בצורה יותר נאמנה את מושג התוחלת.
\begin{example}
	נניח ש־$X \sim Geo(p)$, ונחשב את התוחלת של $X$. \\*
	יש להעריך את הטור $\sum_{n = 1}^\infty n \PP(X = n)$.
	\[
		\sum_{k = 1}^\infty k \PP(X = k)
		= \sum_{n = 1}^\infty k p {(1 - p)}^{k - 1}
		= p \sum_{n = 1}^\infty k {(1 - p)}^{k - 1}
	\]
	נגדיר $f \in (0, 1)$ את $f(q) = \frac{1}{1 - q} = \sum_{k = 0}^{\infty} q^k$ ולכן $f$ אנליטית ומתכנסת במידה שווה על תת־קבוצות של התחום שלה.
	אז
	\[
		\frac{1}{{(1 - q)}^2}
		= f'(q)
		= \sum_{k = 0}^{\infty} k q^{k - 1}
		= \sum_{k = 1}^{\infty} k q^{k - 1}
	\]
	אם נציב $q = 1 - p$ אז נובע
	\[
		p \sum_{n = 1}^\infty k {(1 - p)}^{k - 1}
		= p \cdot \frac{1}{{(1 - (1 - p))}^2}
		= \frac{1}{p}
	\]
\end{example}
\begin{example}
	אם $X \sim Bin(n, p)$ ואם נגדיר $Y \sim Bin(n - 1, p)$ אז
	\begin{align*}
		\EE(X)
		& = \sum_{k = 0}^{n} k \binom{n}{k} p^k {(1 - p)}^{n - k} \\
		& = \sum_{k = 1}^{n} \frac{k \cdot n!}{(n - k)! k!} p^k {(1 - p)}^{n - k} \\
		& = \sum_{k = 1}^{n} \frac{n!}{(n - k)! (k - 1)!} p^k {(1 - p)}^{n - k} \\
		& = \sum_{m = 0}^{n - 1} \frac{n!}{(n - m - 1)! m!} p^{m + 1} {(1 - p)}^{n - m - 1} \\
		& = n p \sum_{m = 0}^{n - 1} \frac{(n - 1)!}{(n - m - 1)! m!} p^m {(1 - p)}^{n - m - 1} \\
		& = n p \PP(y \in \supp Y) \\
		& = n p
	\end{align*}
\end{example}
\begin{example}[תוחלת של משתנה מקרי היפר־גאומטרי]
	ניזכר בשאלה: בכד יש $a$ כדורים אדומים ו־$b$ כדורים שחורים ושולפים $k$ כדורים ללא החזרה לכד. \\*
	$X$ משתנה מקרי שסופר את מהספר הכדורים האדומים. \\*
	נגדיר $X_i$ המשתנה המקרי שבשליפה ה־$i$ יצא כדור אדום, ו־$X = \sum_{i = 1}^k X_i$.
	נוכל להגדיר גם $\Omega = \{ (y_1, \dots, y_k) \mid i \ne j \implies y_i \ne y_j \}$, כאשר מסמנים את הכדורים האדומים ב־$\{1, \dots, a\}$ וכן את השחורים ב־$\{a + 1, \dots, b\}$.
	אז
	\[
		\PP(X_i = 1)
		= \PP(y_i \le a)
		= \sum_{j = 1}^{a} \PP(y_i = j)
		= \sum_{j = 1}^{a} \frac{1}{a + b}
		= \frac{a}{a + b}
	\]
	ולכן $\EE(X) = \frac{k \cdot a}{a + b}$.
\end{example}
נעבור לבחינת דוגמה לשימוש בנוסחת התוחלת השלמה, אותה ראינו בהרצאה האחרונה.
\begin{exercise}
	מטילים קובייה הוגנת שוב ושוב עד שיוצא $1$. \\*
	מה תוחלת סכום ערכי הקובייה?
\end{exercise}
\begin{solution}
	נגדיר $X$ משתנה מקרי שסוכם את מספר סיבובי המשחק, לכן $X \sim Geo(\frac{1}{6})$. \\*
	נגדיר גם $Y_i$ להיות תוצאת ההטלה ה־$i$, אם היא התקיימה. \\*
	בנוסף $Y = \sum_{i = 1}^{\infty} Y_i$, הערך המעניין אותנו, סכום הקובייה בסוף המשחק.
	\[
		Y_i \mid X = n
		\sim \begin{cases}
			U(2, \dots, 6) & i < n \\
			1 & i = n \\
			0 & i > n
		\end{cases}
	\]
	ולכן נשתמש בנוסחת התוחלת השלמה
	\begin{align*}
		\EE(Y)
		& = \sum_{n = 1}^\infty \EE(Y \mid X = n) \cdot \PP(X = n) \\
		& = \sum_{n = 1}^\infty (\sum_{i = 1}^n \EE(X_i \mid X = n)) \cdot \PP(X = n) \\
		& = \sum_{n = 1}^\infty (\sum_{i = 1}^n 4 + 1) \cdot {(\frac{5}{6})}^{n - 1} \\
		& = \frac{1}{6} \sum_{n = 1}^\infty (4n - 3) {(\frac{5}{6})}^{n - 1} \\
		& = \frac{1}{6} (4 \sum_{n = 1}^\infty n {(\frac{5}{6})}^{n - 1} - 3 \sum_{n = 1}^\infty n {(\frac{5}{6})}^{n - 1}) \\
		& = \frac{1}{6} (4 \cdot \frac{1}{{(\frac{1}{6})}^2} - 3 \cdot 6) \\
		& = 21
	\end{align*}
\end{solution}

\section{שיעור 14 --- 12.12.2024}
\subsection{תוחלת --- המשך}
נבחין כי מתקיימת הטענה הבאה, אך לא נוכיח אותה שכן אין בכך ערך לימודי:
\begin{proposition}[נוסחת התוחלת השלמה הבת־מניה]
	אם $X, Y$ משתנים מקריים בדידים, אז
	\[
		\EE(X) = \sum_{t \in \RR} \PP(Y = t) \EE(X \mid Y = t)
	\]
\end{proposition}
נבחן תכונה נוספת.
\begin{proposition}[תוחלת מכפלת משתנים מקריים בלתי־תלויים]
	אם $X, Y$ משתנים מקריים בלתי־תלויים ובעלי תוחלת, אז
	\[
		\EE(X Y) = \EE(X) \EE(Y)
	\]
\end{proposition}
נבחין שבשונה מלינאריות תוחלת, במקרה הזה אנו צריכים את חוסר־התלות.
\begin{proof}
	לפי נוסחת התוחלת השלמה
	\[
		\EE(XY) = \sum_{t \in \RR} \PP(Y = y) \EE(XY \mid Y = t)
	\]
	בהינתן $Y = t$ ההתפלגות של $Y$ מתרכזת כולה ב־$t$, כלומר
	\[
		\PP(Y = s \mid Y = t) = \begin{cases}
			1 & s = t \\
			0 & \text{else}
		\end{cases}
	\]
	לכן בהינתן $Y = t$ מתקבל $XY \overset{a.s.}{=} Xt$ ובהתאם
	\[
		XY \mid Y = t \overset{a.s.}{=} Xt \mid Y = t
	\]
	לכן
	\[
		\EE(XY \mid Y = t)
		= \EE(Xt \mid Y = t)
		= t \EE(X \mid Y = t)
		= t \EE(X)
	\]
	ומשילוב השוויונות שמצאנו נובע
	\[
		\EE(XY)
		= \sum_{t \in \RR} \PP(Y = y) t \EE(X)
		= \EE(X) \EE(Y)
	\]
\end{proof}
נעבור לדון במה בכלל המשמעות של תוחלת.
עד כה מצאנו תכונות שלה, ואף הגדרות שקולות, אך מה המשמעות של התוחלת בהקשר הסתברותי?
באיזה מובן עלינו להתחשב בתוחלת במקרה שבו אנו יודעים את ערכה, כשהיא חיובית?
נעבור להליך שנותן לנו מידע בהסתברות מתוך מידע על תוחלות.
\begin{theorem}[אי־שוויון מרקוב]\label{markov_inequality}
	יהי $X$ משתנה מקרי אי־שלילי (דהינו $X \overset{a.s.}{\ge} 0$) ובעל תוחלת. \\*
	אז לכל $a > 0$ מתקיים
	\[
		\PP(X \ge a) \le \frac{\EE(X)}{a}
	\]
\end{theorem}
\begin{proof}
	נבחן את החלוקה $\{X < 0\}, \{0 \le X < a\}, \{X \ge a\}$, זוהי חלוקה של $\Omega$, נסמן אותם גם ב־$A_0, A_1, A_2$ בהתאמה.
	\[
		\EE(X) = \overbrace{\EE(X 1_{A_0})}^{=0} + \EE(X 1_{A_1}) + \EE(X 1_{A_2})
	\]
	נוכל לקבל תוצאה דומה עם נוסחת התוחלת השלמה. \\*
	המחובר השני הוא אי־שלילי מההגדרות שהנחנו, והמחובר השלישי מקיים
	\[
		\PP(X \ge a \mid X \ge a) = 1
		\implies \EE(X \mid X \ge a) \ge \EE(a \mid X \ge a) = a
	\]
	ולכן חסום על־ידי $\PP(a \in X) a$.
\end{proof}

\subsection{שימושים של אי־שוויון מרקוב}
\begin{example}
	נניח ש־$X \sim Geo(\frac{1}{2})$, אז $\EE(X) = 2$, ובהתאם $\PP(X \ge 4) \le \frac{\EE(X)}{4} = \frac{2}{4} = \frac{1}{2}$. \\*
	נוכל לחשב את ההסתברות עצמה על־ידי $\PP(X \ge 4) = \sum_{k = 4}^{\infty} \PP(X = k) = \sum_{k = 4}^{\infty} \frac{1}{2^k} = \frac{1}{8}$. \\*
	קיבלנו שהחסם שנובע מאי־שוויון ברקוב לא מאוד מועיל לנו.

	אם $Y = X - 1$ ואנו מחפשים את $Y \ge 0$ אז $\EE(Y) = 1$. \\*
	במקרה זה $\PP(X \ge 4) = \PP(Y \ge 3) \le \frac{\EE(Y)}{3} = \frac{1}{3}$. \\*
	אז קיבלנו חסם יותר טוב לערך, זאת אומרת שיש לנו דרך נוספת להשתמש באי־השוויון.
\end{example}
\begin{example}
	אם $X \sim Po(\lambda)$ ואנו מחפשים את $\PP(X \ge 1)$. \\*
	אנו כבר יודעים ש־$\EE(X) = \lambda$, ולכן $\PP(X \ge 1) \le \frac{\EE(X)}{1} = \lambda$. \\*
	אם $\lambda \ge 1$ אז מצאנו שהחסם הוא 1, והוא לא ממש מועיל לנו. \\*
	מצד שני $\PP(X \ge 1) = 1 - \PP(X = 0) = 1 - e^{-\lambda} \frac{\lambda^k}{k!} = 1 - e^{-\lambda}$. \\*
	לכן $1 - e^{-\lambda} \le \lambda$ ו־$1 - \lambda \le e^{-\lambda}$.
	באופן כללי ראינו שהחסם אכן די מדויק עבור מקרים רבים של ערכי $\lambda$.
\end{example}
\begin{example}
	$n$ מכתבים מגיעים ל־$n$ תיבות דואר ובוחרים תמורה מקרית על $[n]$. \\*
	נבחן את $X$ מספר נקודות השבת של התמורה.
	\[
		X = \sum_{i = 1}^{n} 1_{A_i}
	\]
	כאשר $A_i$ המאורע של נקודת שבת של התמורה ב־$i$, כלומר $\sigma(i) = i$. \\*
	נחשב גם $\EE(X) = \sum_{i = 1}^{n} \EE(1_{A_i}) = \sum_{i = 1}^{n} \PP(A_i)$ ולכן $\PP(X \ge a) \le \frac{1}{a}$.
\end{example}
\begin{example}
	נחזור לפרדוקס יום ההולדת, $X_i \sim U([n])$ בלתי־תלויים עבור $i \in [k]$, כאשר המשתנה המקרי מייצג את הסיכוי שלאדם ה־$i$ יש יום הולדת. \\*
	$X$ הוא מספר ימי ההולדת המשותפים,
	\[
		X = \sum_{1 \le i < j \le k} 1_{\{X_i = X_j\}}
	\]
	ונוכל גם לכתוב
	\[
		\PP(X_i = X_j)
		= \sum_{l = 1}^m \PP(X_i = l, X_j = l)
		= \sum_{l = 1}^m \PP(X_i = l) \PP(X_j = l)
		= \sum_{l = 1}^m \frac{1}{m} \cdot \frac{1}{m}
		= \frac{1}{m}
	\]
	ונובע
	\[
		\EE(X)
		= \sum_{1 \le i < j \le k} \PP(X_i = X_j)
		= \binom{k}{2} \frac{1}{m}
	\]
	ולבסוף
	\[
		\PP(X \ge 1) \le \frac{\EE(X)}{1} = \binom{k}{2} \frac{1}{m}
	\]
	זוהי הכללה של חסם האיחוד.
\end{example}
\begin{example}
	יהיו $A_1, \dots, A_N$ קבוצות, $|A_i| \ge n$ ו־$N < 2^{n - 1}$, אז קיימת קבוצה $B$ כך ש־$B \cap A_i \ne \emptyset$ ו־$\forall 1 \le i \le N, A_i \not\subseteq B$.

	נגדיר $A = \bigcup_{i = 1}^N A_i$.
	יהיו $X_a \sim Ber(\frac{1}{2})$ בלתי תלויים לכל $a \in A$ ונגדיר $B = \{ a \mid X_a = 1 \}$. \\*
	נחשב את ההסתברות $\PP(A_i \subseteq B) = \PP(\forall a \in A_i, X_a = 1) = {(\frac{1}{2})}^{|A_i|} \le \frac{1}{2^n}$. \\*
	מצד שני $\PP(A_i \cap B \ne \emptyset) = \PP(\forall a \in A_i, X_a = 0) = {(\frac{1}{2})}^{|A_i|} \le \frac{1}{2^n}$.
	אז
	\[
		\PP(\exists 1 \le i \le N, A_i \subseteq B \lor A_i \cap B = \emptyset)
		\le \sum_{i = 1}^{N} \PP(A_i \subseteq B) + \PP(A_i \cap B = \emptyset)
		\le N \cdot (\frac{1}{2^n} + \frac{1}{2^n})
		= \frac{N}{2^{n - 1}}
		< 1
	\]
	ולכן קיימת $B$ כזאת.
\end{example}

\section{שיעור 15 --- 17.12.2024}

\subsection{נוסחה לתוחלות}
נתחיל בנוסחה קטנה שתעזור לנו לפתח אינטואיציה, אך לא נשתמש בה רבות.
\begin{proposition}[נוסחת הזנב לתוחלת]
	אם $X$ משתנה מקרי שנתמך על־ידי $\NN \cup \{0\}$ אז
	\[
		\EE(X) = \sum_{n \in \NN} \PP(X \ge n)
	\]
\end{proposition}
\begin{proof}
	ממשפט פוביני לסכומים מרובים
	\[
		\EE(X)
		= \sum_{n = 1}^\infty n \PP(X = n)
		= \sum_{n = 1}^\infty \left( \sum_{k = 1}^n \PP(X = n) \right)
		= \sum_{\substack{n, k \in \NN \\ k \le n}} \PP(X = n)
		= \sum_{k = 1}^\infty \sum_{n = k}^\infty \PP(X = n)
		= \sum_{k = 1}^\infty \PP(X \ge k)
	\]
\end{proof}

\subsection{שונות}
באנגלית Variance.
אנו רוצים לשאול את השאלה כמה הסתברות רחוקה בעצם מהתוחלת, כך שנוכל לאפיין את שתי התכונות באופן מוצלח יותר אחת על־ידי השנייה.
לדוגמה אם $\EE(X) = \EE(Y) = 5$ אבל $X = 5$ כמעט תמיד ומצד שני $\PP(Y = 5000000) = \frac{1}{10^6}$ ו־$\PP(Y = 0) = 1 - \frac{1}{10^6}$ בעלות תוחלת זהה אבל בבירור שונות בתכלית.
\begin{definition}[שונות]
	$X$ משתנה מקרי בעל תוחלת $\mu = \EE(X)$, אז ה\textbf{שונות} של $X$ היא
	\[
		\var(X) = \EE({(X - \mu)}^2)
	\]
\end{definition}
\begin{example}
	במקרה שראינו זה עתה
	\[
		\var(X) = \EE({(X - 5)}^2) = 0
	\]
	בעוד שמתקיים
	\[
		\var(Y) = \EE({(Y - 5)}^2) = {(5000000 - 5)}^2 \cdot \frac{1}{10^6} + {(0 - 5)}^2 \cdot (1 - \frac{1}{10^6}) \approx 25000000
	\]
	כפי שאנו רואים, הפעם השונות מייצגת את ההבדל המשמעותי שבין שני המשתנים המקריים.
\end{example}
נוסיף הגדרה שלא נעסוק בה אך שרבים מאיתנו שמעו בעבר, והוא מושג סטיית התקן, מושג שמשמש רבות בסטטיסטיקה.
\begin{definition}[סטיית תקן]
	סטיית התקן של $X$ היא $\sigma(X) = \sqrt{\var(X)}$.
\end{definition}
נראה הגדרה נוספת לשונות שמשומשת אף היא, הגדרה זו שקולה להגדרה שראינו
\begin{definition}[הגדרה שקולה לשונות]
	נגדיר את השונות להיות
	\[
		\var(X) = \EE(X^2) - {(\EE(X))}^2
	\]
\end{definition}
\begin{proof}[הוכחת השקילות]
	נסמן $\mu = \EE(X)$ ולכן מתכונות התוחלת
	\[
		\EE({(X - \mu)}^2)
		= \EE(X^2 - 2X\mu + \mu^2)
		= \EE(X^2) - \EE(2X\mu) + \EE(\mu^2)
		= \EE(X^2) - 2\mu \EE(X) + \mu^2
		= \EE(X^2) - 2\mu^2 + \mu^2
	\]
	ומצאנו כי מתקיים השוויון שחיפשנו.
\end{proof}
\begin{proposition}[תכונות של שונות]
	כלל התכונות הבאות מתקיימות עבור $X$ משתנה מקרי בעל תוחלת:
	\begin{enumerate}
		\item $\var(X) \ge 0$ ו־$\var(X) = 0$ אם $X$ קבוע כמעט תמיד. השונות היא חיובית ואם המשתנה המקרי קבוע, אז השינוי שהיא מייצגת הוא אפס סביב התוחלת, היא גם ההסתברות.
		\item $\var(X) = \var(X + a)$ לכל $a \in \RR$. השונות לא מושפעת מהזהה, היא תכונה כללית.
		\item $\var(aX) = a^2 \var(X)$. מתיחה של המשתנה המקרי מגדילה אפילו יותר את השונות, נבחין כי השונות מייצגת את הטווח סביב התוחלת, ונוכל להסתכל עליה כשטח של איזשהו רדיוס סביב התוחלת, ככה נקבל את הריבוע.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item ${(X - \mu)}^2 \ge 0$ ולכן גם $\EE({(X - \mu)}^2) \ge 0$. גם
			\[
				X - \mu \overset{a.s.}{=} 0 \iff {(X - \mu)}^2 \overset{a.s.}{=} 0 \iff \EE({(X - \mu)}^2) = 0
			\]
		\item נגדיר $Y = X + a$ ולכן $\EE(Y) = \mu + a$, ואז
			\[
				\var(Y) = \EE({(Y - (\mu + a))}^2) = \EE({((X + a) - (\mu + a))}^2) = \EE({(X - \mu)}^2) = \var(X)
			\]
		\item נגדיר $Y = aX$ ובהתאם $\EE(Y) = \EE(aX) = a \EE(X) = a \mu$, ולכן
			\[
				\var(Y) = \EE({(Y - a \mu)}^2) = \EE({(aX - a\mu)}^2) = a^2 \EE({(X - \mu)}^2) = a^2 \var(X)
			\]
	\end{enumerate}
\end{proof}
נבחין כי בעוד שאנו יודעים כי תוחלות הן לינאריות, זהו לא המקרה עבור שונות, ננסה לחשב שונות של סכום משתנים מקריים.
נחשב את $\var(X + Y)$ עבור $\mu = \EE(X), \nu = \EE(Y)$, אז $\EE(X + Y) = \mu + \nu$. נעבור לחישוב השונות
\begin{align*}
	\var(X + Y)
	& = \EE({((X + Y) - (\mu + \nu))}^2) \\
	& = \EE({((X - \mu) + (Y - \nu))}^2) \\
	& = \EE({(X - \mu)}^2 + 2 (X - \mu)(Y - \nu) + {(Y - \nu)}^2) \\
	& = \EE({(X - \mu)}^2) + 2 \EE((X - \mu)(Y - \nu)) + \EE({(Y - \nu)}^2) \\
\end{align*}
ניתן שם לביטוי לחלק הביטוי שיצא לנו, ונגדיר
\begin{definition}[שונות משותפת]
	נגדיר
	\[
		\cov(X, Y) = \EE((X - \mu)(Y - \nu))
	\]
	עבור $\EE(X) = \mu, \EE(Y) = \nu$.
\end{definition}
כאשר $\cov$ הוא קיצור ל־Covariance, הוא בתורו קיצור למילה Cooperative Variance.
ולכן נוכל לקבל
\begin{conclusion}
	עבור משתנים מקריים $X, Y$ בעלי תוחלת, מתקיים
	\[
		\var(X + Y) = \var(X) + 2 \cov(X, Y) + \var(Y)
	\]
\end{conclusion}
\begin{proposition}
	אם $X$ ו־$Y$ בלתי־תלויים ובעלי שונות, אז $\cov(X, Y) = 0$.
\end{proposition}
\begin{proof}
	נראה בהמשך שאם ל־$X$ ול־$Y$ יש שונות אז $\cov(X, Y)$ מוגדר (כלומר הטור מתכנס בהחלט). \\*
	נניח כרגע שזה נכון ולכן
	\[
		\cov(X, Y)
		= \EE((X - \mu)(Y - \nu))
		\overset{(1)}{=} \EE(X - \mu) \EE(Y - \nu)
		= 0 \cdot 0
		= 0
	\]
	כאשר
	\begin{enumerate}
		\item $X - \mu$ ו־$Y - \nu$ בלתי־תלויים, מאי־התלות של $X, Y$ עצמם.
	\end{enumerate}
\end{proof}
\begin{example}
	אם $X = c$ כמעט תמיד אז $\EE(X) = c$ ו־$\var(X) = 0$.
\end{example}
\begin{example}
	נניח $X \sim Ber(p)$ ולכן $\var(X) = \EE(X^2) - {\EE(X)}^2 = p - p^2 = p(1 - p)$. \\*
	אם $X \sim Ber(p)$ אז $1 - X \sim Ber(1 - p)$ ואז $\var(1 - X) = \var(-X) = {(-1)}^2 \var(X) = \var(X)$. \\*
	זאת אומרת, לא מפתיע שהשונות היא סימטרית במקרה זה עבור שני המשתנים.
\end{example}
\begin{example}
	אם $X \sim Bin(n, p)$, אז נוכל להשתמש ישירות בהגדרת השונות, אבל נשתמש בעובדה שמשתנה בינומי הוא סכום של משתנים ברנולי,
	כלומר $X_i \sim Ber(p)$ בלתי־תלויים עבור $1 \le i \le n$ אז אם $X = \sum_{i = 1}^{n} X_i$ גם $X \sim Bin(n, p)$, ולכן
	\[
		\var(X)
		= \var(\sum_{i = 1}^{n} X_i)
		= \sum_{i = 1}^{n} \var(X_i)
		= n p(1 - p)
	\]
\end{example}
\begin{example}
	נניח עתה $X \sim Poi(\lambda)$, ונחשב על־ידי ההגדרה השקולה והעובדה שאנו כבר יודעים ש־$\EE(X) = \lambda$:
	\begin{align*}
		\EE(X^2)
		& = \sum_{k = 0}^{\infty} k^2 \PP(X = k) \\
		& = \sum_{k = 1}^{\infty} k^2 \frac{e^{-\lambda} \lambda^k}{k!} \\
		& = e^{-\lambda} \sum_{k = 1}^{\infty} \frac{k \lambda^k}{(k - 1)!} \\
		& = e^{-\lambda} \sum_{k = 1}^{\infty} \frac{((k - 1) + 1) \lambda^k}{(k - 1)!} \\
		& = e^{-\lambda} \sum_{k = 1}^{\infty} \frac{(k - 1) \lambda^k}{(k - 1)!} + e^{-\lambda} \sum_{k = 1}^{\infty} \frac{\lambda^k}{(k - 1)!} \\
		& = e^{-\lambda} \lambda^2 \sum_{k = 2}^{\infty} \frac{\lambda^{k - 2}}{(k - 2)!} + e^{-\lambda} \lambda \sum_{k = 1}^{\infty} \frac{\lambda^{k - 1}}{(k - 1)!} \\
		& = \lambda^2 + \lambda
	\end{align*}
	ולכן נסיק $\var(X) = \EE(X^2) - {\EE(X)}^2 = \lambda$.
\end{example}

\section{תרגול 8 --- 19.12.2024}
\subsection{שימושים למשפט מרקוב}
\begin{exercise}
	מטילים מטבע מוטה עם הסתברות $p$ לעץ, 20 פעמים,
	חסמו את ההסתברות שהרצף עץ עץ יצא פחות מפעמיים.
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {\{0, 1\}}^{20}$ וכן $X = 1_{\{\omega_i = \omega_{i + 1} = 1\}}$, וגם $X = \sum X_i$, אנו רוצים לחשב את $\PP(X \le 1)$. \\*
	נבחין כי $X_i \sim Ber(p^2)$, וכן $\EE(X) = \sum \EE(X_i)  = 19 p^2$, שכן $0 \le X \le 19$.
	\[
		\PP(X \le 1)
		= \PP(19 - X \ge 18)
		\le \frac{\EE(19 - X)}{18}
		= \frac{19 - 19p^2}{18}
	\]
\end{solution}

\subsection{שאלות נבחרות בנושא שונות}
נתחיל ונבחין ששונות היא תבנית בי־לינארית (תבנית ריבועית), ובשל כך היא מקיימת את הטענה שהיא אי־שלילית, היא אדישה להזזות קבועות ויש לה כיול ריבועי,
\begin{example}
	אם $X \sim U([6])$ משתנה מקרי להטלת קובייה הוגנת, אז $\EE(X) = 3 \frac{1}{2}$ וכן
	\[
		\var(X) = \frac{6^2 - 1}{12} = \frac{35}{12} \approx 3
	\]
	זאת־אומרת שהשונות באמת מתכתבת עם המרחק של הערכים מהתוחלת.
\end{example}
\begin{exercise}
	יהי $X \sim Geo(q)$, חשבו את $\var(X)$.
\end{exercise}
\begin{solution}
	אנו יודעים ש־$\EE(X) = \frac{1}{q}$. \\*
	עוד אנו יודעים מאנליטיות $\frac{1}{1 - x}$ ופיתוח טיילור, מתקיים
	\[
		\sum_{n = 0}^{\infty} n (n - 1) x^{n - 2} = \frac{2}{{(1 - x)}^3}
	\]
	נשתמש בנוסחה זו ונובע
	\[
		\EE(X(X - 1))
		= \sum_{n = 0}^{\infty} n (n - 1) q {(1 - q)}^{n - 1}
		= q (1 - q) \sum_{n = 0}^{\infty} n (n - 1) {(1 - q)}^{n - 2}
		= q (1 - q) \frac{2}{q^3}
	\]
	ולכן $\EE(X^2) = \frac{2(1 - q)}{q^2} + \frac{1}{q}$.
	נעבור לחישוב השונות
	\[
		\var(X)
		= \EE(X^2) - {\EE(X)}^2
		= \frac{2(1 - q)}{q^2} + \frac{1}{q} - \frac{1}{q^2}
		= \frac{2(1 - q) + q - 1}{q^2}
		= \frac{1 - q}{q^2}
	\]
\end{solution}
ניזכר בשונות משותפת, נבחין כי מבי־לינאריות השונות מתקיים
\begin{align*}
	\var(X + Y)
	& = \cov(X + Y, X + Y) \\
	& = \cov(X, X) + \cov(X, Y) + \cov(Y, X) + \cov(Y, Y) \\
	& = \var(X) + \var(Y) + 2 \cov(X, Y)
\end{align*}
בהתאם גם
\[
	\var(X)
	= \var(\sum_{i = 1}^{n} X_i)
	= \sum_{i, j = 1}^{n} \cov(X_i, X_j)
	= \sum_{i = 1}^{n} \cov(X_i) + 2 \sum_{1 = i < j = n} \cov(X_i, X_j)
\]
\begin{exercise}
	חשבו את השונות ואת התוחלת של מספר נקודות השבת בתמורה מקרית על $\{1, \dots, n\}$.
\end{exercise}
\begin{solution}
	נגדיר $X_i$ נקודת השבת במקום ה־$i$.
	$X_i \sim Ber(\frac{1}{n})$ ו־$X = \sum X_i$.
	בהתאם
	\[
		\EE(X) = n \frac{1}{n} = 1
	\]
	אם $i \ne j$ אז $\PP(X_i, X_j = 1) = \frac{1}{n} \cdot \frac{1}{n - 1} \ne \frac{1}{n^2} = \PP(X_i = 1) \PP(X_j = 1)$, כלומר אנו רואים כי משתנים אלה תלויים.
	\[
		\var(X_i) = \frac{1}{n} (1 - \frac{1}{n})
	\]
	וכן
	\[
		i \ne j
		\implies \cov(X_i, X_j)
		= \EE(X_i X_j) - \EE(X_i) \EE(X_j)
		= \frac{1}{n} \cdot \frac{1}{n - 1} - \frac{1}{n^2}
	\]
	לכן
	\begin{align*}
		\var(X)
		& = \sum_{i = 1}^{n} \frac{1}{n}(1 - \frac{1}{n}) + 2 \sum_{1 = i < j = n} (\frac{1}{n} \cdot \frac{1}{n - 1} - \frac{1}{n^2}) \\
		& = (1 - \frac{1}{n}) + 2 \binom{n}{2} (\frac{1}{n(n - 1)} - \frac{1}{n^2}) \\
		& = (1 - \frac{1}{n}) + 2 \frac{n!}{2! (n - 2)!} (\frac{1}{n(n - 1)} - \frac{1}{n^2}) \\
		& = (1 - \frac{1}{n}) + \frac{n(n - 1)}{1} (\frac{1}{n(n - 1)} - \frac{1}{n^2}) \\
		& = 1 - \frac{1}{n} + 1 - \frac{n - 1}{n} \\
		& = 1
	\end{align*}
\end{solution}

\section{שיעור 16 --- 19.12.2024}
\subsection{שונות --- המשך}
נרחיב את הטענה מההרצאה הקודמת.
\begin{proposition}
	עבור משתנים מקריים $X, Y$ בעלי תוחלת מתקיים
	\[
		\cov(X, Y)
		= \EE((X - \EE(X))(Y - \EE(Y)))
		= \EE(XY) - \EE(X) \EE(Y)
	\]
\end{proposition}
\begin{proof}
	\[
		\EE((X - \EE(X))(Y - \EE(Y)))
		= \EE(XY - \EE(X) Y - \EE(Y) X + \EE(X) \EE(Y))
		= \EE(XY) - \EE(X) \EE(Y) - \EE(Y) \EE(X) + \EE(X) \EE(Y)
	\]
\end{proof}
\begin{remark}
	אם $X$ ו־$Y$ בלתי־תלויים אז $\cov(X, Y) = 0$.
\end{remark}
\begin{definition}[משתנים מקריים בלתי־מתואמים]
	אם מתקיים $\cov(X, Y) = 0$ אז נאמר ש־$X$ ו־$Y$ \textbf{בלתי־מתואמים}.
\end{definition}
נבחין כי זהו תנאי הרבה יותר חלש מאי־תלות.
\begin{proposition}[תכונות של שונות משותפת]
	לכל שני משתנים מקריים $X, Y$ בעלי־תוחלת מתקיימות התכונות הבאת
	\begin{enumerate}
		\item $\cov(X, Y) = \cov(Y, X)$
		\item $\cov(a + X, Y) = \cov(X, Y)$ 
		\item $\cov(aX, Y) = a \cdot \cov(X, Y)$
		\item $\var(X) = \cov(X, X)$
		\item $\cov(X + Y, Z) = \cov(X, Z) + \cov(Y, Z)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	התכונה הראשונה טריוויאלית ונובעת מההגדרה, ולכן נוכיח את התכונה השנייה.
	\[
		\EE(((a + X) - \EE(a + X))(Y - \EE(Y)))
		= \EE((X - \EE(X))(Y - \EE(Y)))
	\]

	נוכיח את התכונה החמישית.
	\begin{align*}
		\EE(((X + Y) - \EE(X + Y))(Z - \EE(Z)))
		& = \EE(((X - \EE(X)) + (Y - \EE(Y)))(Z - \EE(Z))) \\
		& = \EE((X - \EE(X))(Z - \EE(Z))) + \EE((Y - \EE(Y))(Z - \EE(Z)))
	\end{align*}
\end{proof}
\begin{proposition}
	נניח ש־$X_1, \dots, X_n$ משתנים מקריים בעלי תוחלת, אז
	\[
		\var(\sum_{i = 1}^{n} X_i)
		= \sum_{i = 1}^{n} \var(X_i) + 2 \sum_{1 \le i < j \le n} \cov(X_i, Y_i)
	\]
\end{proposition}
\begin{proof}
	\begin{align*}
		\var(\sum_{i = 1}^{n} X_i)
		& = \cov(\sum_{i = 1}^{n} X_i, \sum_{i = 1}^{n} X_i) \\
		& = \sum_{i = 1}^n \cov(X_i, \sum_{i = 1}^{n} X_i) \\
		& = \sum_{i = 1}^n \sum_{j = 1}^n \cov(X_i, X_j) \\
		& = \sum_{i = 1}^{n} \var(X_i) + 2 \sum_{1 \le i < j \le n} \cov(X_i, Y_i)
	\end{align*}
\end{proof}
\begin{example}
	מטילים מטבע הוגן $n$ פעמים וסופרים את מספר ה־$HH$, נחשב את התוחלת ואת השונות.

	נגדיר ${\{X_i\}}_{i = 1}^n$ משתנים מקריים $Ber(\frac{1}{2})$ בלתי־תלויים, ונגדיר $Y_i = X_i + X_{i + 1}$ עבור $i = 1, \dots, n - 1$.
	נגדיר גם $Y = \sum_{i = 1}^{n - 1} Y_i$.
	\[
		\EE(Y_i) = \EE(X_i) \EE(X_{i + 1}) = \frac{1}{4},
		\qquad
		\EE(Y) = \sum_{i = 1}^{n - 1} \EE(Y_i) = \frac{n - 1}{4}
	\]
	וכן
	\[
		\var(Y)
		= \var(\sum_{i = 1}^{n - 1} Y_i)
		= \sum_{n = 1}^{n - 1} \var(Y_i) + 2 \sum_{1 \le i < j \le n - 1} \cov(Y_i, Y_j)
	\]
	ונעבור לחישוב אלה האחרונים.
	\[
		\var(Y_i) = \frac{3}{16}
	\]
	נבחן דוגמה ספציפית למקרה שהמשתנים שונים,
	\[
		\cov(Y_1, Y_7)
		= \cov(X_1 X_2, X_7 X_8)
	\]
	לכל $i + 1 < j$ המשתנים $Y_i = X_i X_{i + 1}$ ו־$Y_j = X_j X_{j + 1}$ בלתי־תלויים (בתור פונקציות מעל משתנים מקריים בקבוצות זרות), לכן $\cov(Y_i, Y_j) = 0$. \\*
	נשאר המקרה $j = i + 1$.
	\[
		\forall 1 \le i \le n - 2, \cov(Y_i, Y_{i + 1})
		= \cov(X_i X_{i + 1}, X_{i + 1} X_{i + 2})
		= \EE(X_i X_{i + 1}^2 X_{i + 2}) - \EE(Y_i)\EE(Y_{i + 1})
		= \frac{1}{8} - \frac{1}{16}
		= \frac{1}{16}
	\]
	לבסוף $\var(Y) = (n - 1) \frac{3}{16} + 2(n - 2) \frac{1}{16} = o(n)$.
\end{example}
ניזכר באי־שוויון מרקוב\ \ref{markov_inequality} ונגדיר אי־שוויון חדש
\begin{theorem}[אי־שוויון צ'בישב]
	נניח ש־$X$ משתנה מקרי בעל שונות (ולכן בעל תוחלת), אז
	\[
		\forall \lambda > 0,\ 
		\PP(|X - \EE(X)| \ge \lambda) \le \frac{\var(X)}{\lambda^2}
	\]
\end{theorem}
\begin{proof}
	נשים לב ש־$\{ |X - \EE(X)| \ge \lambda \} = \{ {(X - \EE(X))}^2 \ge \lambda^2 \}$. \\*
	נגדיר $Y = {(X - \EE(X))}^2$ ואז $0 \le Y$ וכן $\EE(Y) = \EE({(X - \EE(X))}^2) = \var(X)$, ולכן מ־\ref{markov_inequality}
	\[
		\PP(Y \ge \lambda^2) \le \frac{\var(X)}{\lambda^2}
	\]
\end{proof}
\begin{example}
	מטילים מטבע הוגן $10^6$ פעמים.
	$X$ מספר העצים ואנו רוצים לדעת מה ההסתברות ש־$495000 < X < 505000$. אנו יודעים ש־$X \sim Bin(10^6, \frac{1}{2})$.
	\begin{align*}
		\PP(495000 < X < 505000)
		& = 1 - \PP(X \le 495000 \lor X \ge 505000) \\
		& = 1 - \PP(|X - 500000| \ge 5000) \\
		& \ge \frac{\var(X)}{5000^2}
	\end{align*}
	אנו גם יודעים שמתקיים
	\[
		X = \sum_{i = 1}^{10^6} X_i
		\implies
		\var(X) = \sum_{i = 1}^{10^6} \var(X_i) = 10^6 \cdot \frac{1}{4}
	\]
	ולכן
	\[
		\PP(|X - 500000| \ge 5000)
		\le \frac{\var(X)}{5000^2}
		= \frac{10^6 \cdot \frac{1}{4}}{5000^2}
		= \frac{1}{100}
	\]
	ולכן ההסתברות הזאת גדולה מ־$0.99$, זאת־אומרת שמצאנו חסם מאוד טוב למספר ההטלות שקיבלו עץ באופן יחסי.
\end{example}
\begin{example}
	אם נחזור לדוגמה איתה פתחנו את ההרצאה, אז נוכל לקבוע
	\[
		\var(Y_n) < 100n
		\qquad
		\EE(Y_n) = \frac{n - 1}{4}
	\]
	אז
	\[
		\PP\left(\left\lvert\frac{Y_n}{\frac{n - 1}{4}} - 1\right\rvert \ge \epsilon\right)
		\le \frac{\var(\frac{Y_n}{\frac{n - 1}{4}})}{\epsilon^2}
		= \frac{\var(Y_n)}{\epsilon^2 {(\frac{n - 1}{4})}^2}
		\le \frac{o(n)}{\frac{\epsilon^2}{16} {(n - 1)}^2}
		= o(\frac{1}{n})
	\]
\end{example}
\begin{theorem}[החוק החלש של המספרים הגדולים]
	תהי $X_1, X_2, \dots$ סדרת משתנים מקריים בלתי־תלויים שווי התפלגות ובעלי תוחלת $\mu$. \\*
	אם $Y_n = \frac{\sum_{i = 1}^{n} X_i}{n}$, אז לכל $\epsilon > 0$
	\[
		\PP(|Y_n - \mu| \ge \epsilon) \xrightarrow[n \to \infty]{} 0
	\]
\end{theorem}
\begin{proof}
	נוכיח בהנחת קיום שונות, כאשר ניתן להוכיח גם ללא הנחה זו.
	\[
		\EE(Y_n)
		= \frac{\EE(\sum_{i = 1}^{n} X_i)}{n}
		= \frac{\sum_{i = 1}^{n} \EE(X_i)}{n}
		= \mu
	\]
	ולכן
	\[
		\PP(|Y_n - \mu| \ge \epsilon)
		\le \frac{\var(Y_n)}{\epsilon^2}
		= \frac{\var(\frac{\sum_{i = 1}^{n} X_i}{n})}{\epsilon^2}
		= \frac{\var(\sum_{i = 1}^{n} X_i)}{n^2 \epsilon^2}
		= \frac{n\var(X_1)}{n^2 \epsilon^2}
		\xrightarrow[n \to \infty]{} 0
	\]
\end{proof}

\section{שיעור 17 --- 31.12.2024}
\subsection{בעיית אספן הקופונים}
\begin{exercise}[אספן הקופונים]
	יהי אספן קופונים אשר מקבל כל יום קופון כלשהו מבין מספר קופונים אפשריים, מה החסם שמעיד שהאספן השיג את כל הקופונים? 
\end{exercise}
\begin{solution}
	נגדיר $X_1, \dots, X_m$ משתנים מקריים בלתי־תלויים המתפלגים אחיד $U([n])$, כלומר $m$ מספר סוגי הקופונים ו־$n$ מספר השליפות, ואנו רוצים לחסום את $\PP(\forall k \in [n], \exists i \in [m], X_i = k)$.
	נבחן את המאורע המשלים, $\PP(\exists k \in [m], \forall i \in [n], X_i \ne k) = \PP(\bigcup_{k = 1}^n A_k)$ כאשר האיחוד איננו זר ו־$A_k = \forall i \in [m], X_i \ne k$, אפשר לחסום זאת עם חסם האיחוד,
	\[
		\PP(\bigcup_{k = 1}^n A_k)
		\le \sum_{k = 1}^{n} \PP(A_k)
		= \sum_{k = 1}^{n} {(1 - \frac{1}{n})}^m
		= n {(1 - \frac{1}{n})}^m
	\]
	אנו רוצים להבין מתי הביטוי שהתקבל שואף ל־$0$, נשים לב שידוע כי $\forall x \in \RR, 1 + x \le e^x$, ולכן
	\[
		n {(1 - \frac{1}{n})}^m
		\le n {(e^{-\frac{1}{n}})}^m
		= n e^{-\frac{m}{n}}
	\]
	אז אם $m = \lceil cn \log n \rceil$ ו־$c > 1$ אז
	\[
		n e^{-\frac{m}{n}}
		= n e^{-c \log n}
		= n n^{-c}
		= n^{1 - c}
		\xrightarrow[n \to \infty]{} 0
	\]
	נבחין כי חסם האיחוד הוא מקרה פרטי של אי־שוויון מרקוב.
	נעבור לחישוב של תוחלת ושונות של כמה קופונים האספן לא השיג,
	נסמן ב־$Y$ את מספר הקופונים החסרים, וכן $Y = \sum_{k = 1}^{n} Y_n$ כאשר $Y_k = 1_{A_k}$.
	מהחישובים שעשינו עד כה נוכל להסיק
	\[
		\EE(Y) = n {(1 - \frac{1}{n})}^m
	\]
	שכן מאי־שוויון מרקוב $\PP(Y \ge 1) \le \frac{\EE(Y)}{1}$.
	נראה שבהצבה של $m$ שקטן ממה שמצאנו (יחד עם $c < 1$) חסמנו מלמעלה את $\PP(Y = 0)$. נעבור לחישוב השונות של $Y$,
	\[
		\var(Y_k)
		= {(1 - \frac{1}{n})}^m (1 - {(1 - \frac{1}{n})}^m)
		\le {(1 - \frac{1}{n})}^m
		= \EE(Y_k)
	\]
	וכן
	\begin{align*}
		k \le l,
		\cov(Y_k, Y_l)
		& = \EE(Y_k \cdot Y_l) - \EE(Y_k) \EE(Y_l) \\
		& = {(1 - \frac{2}{n})}^m - {(1 - \frac{1}{n})}^{2m} \\
		& = {(1 - \frac{2}{n})}^m - {({(1 - \frac{1}{n})}^{2})}m \\
		& = {(1 - \frac{2}{n})}^m - {({(1 - \frac{2}{n} + \frac{1}{n^2})}^{2})}m \\
		& \le 0
	\end{align*}
	ולכן $\var(Y) \le n {(1 - \frac{1}{n})}^m = \EE(Y)$ (מחקנו את האיברים השליליים) ובהתאם
	\[
		\frac{\var(Y)}{{(\EE(Y))}^2}
		\le \frac{1}{\EE(Y)}
	\]
	נשאר למצוא מתי $EE(Y) \to \infty$,
	עבור $m = cn \log n$ כאשר $1 > c$ נובע
	\[
		\log(n {(1 - \frac{1}{n})}^m)
		= \log(n) + m \log(1 + \frac{1}{n})
		= \log(n) + cn \log(1 - \frac{1}{n}) \to \infty
	\]
	כאשר את המקרה $c = 1$ אין לנו היכולת להראות.
\end{solution}

\subsection{בין הסתברות ללינארית}
\begin{proposition}
	$X$ משתנה מקרי בעל שונות ו־$f(a) = \EE({(X - a)}^2)$, אז $f$ מקבלת מינימום ב־$a = \EE(X)$.
\end{proposition}
\begin{proof}
	\[
		f(a)
		= \EE({(X - a)}^2)
		= \EE(X^2 0 2a X + a^2)
		= \EE(X^2) - 2a \EE(X) + a^2
	\]
	ולכן
	\[
		f'(a)
		= -2 \EE(X) + 2a
	\]
	ונובע ש־$f'(\EE(X)) = 0$.
\end{proof}
אנו נתקלים בקושי של הקשר בין משתנים מקריים, תוחלת ושונות עם אלגברה לינארית.
\begin{definition}[קבוצת כל המשתנים המקריים]
	נגדיר את $L_2$ להיות קבוצת כל המשתנים המקריים (במרחב הסתברות כלשהו) בעלי שונות,
	כאשר אנו מזהים משתנים מקריים ששווים כמעט תמיד.
\end{definition}
\begin{proposition}
	$L_2$ הוא מרחב מכפחה פנימית עם $\langle X, Y \rangle = \EE(XY)$.
\end{proposition}
לפני שניגש להוכחה נבחן דוגמה שתבהיר לנו את הטענה.
\begin{example}
	נגדיר $\Omega = [n]$ עם הסתברות אחידה, אז אם $X$ משתנה מקרי אז $X : \Omega \to \Omega$, כלומר $L_2 = \RR^\Omega$.
	מתקיים
	\[
		\langle X, Y \rangle
		= \sum_{\omega \in \Omega} \PP(\{\omega\}) X(\omega) Y(\omega)
		= \sum_{k = 1}^{n} \frac{1}{n} X(k) Y(k)
	\]
	וזו אכן מכפלה פנימית.
\end{example}
נעבור להוכחה.
\begin{proof}
	מהגדרת התוחלת של $\EE(XY)$ קל להוכיח כי $\langle aX, Y \rangle = a \langle X, Y \rangle$ וכן ש־$\langle X + Y, Z \rangle = \langle X, Z \rangle + \langle Y, Z \rangle$,
	ואף $\EE(X^2) = \langle X, X \rangle \iff X = 0$.
	אם מצטמצמים לתת־מרחב של המשתנים המקריים של $\EE(X) = 0$ אז $\langle X, Y \rangle = \cov(X, Y)$ ו־$\lVert X \rVert = \var(X)$.
	נשים לב כי הפעולה של התוחלת היא פונקציה ולכן $\EE : L_2 \to \RR$ לינארית ונחשוב על $\RR$ כ־$K$ כאשר $K$ כל המשתנים המקריים הקבועים.
	נזהה את התוחלת על־ידי הטלה אורתוגונלית. כלומר $\EE(X)$ אורתוגונלי ל־$X - \EE(X)$ וזה ברור כי $\langle X, X - \EE(X) \rangle = \EE((X - \EE(X)) \EE(X)) = 0$. \\
	את ההוכחה הפורמלית נעשה עם משפט קושי־שוורץ.
	לכל $X, Y \in L_2$, נראה
	\[
		|\EE(XY)|
		\le \sqrt{\EE(X^2) \EE(Y^2)}
	\]
	נגדיר
	\[
		\overline{X} = \frac{X}{\sqrt{\EE(X^2)}},
		\qquad
		\overline{Y} = \frac{Y}{\sqrt{\EE(X^2)}}
	\]
	ואז $\EE(\overline{X}^2) = \EE(\overline{Y}^2) = 0$.
	ניזכר שאם $X, Y$ אי־שליליים אז $0 \le XY \le \frac{X^2 + Y^2}{2}$ ונניח שהמשתנים המקריים שלנו אי־שליליים.
	אז
	\[
		\EE(\overline{X} \overline{Y})
		\le \EE(\frac{\overline{X}^2 + \overline{Y}^2}{2})
		= \frac{1}{2}(\EE(\overline{X}^2) + \EE(\overline{Y}^2))
		= 1
	\]
\end{proof}

\section{תרגול 9 --- 2.1.2025}
\subsection{תרגילים שונים בנושא שונות}
\begin{example}
	סופר שואל סטודנטים בקמפוס אם הם קראו ספר שלו או לא. \\
	אחוז הסטודנטים שקראו את הספר הוא $q$.
	אנו רוצים למצוא את $q$ על־ידי שאילת רק חלק מהסטודנטים. \\
	נניח שדגמנו $n$ סטודנטים (מספר הסטודנטים הכללי לא ידוע), נסמן ב־$X$ את מספר הסטודנטים שענו שהם קראו.
	נמצא כמה סטודנטים אנו צריכים לשאול כדי שיתקיים
	\[
		\PP(| \frac{X}{n} - q| \ge 0.1) < 0.05
	\]
	נניח ש־$X \sim Bin(n, q)$ (זוהי הנחה מסטטיסטיקה שלא נעסוק בה).
	לכן
	\[
		\EE(X) = nq,
		\EE(\frac{X}{n}) = q,
		\var(X) = nq(1 - q) \le n \frac{1}{4}
	\]
	בהתאם
	\[
		\var(\frac{X}{n}) \le \frac{1}{4n}
	\]
	מאי־שוויון צ'בישב
	\[
		\PP(|\frac{X}{n} - q| \ge 0.1) <\frac{4n}{0.01} = \frac{25}{n} < \frac{5}{100}
		\iff 500 < n
	\]
\end{example}
\begin{exercise}
	הזמן (בחודשים) עד שתנור חדש מתקלקל מתפלג $X$ עם תוחלת $\mu$ ושונות $\sigma^2$. \\
	חברת ביטוח רוצה להציע אחריות לתנורים, היא חישבה שעל־מנת להרוויח היא צריכה שיחס מימושי הביטוח תהיה לכל היותר $p$. \\
	כמה חודשי ביטוח על החברה להציע?
\end{exercise}
\begin{solution}
	אנו מחפשים את $\PP(X < M) < p$ עבור $M$ מספר חודשים. נעשה שינוי ונכתוב $\{X < \mu - k\} \subseteq \{ |X - \mu| > k \}$.
	צ'בישב
	\[
		\PP(X < \mu - k)
		\le \PP(|X - \mu| \ge k)
		< \frac{\var(X)}{k^2}
		= \frac{\sigma^2}{k^2}
		< p
	\]
	ולכן
	\[
		k > \frac{\sigma}{\sqrt{p}}
	\]
	ועל החברה להציע ביטוח לכל היותר ל־$\mu - \frac{\sigma}{\sqrt{p}}$ חודשים.
\end{solution}
\begin{exercise}
	מגרילים מטריצה $n \times n$ ממקדמים ב־$\ZZ_{/2}$ על־ידי זה שמגרילים מטבע בכל כניסה באופן בלתי תלוי. \\
	חשבו את התוחלת והשונות של מספר תתי־המטריצות $2 \times 2$ מהצורה $\begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$.
\end{exercise}
\begin{solution}
	נסמן $X_{i, j}$ משתנה מקרי ברנולי לפי האם הכניסה ה־$i, j$ היא $1$ ו־$Y_{i, j}$ ברנולי של האם התת־מטריצה בגודל שתיים במקום ה־$i, j$ היא מטריצת יחידות כרצוי.
	נבחין שמתקיים
	\[
		Y_{i, j} = X_{i, j} X_{i + 1, j} X_{i, j + 1} X_{i + 1, j + 1}
	\]
	נסמן
	\[
		Y = \sum_{1 \le i, j \le n - 1} Y_{i, j}
	\]
	ונעבור לחישוב הערכים,
	\[
		\EE(Y)
		= {(n - 1)}^2 \frac{1}{2^2}
	\]
	וכן עבור השונות
	\begin{align*}
		\var(Y)
		& = \cov(Y, Y) \\
		& = \sum_{1 \le i, j \le n - 1} \cov(Y_{i, j}, Y_{i, j}) \\
		& = \sum_{1 \le i, j \le n - 1} \var(Y_{i, j})
		+ \sum_{\substack{1 \le i \le n - 1 \\ 1 \le j \le n - 2}} \cov(Y_{i, j}, Y_{i + 1, j})
		+ \cdots
		+ \sum_{1 \le j \le n - 2} \cov(Y_{i, j}, Y_{i + 1, j + 1})
		+ \sum_{\substack{1 \le i, j \le n - 1 \\ |i - j| \ge 2}} 0
	\end{align*}
	ויש לנו שלושה סוגי חפיפה שנוספים אף הם ולא כתבנו, עתה נוכל לעבור לחישוב החלקים השונים ביתר קלות.
	לדוגמה
	\[
		\cov(Y_{i, j}, Y_{i, j + 1})
		= \EE(X_{i, j} X_{i + 1, j} X_{i, j + 1} X_{i + 1, j + 1} X_{i + 1, j + 1} X_{i + 2, j + 1} X_{i + 1, j + 2} X_{i + 2, j + 2}) - \EE(Y_{i, j}) \EE(Y_{i, j + 1})
		= \frac{1}{2^6} - \frac{1}{2^8}
	\]
	ולכן
	\[
		\var(Y)
		= {(n - 1)}^2 (\frac{1}{2^4} - \frac{1}{2^8})
		+ 4(n - 1)(n - 2) (\frac{1}{2^6} - \frac{1}{2^8})
		+ 4 {(n - 2)}^2 {(n - 1)}^2 (\frac{1}{2^7} - \frac{1}{2^8})
	\]
\end{solution}

\section{שיעור 18 --- 2.1.2025}
\subsection{מומנטים גבוהים}
\begin{definition}[מומנט]
	המומנט ה־$k$ של $X$ הוא $\EE(X^k)$.
\end{definition}
\begin{definition}[מומנט מרכזי]
	המומנט המרכזי הוא $\EE({(X - \EE(X))}^k)$.
\end{definition}
\begin{proposition}[צ'בישב מוכלל]
	\[
		\PP(|X - \EE(X)| \ge \lambda) \le \frac{\EE({(X - \EE(X)))}^k}{\lambda^k}
	\]
	לכל $k$ זוגי.
\end{proposition}
ההוכחה מאוד דומה להוכחה של אי־השוויון במקרה הרגיל.
\begin{example}
	מטילים מטבע הוגן $n$ פעמים ואנו רוצים לחסום את ההסתברות שקיבלנו יותר מ $\frac{3}{4}n$ עצים. \\
	מגדירים $X_1, \dots, X_n \sim Ber(\frac{1}{2})$ וכן $X = \sum_{i = 1}^n X_i$ ולכן
	\[
		\PP(X \ge \frac{3}{4}n)
		\le \PP(|X - \frac{n}{2}| \ge \frac{n}{4})
		\le \frac{\var(X)}{{(\frac{n}{4})}^2}
	\]
	אבל מ־$X \sim Bin$ נסיק
	\[
		\var(X) = \frac{n}{4}
	\]
	ולכן
	\[
		\frac{\var(X)}{{(\frac{n}{4})}^2}
		= \frac{\frac{n}{4}}{{(\frac{n}{4})}^2}
		= \frac{4}{n}
	\]
	ננסה להשתמש בנוסחה החדשה.
	\[
		\EE({(X - \frac{n}{2})}^4)
		= \EE({(\sum_{i = 1}^{n}  (X_i - \frac{1}{2}))}^4)
	\]
	נגדיר $Y_i = X_i - \frac{1}{2}$ ולכן
	\[
		\EE({(X - \frac{n}{2})}^4)
		= \EE({(\sum_{i = 1}^{n} Y_i)}^4)
		= \EE(\sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k = 1}^{n} \sum_{l = 1}^{n} Y_i Y_j Y_k Y_l)
		= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k = 1}^{n} \sum_{l = 1}^{n} \EE(Y_i Y_j Y_k Y_l)
	\]
	במצב הרגיל אנו יכולים לחלק למקרים עבור אינקסים זהים ושונים, הפעם יש לנו סוגי התלכדות שונים, נחשב לדוגמה את המקרה הזר, נניח ש$i, j, k, l$ שונים ולכן
	\[
		\EE(Y_i Y_j Y_k Y_l) = 0
	\]
	למעשה מספיק שאחד מהם יהיה שונה מכל השאר, לדוגמה $i \ne j, k, l$ אז
	\[
		\EE(Y_i Y_j Y_k Y_l)
		= \EE(Y_i) \EE(Y_j Y_k Y_l)
		= 0
	\]
	ולכן
	\[
		\EE({(X - \frac{n}{2})}^4)
		= \sum_{i = 1}^{n} \EE(Y_i^4)
		+ \binom{4}{2} \sum_{i = 1}^{n} \sum_{j = 1, j \ne i}^{n} \EE(Y_i^2) \EE(Y_j^2)
		\le K n^2
	\]
	ומאי־שוויון צבישב המוכלל נקבל
	\[
		\PP(|X - \frac{n}{2}| \ge \frac{3n}{4})
		\le o(\frac{1}{n^2})
	\]
	במקום לעבוד עם פולינומים נעבוד עם משתנים מערכיים על־ידי ההגדרה
	\[
		Z = 2^X
	\]
	ולכן גם $Z - 2^X = 2^{\sum_{i = 1}^{n} X_i} = \prod_{i = 1}^n 2^{X_i}$, נגדיר את מכפלות אלה כ־$Z_i$ ואז
	\[
		\PP(X \ge \frac{3n}{4})
		= \PP(Z \ge 2^{\frac{3n}{4}})
		\le \frac{\EE(Z)}{2^{\frac{3n}{4}}}
		= \frac{\prod_{i = 1}^n \EE(Z_i)}{2^{\frac{3n}{4}}}
	\]
	וגם
	\[
		\EE(Z_i) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot 2 = \frac{3}{2}
	\]
	ולכן
	\[
		\EE(Z) = {\left(\frac{3}{2}\right)}^n
	\]
	וכן
	\[
		\frac{\prod_{i = 1}^n \EE(Z_i)}{2^{\frac{3n}{4}}}
		= \frac{\frac{3^n}{2^n}}{2^{\frac{3n}{4}}}
		= {(\frac{\frac{3}{2}}{2^{\frac{3}{4}}})}^n
		\xrightarrow[n \to \infty]{} 0
	\]
\end{example}
\begin{definition}[פונקציה יוצרת מומנטים]
	$X$ משתנה מקרי, אז הפונקציה יוצרת המומנטים שלו היא
	\[
		M_X(t) = \EE(e^{tX})
	\]
	והיא מוגדרת עבור חלק מערכי $t \in \RR$.
\end{definition}
\begin{proposition}[אי־שוויון צ'רנוף]
	נניח $X$ משתנה מקרי ו־$\lambda \in \RR$ אז
	\[
		\PP(X \ge \lambda)
		\le \frac{M_X(t)}{e^{t\lambda}}
	\]
	לכל $t > 0$ עבורו $M_X(t)$ מוגדרת.
\end{proposition}
\begin{proof}
	\[
		\PP(X \ge \lambda)
		= \PP(e^{tX} \ge e^{t\lambda})
		\le \frac{\EE(e^{tX})}{e^{t\lambda}}
	\]
\end{proof}
\begin{proposition}[כפליות פונקציה יוצרת מומנטים]
	אם $X, Y$ בלתי־תלויים אז
	\[
		M_{X + Y}(t) = M_X(t) \cdot M_Y(t)
	\]
	לכל $t$ בתחום ההגדרה של $X, Y$.
\end{proposition}
\begin{proof}
	\[
		\EE(e^{t(X + Y)})
		= \EE(e^{tX} \cdot e^{tY})
		= \EE(e^{tX}) \EE(e^{tY})
	\]
\end{proof}
\begin{example}
	נניח ש־$X \sim Ber(p)$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		= (1 - p) e^{t \cdot 0} + p e^{t \cdot 1}
		= 1 + p(e^t - 1)
	\]
	ועל־ידי הטענה האחרונה אם $X \sim Bin(n, p)$ אז
	\[
		M_X(t)
		= {(1 + p(e^t - 1))}^n
	\]
\end{example}
\begin{example}
	נניח ש־$X \sim Poi(\lambda)$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		= \sum_{k = 0}^{\infty} e^{tk} e^{-\lambda} \frac{\lambda^k}{k!}
		= e^{-\lambda} \sum_{k = 0}^{\infty} \frac{{(\lambda \cdot e^t)}^k}{k!}
		= e^{-\lambda} e^{e^t \lambda}
		= e^{\lambda(e^t - 1)}
	\]
\end{example}
\begin{remark}
	אם ניקח $X_n \sim Bin(n, \frac{\lambda}{n})$ אז $M_{X_n}(t) = {(1 + \frac{\lambda(e^t - 1)}{n})}^n \xrightarrow[n \to \infty]{} e^{\lambda(e^t - 1)} = M_X(t)$.
\end{remark}
\begin{example}
	נניח ש־$X \sim Geo(p)$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		= \sum_{k = 1}^{\infty} e^{tk} {(1 - p)}^{k - 1} p
		= p e^t \sum_{k = 1}^{\infty} {(e^t (1 - p))}^{k - 1}
		= \frac{p e^t}{1 - e^t(1 - p)}
	\]
	וזה מוגדר רק כאשר $e^t(1 - p) < 1$.
\end{example}
\begin{theorem}[אי־שוויון הופדינג]\label{hofding_inequality_theorem}
	יהיו $X_1, \dots, X_n$ משתנים מקריים בלתי־תלויים כך ש־$\EE(X_i) = 0$, ו־$|X_i| \le 1$ כמעט תמיד, אז
	\[
		\PP(\sum_{i = 1}^{n} X_i \ge \lambda)
		\le e^{-\frac{\lambda^2}{2n}}
	\]
\end{theorem}
את ההוכחה נראה בהרצאה הבאה, אבל כן נראה דוגמה
\begin{example}
	מטילים $n = 10^6$ מטבעות הוגנים באופן בלתי־תלוי,
	\[
		0.99 \le \PP(495000 \le X \le 505000)
	\]
	ונמצא חסם נוסף להסתברות זו.
	נגדיר $Y_i = 2(X_i - \frac{1}{2})$ כדי למרכז אותם, ואז
	\[
		\EE(Y_i) = 0,
		|Y_i| \le 1
	\]
	אז אפשר להשתמש באי־שוויון הופדינג על $Y = \sum_{i = 1}^{n} Y_i$.
	\[
		\PP(Y \ge \lambda)
		\le e^{-\frac{\lambda^2}{2n}}
	\]
	רוצים לחסום את $|X - \frac{n}{2}| \ge 5000$ אז
	\[
		Y
		= \sum_{i = 1}^{n} Y_i
		= \sum_{i = 1}^{n} 2X_i - 1
		= 2X - n
		= 2 (X - \frac{n}{2})
	\]
	ולכן במקרה של $Y$ אנו מחפשים את
	\[
		|Y| \ge 10000
	\]
	ועתה נוכל מטעמי סימטריה לבחון רק את המקרה החיובי,
	\[
		\PP(|X - \frac{n}{2}| \ge 5000)
		\le 2 \PP(Y \ge 10000)
		\le e^{\frac{10000^2}{2n}}
		= e^{-50}
	\]
\end{example}

\section{שיעור 19 --- 7.1.2024}
\subsection{פונקציה יוצרת מומנטים --- המשך}
בהרצאה הקודמת ראינו את אי־שוויון הופדינג\ \ref{hofding_inequality_theorem}, אי־שוויון שימושי במיוחד עבור חסמים, עתה נראה את ההוכחה שלו ודוגמות נוספות.
\begin{example}
	אם $X_i \sim U(\{-1, 1\})$ מקיימים את תנאי \ \ref{hofding_inequality_theorem}. \\
	מאי־שוויון צ'בישב נקבל את החסם
	\[
		\PP(| \sum_{i = 1}^{n} X_i | \ge \lambda)
		\le \frac{\var(\sum_{i = 1}^{n} X_i)}{\lambda^2}
		= \frac{n}{\lambda^2}
	\]
	בזמן שמאי־שוויון הופדינג נובע
	\[
		\PP(| \sum_{i = 1}^{n} X_i | \ge \lambda)
		\le 2 \exp(- \frac{\lambda^2}{2n})
	\]
\end{example}
נראה עתה למה שנצטרך להוכחת אי־השוויון.
\begin{lemma}[הלמה של הופדינג]
	אם $X$ משתנה מקרי כך ש־$\EE(X) = 0$ ו־$|X| \le 1$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		\le e^{\frac{t^2}{2}}
	\]
	לכל $t \in \RR$.
\end{lemma}
\begin{proof}
	יהי $t \in \RR$.
	נגדיר את הפונקציה הקווית $L(x) = \frac{e^t - e^{-t}}{2} x + \frac{e^t + e^{-t}}{2}$ על־ידי שימוש בשיפוע על גרף הפונקציה $e^{tX}$. \\
	לכל $x \in [-1, 1]$ נובע $e^{tx} \le L(x)$.
	עבור $X$ שמקיים $|X| \le 1$ מתקיים $e^{tX} \le L(X)$.
	לכן מתכונות התוחלת נובע $\EE(e^{tX}) \le \EE(L(X))$.
	נותר אם כן לחשב את הערך האחרון,
	\[
		\EE(L(X))
		= \EE(\frac{e^t - e^{-t}}{2} X + \frac{e^t + e^{-t}}{2})
		= \frac{e^t - e^{-t}}{2} \EE(X) + \frac{e^t + e^{-t}}{2}
		= \frac{e^t + e^{-t}}{2}
	\]
	ומצאנו חסם, אך לא האחד שרצינו, נמשיך ונראה כי החסם המבוקש מתקיים אף הוא.
	מפיתוח טיילור נקבל
	\[
		\frac{e^t + e^{-t}}{2}
		= \frac{1}{2} \left(\sum_{k = 0}^{\infty} \frac{t^k}{k!} + \sum_{k = 0}^{\infty} \frac{{(-t)}^k}{k!} \right)
		= \frac{1}{2} \left(\sum_{k = 0}^{\infty} \frac{t^k + {(-t)}^k}{k!}\right)
		= \frac{1}{2} \left(\sum_{l = 0}^{\infty} \frac{t^{2l}}{(2l)!}\right)
	\]
	מהצד השני
	\[
		e^{\frac{t^2}{2}}
		= \sum_{l = 0}^{\infty} \frac{{\left(\frac{t^2}{2}\right)}^l}{l!}
		= \sum_{l = 0}^{\infty} \frac{t^{2l}}{2^l l!}
	\]
	אבל לכל $l \ge 0$ מתקיים
	\[
		2^l l! = 2l (2l - 2) \cdots 2 \le 2 l (2l - 1) \cdots 1 = (2l)!
	\]
	ולכן מצאנו את החסם הרצוי בדיוק.
\end{proof}
נעבור להוכחת אי־השוויון\ \ref{hofding_inequality_theorem}.
\begin{proof}
	לפי הלמה לכל $i$,
	\[
		M_{X_i}(t) \le e^{\frac{t^2}{2}}
	\]
	אנו גם יודעים ש־$X_i$ בלתי־תלויים ולפי כפליות
	\[
		M_{\sum_{i = 1}^{n} X_i}(t)
		= \prod_{i = 1}^n M_{X_i}(t)
		\le {(e^{\frac{t^2}{2}})}^n
		= e^{\frac{nt^2}{2}}
	\]
	ולכן לפי צ'רנוף
	\[
		\PP(\sum_{i = 1}^{n} X_i \ge \lambda)
		\le \frac{M_{\sum_{i = 1}^{n} X_i}(t)}{e^{\lambda t}}
		\le e^{\frac{nt^2}{2} - \lambda t}
	\]
	נמצא את הערך הקן ביותר עבור חסם זה על־ידי מציאת ערך קטן ביותר לביטוי $\frac{n t^2}{2} - \lambda t$, נגזור את הביטוי ונקבל $nt - \lambda$ ולכן נבחר $t = \frac{\lambda}{n}$ ונקבל
	\[
		e^{\frac{n {(\frac{\lambda}{n})}^2}{2} - \lambda \frac{\lambda}{n}}
		= e^{\frac{\lambda^2}{2n} - 2 \frac{1}{2}\frac{\lambda^2}{n}}
		= e^{-\frac{\lambda^2}{2n}}
	\]
	כפי שרצינו להראות.
\end{proof}
\begin{example}
	$X_i \sim U([6])$ עבור $i \in [100]$ ונרצה לחשב את ההסתברות שממוצע מאה ההטלות הוא לפחות 4.
	\[
		\PP(\frac{\sum_{i = 1}^{100} X_i}{100} \ge 4)
	\]
	נמרכז את המשתנים המקריים על־ידי הגדרת $Y_i = \frac{X_i - \frac{7}{2}}{\frac{5}{2}}$ כדי לעמוד בדרישות אי־השוויון\ \ref{hofding_inequality_theorem}. \\
	מההגדרה אכן $\EE(Y_i) = 0$ וגם $|Y_i| \le 1$.
	לכן נובע מאי־שוויון הופדינג
	\[
		\PP(\sum_{i = 1}^{100} Y_i \ge \lambda)
		\le e^{- \frac{\lambda^2}{100}}
	\]
	ואנו רוצים למצוא את ערך $\lambda$ המתאים כדי לתרגם את אי־השוויון הזה לאחד ששואל על $\frac{\sum_{i = 1}^{100} X_i}{100} \ge 4$.
	מהצבה
	\[
		X_i = \frac{5}{2} Y_i + \frac{7}{2}
	\]
	ולכן
	\[
		\frac{\sum_{i = 1}^{100} \frac{5}{2} Y_i + \frac{7}{2}}{100} \ge 4
		\iff
		\frac{\frac{5}{2} \sum_{i = 1}^{100} Y_i}{100} + \frac{7}{2} \ge 4
		\iff
		\frac{5}{2} \sum_{i = 1}^{100} Y_i \ge 50
		\iff
		\sum_{i = 1}^{100} Y_i \ge 20
	\]
	ולכן נקבל
	\[
		\PP(\frac{\sum_{i = 1}^{100} X_i}{100} \ge 4)
		\le e^{- \frac{20^2}{200}}
		= e^{-2}
	\]
\end{example}
לבסוף נעיר הערה על השם פונקציה יוצרת מומנטים, הסיבה שאנו קוראים לה ככה הוא שהנגזרות שלה הם המומנטים, כלומר $\EE(X^2)$ יהיה ערך הנגזרת השנייה של $M_X(t)$ וכן הלאה, כך נראה עתה.
\begin{proposition}
	אם $M_X(t)$ מוגדרת בסביבת $0$ אז היא חלקה בסביבת $0$ ונגזרותיה הן המומנטים של $X$.
\end{proposition}
\begin{proof}[הוכחה במקרה של תומך סופי]
	\[
		M_X(t)
		= \sum_{s \in \supp X} e^{ts} \PP(X = s)
	\]
	ולכן
	\[
		M_X'(t)
		= \sum_{s \in \supp X} s e^{ts} \PP(X = s)
	\]
	ובאופן כללי
	\[
		M_X^{(k)}(t)
		= \sum_{s \in \supp X} s^k e^{ts} \PP(X = s)
	\]
	ולכן
	\[
		M_X^{(k)}(0)
		= \sum_{s \in \supp X} s^k \PP(X = s)
		= \EE(X^k)
	\]
\end{proof}

\subsection{מבוא למרחבי הסתברות רציפים}
ניזכר שהגדרנו $(\Omega, \mathcal{F}, \PP)$ עבור מרחב הסתברות, אבל לא דיברנו על המשמעות של $\mathcal{F}$ כדי להבין מה היכולות האמיתיות של ההגדרה שלנו.
ננסה עתה להגדיר מרחב הסתברות לא בדיד בצורה נאיבית. למען הפשטות נבחן את $\Omega = [0, 1]$.
אנו רוצים ש־$\PP([0, \frac{1}{2}]) = \frac{1}{2}$ וכן ש־$\PP([\frac{1}{2}, 1]) = \frac{1}{2}$ אבל אז נובע ישירות ש־$\PP(\{\frac{1}{2}\}) = 0$.
ככלל נגדיר ש־$\PP([a, b]) = b - a$.
אבל בהגדרה זו אנו נתקלים בפרדוקס בשם בנך־טרסקי.
אפשר לחלק את כדור היחידה ב־$\RR^3$ למספר סופי של חלקים זרים ולהזיז ולסובב את החלקים ולהרכיב מהם שני כדורי יחידה.
כדי לא להיתקל בפרדוקס הזה אנו הולכים להשתמש במידה ולהגביל את $\mathcal{F}$.

\section{תרגול 10 --- 9.1.2025}
\subsection{פונקציות יוצרות מומנטים}
ניזכר כי המומנט ה־$k$ של $X$ הוא $\EE(X^k)$.
\begin{example}
	נניח ש־$X \sim Ber(p)$, אז $\forall t \in \RR$,
	\[
		M_X(t)
		= \EE(e^{tX})
		= \sum_{x \in \supp X} e^{tx} \PP(X = x)
		= e^0 \PP(X = 0) + e^t \PP(X = 1)
		= (1 - p) + p e^t
	\]
\end{example}
\begin{example}
	נניח ש־$X \sim Bin(n, p)$, אז נשתמש בקשר להתפלגות ברנולי ונקבל
	\[
		M_X(t)
		= \EE(e^{tX})
		= \EE(e^{t \sum X_i})
		= \EE(\prod_{i = 1}^n e^{t X_i})
		= \prod_{i = 1}^n M_{X_i}(t)
		= {((1 - p) + p e^t)}^n
	\]
\end{example}
\begin{example}
	נגיד ש־$X \sim \zeta(2)$ אם $\supp X = \NN$ וכן
	\[
		\PP(X = n)
		= \frac{1}{c n^2}
	\]
	כאשר
	\[
		c = \sum \frac{1}{n^2} = \frac{\pi^2}{6}
	\]
	ולכן
	\[
		M_X(t)
		= \EE(e^{tX})
		= \sum_{n = 1}^{\infty} \frac{e^{tn}}{c n^2}
		< \infty
		\iff
		t \le 0
	\]
	אז $\sum \frac{1}{n} \sim \EE(X)$ ולכן $X$ חסר תוחלת.
\end{example}
\begin{remark}
	נראה סימון נוסף למה שכבר ראינו על מומנטים,
	\[
		\left. \frac{\partial^k M_X(t)}{\partial^k t} \right\lvert_{t = 0} = \EE(X^k)
	\]
\end{remark}

\subsection{אי־שוויון צ'רנוף}
\begin{example}
	נניח ש־$X \sim Poi(\lambda)$ ואנו רוצים לחסום את $\PP(|X - \lambda| > \lambda)$. \\
	נתחיל בחסימה על־ידי צ'בישב, נובע
	\[
		\PP(|X - \lambda| > \lambda)
		< \frac{\var(X)}{\lambda^2}
		= \frac{1}{\lambda}
	\]
	ועתה נשתמש באי־שוויון צ'רנוף,
	\[
		\{ |X - \lambda| > \lambda \}
		= \{ X - \lambda > \lambda \} \cup \overbrace{\{X - \lambda < - \lambda \}}^{= 0}
	\]
	ולכן
	\[
		\PP(|X - \lambda| > \lambda)
		= \PP(X > 2 \lambda)
		< M_\lambda(t) e^{-t 2\lambda}
		= e^{\lambda(e^t - 1)} e^{-t2\lambda}
		= \exp(\lambda e^t - \lambda - 2 \lambda t)
	\]
	נגזור את $f(t) = \lambda e^t - \lambda - 2 \lambda t$ כדי למצוא את החסם היעיל ביותר,
	\[
		f'(t)
		= \lambda e^t - 2 \lambda
		= 0
		\iff
		t = \log 2
	\]
	בנוסף
	\[
		f''(t) = \lambda e^t > 0
	\]
	ולכן זוהי נקודת מינימום, וקיבלנו מאי־השוויון את החסם
	\[
		\PP(|X - \lambda| > \lambda)
		< \exp(\lambda e^{\log 2} - \lambda - 2 \lambda \log 2)
		= \exp (2\lambda - \lambda - 2 \lambda \log 2)
		= \frac{e^\lambda}{e^{\log 2 \cdot 2 \lambda}}
		= {\left(\frac{e}{4}\right)}^\lambda
	\]
	זהו כמובן חסם הרבה יותר הדוק, ואחד שנותן לנו מידע נוסף על התנהגות ההתפלגות.
\end{example}
\begin{exercise}
	מטילים מטבע הוגן $n$ פעמים באופן בלתי־תלוי.
	נגדיר את $X$ להיות מספר הפעמים שיצא הרצף $1, 1$.
	חסמו את $\PP(X < \frac{n - 1}{4} - c)$ לכל $c > 0$.
\end{exercise}
\begin{solution}
	$X \sim ber(\frac{1}{4})$ שכן $X$ מציין שיצא $1, 1$ בהטלות $i, i + 1$, כאשר $X = \sum_{i = 1}^{n - 1} X_i$. \\
	נגדיר $Y_i = \frac{1}{4} - X_i$ כדי למרכז ולהתמודד עם הכיוון של אי־השוויון שאנו מחפשים. \\
	נחשב ונקבל ש־$\supp Y_i = \{\frac{1}{4}, - \frac{3}{4} \} \subseteq [-1, 1]$ כפי שאנו רוצים.
	\begin{align*}
		\PP(X < \frac{n - 1}{3} - c)
		& = \PP( \sum X_i < \frac{n - 1}{4} - c) \\
		& = \PP(\sum(\frac{1}{4} - Y_i) < \frac{n - 1}{4} - c) \\
		& = \PP(\frac{n - 1}{4} - \sum Y_i < \frac{n - 1}{4} - c) \\
		& = \PP(\sum Y_i > c)
	\end{align*}
	ולכן נותר לחשב את $\PP(\sum Y_i > c)$, אבל בין המשתנים הללו יש תלות (שלא כמו $X_i$), כדי להשתמש באי־שוויון צ'רנוף עלינו להתמודד עם בעיה זו.
	לשם כך נגדיר שני סכומים נפרדים,
	\[
		S_1 = \sum_{\substack{i = 1 \\ i \equiv 0 \mod 2}}^{n - 1} Y_i,
		\qquad
		S_2 = \sum_{\substack{i = 1 \\ i \equiv 1 \mod 2}}^{n - 1} Y_i
	\]
	עדיין יש תלות בין $S_1$ ו־$S_2$, אז נשתמש בעובדה ש־$\{S_1, S_2\} \subseteq \{ S_1 > \frac{c}{2} \} \cup \{ S_2 > \frac{c}{2} \}$, לכן מאי־שוויון הופדינג,
	\[
		\PP(\sum Y_i > c)
		= \PP(S_1 + S_2 > c)
		\le \PP(S_1 > \frac{c}{2}) + \PP(S_2 > \frac{c}{2})
		< \begin{cases}
			2\exp(\frac{- {(\frac{c}{2})}^2}{n - 1}) & n - 1 \equiv 0 \mod 2 \\
			\exp(\frac{- {(\frac{c}{2})}^2}{n - 2}) + \exp(\frac{- {(\frac{c}{2})}^2}{n}) & n - 1 \equiv 1 \mod 2
		\end{cases}
	\]
\end{solution}

\section{שיעור 20 --- 9.1.2025}
\subsection{משתנים מקריים לא בדידים}
לפני שאנחנו מתחילים לעבוד עם משתנים כאלה, חשוב שנבין קודם איפה הם בכלל מופיעים, ומה המשמעות שלהם.
במקרים בדידים ראינו מספר גדול מאוד של דוגמות לשאלות הסתברותיות על מקרים סופיים או בדידים, ועתה נראה דוגמות עבור המקרים הלא בדידים.
ככלל, נדבר פה על מקרים שבהם יש לנו שאלות שהרזולוציה שלהן היא לא טבעית, כשלדוגמה ראשונה נוכל לדבר על משקלים, אלו הם מספרים שתנים שניתנים לדיוק כרצוננו, ואנו יכולים לדבר על התפלגות המשקל של אדם ברזולוציות שונות.
המשמעות היא שמשתנה מקרי לא בדיד הוא משתנה מקרי שכשנמדוד אותו בכל אמת מידה נקבל התפלגות יחסית לאמת המידה, כך לדוגמה נוכל למדוד משקל בקילוגרמים, בגרמים, במיקרוגרמים וכן הלאה, בכל פעם נקבל אמות מידה מדויקות יותר ויותר.
לכן הפעם במקום לשאול למה שווה משתנה מקרי, נשאל את השאלה מתי המשתנה המקרי נמצא בתחומי קטע מסוים, אך במקום זה נתאר את המקרים שבהם המשתנה המקרי נמצא בקרן, מתוך היכולת לחשב התפלגות בקטעים על־ידי חיסור קרניים.
\begin{definition}[פונקציית התפלגות צוברת]
	$X$ משתנה מקרי.
	פונקציית ההתפלגות המצטברת של $X$,
	\[
		F_X : \RR \to [0, 1],
		\qquad
		F_X(a) = \PP(X \le a)
	\]
	מייצגת את ההתפלגות של קרניים כפי שרצינו.
\end{definition}
\begin{remark}
	$F_X$ תלויה רק בהתפלגות $X$.
\end{remark}
נעבור לתכונות פונקציות מעין אלה.
\begin{proposition}[תכונות של פונקציית התפלגות מצטברת]
	אם $X$ משתנה מקרי אז
	\begin{enumerate}
		\item $F_X$ מונוטונית עולה (במובן החלש), $\forall a < b, F_X(a) \le F_X(b)$
		\item $\lim_{a \to \infty} F_X(a) = 1$ וכן $\lim_{a \to -\infty} F_X(a) = 0$
		\item $F_X$ רציפה מימין, $\lim_{a \to b^+} F_X(a) = F_x(b)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	נוכיח את כלל התכונות.
	\begin{enumerate}
		\item $\forall a < b, F_X(a) = \PP(X \le a) \le \PP(X \le b) = F_X(b)$
		\item $\lim_{a \to \infty} F_X(a) = \lim_{n \to \infty} F_X(n) = \lim_{n \to \infty} \PP(X \le n)$,
			עברנו מהמקרה הרציף לגבול בדיד, ואז נקבל סדרה עולה של מאורעות מכילים ואז המסקנה נובעת ממשפט\ \ref{probability_continuous_probability_function_theorem},
			\[
				\lim_{n \to \infty} \PP(X \le n)
				= \PP(\Omega)
				= 1
			\]
			באופן דומה גם
			\[
				\lim_{a \to -\infty} F_X(a)
				= \lim_{n \to -\infty} F_X(a)
				= \lim_{n \to -\infty} \PP(X \le -n)
				= \PP(\emptyset)
				= 0
			\]
		\item נשתמש בעובדה שפונקציה מונוטונית וחסומה היא בעלת גבול, ולכן
			\[
				\lim_{a \to b^+} F_X(a)
				= \lim_{n \to \infty} F_X(b + \frac{1}{n})
				= \lim_{n \to \infty} \PP(X \le b + \frac{1}{n})
			\]
			ומרציפות פונקציית ההסתברות ביטוי זה שווה ל־$\PP(X \le b) = F_X(b)$.
	\end{enumerate}
\end{proof}
\begin{example}
	נניח ש־$X \sim Ber(p)$ ונבין את התנהגות $F_X$. \\
	כאשר $a < 0$ אז $F_X(a) = 0$, כאשר $0 \le a < 1$ אז $F_X(a) = 1 - p$ ולבסוף בשאר התחום $F_X(a) = 1$.
\end{example}
\begin{proposition}
	אם $X$ משתנה מקרי, אז $\PP(X = a) = F_X(a) - \lim_{b \to a^-} F_X(b)$.
\end{proposition}
\begin{proof}
	\begin{align*}
		F_X(a) - \lim_{b \to a^-} F_X(b)
		& = F_X(a) - \lim_{n \to \infty} F_X(a - \frac{1}{n}) \\
		& = \lim_{n \to \infty} F_X(a) - F_X(a - \frac{1}{n}) \\
		& = \lim_{n \to \infty} \PP(X \le a) - \PP(X \le a - \frac{1}{n}) \\
		& = \lim_{n \to \infty} \PP(a - \frac{1}{n} < X \le a) \\
		& \overset{(1)}{=} \PP(X = a)
	\end{align*}
	כאשר $(1)$ נובע מ־\ref{probability_continuous_probability_function_theorem}
\end{proof}
\begin{proposition}
	אם $X, Y$ משתנים מקריים כך ש־$F_X = F_Y$ אז $X \overset{d}{=} Y$, כלומר המשתנים שווי התפלגות.
\end{proposition}
\begin{proof}[הוכחה (עבור משתנים מקריים בדידים)]
	הראינו שניתן לחשב את $\PP(X = a)$ מתוך $F_X$.
\end{proof}
את ההוכחה למקרה הכללי לא נוכל להראות בקורס זה שכן היא מתבססת על תורת המידה.
\begin{proposition}
	אם $F$ מקיימת את שלוש התכונות שראינו זה עתה, אז קיים משתנה מקרי $X$ (על מרחב כלשהו) כך ש־$F \equiv F_X$.
\end{proposition}
גם את הטענה הזו לא נוכל להוכיח בתחומי קורס זה.
טענה זו כמובן חזקה במיוחד, שכן היא מספקת אפיון מלא למשתנים מקריים על־ידי הפונקציות המצטברות שלהם.
\begin{definition}[משתנה מקרי רציף]
	משתנה מקרי $X$ הוא \textbf{רציף} אם $F_X$ רציפה. \\
	טענה זו שקולה למקרה ש־$\PP(X = a) = 0$ לכל $a$.
\end{definition}
הגדרה זו היא הגדרה בעייתית במקצת, היא לא מתכתבת עם ההגדרה הנפוצה בספרות המתמטית, והגדרנו אותה כך לצורך היכולת להבין בין כמה מקרים שנראה בקרוב.
\begin{example}
	קיים משתנה מקרי $X$ עבורו $F_X$ מוגדרת על־ידי
	\[
		F_X(a) = \begin{cases}
			0 & a \le 0 \\
			a & 0 < a \le 1 \\
			1 & 1 < a
		\end{cases}
	\]
	עבור $X$ זה מתקיים $\PP(X \in [a, b]) = b - a$ לכל $0 \le a \le b \le 1$.
	זאת שכן
	\[
		\PP(X \in [a, b])
		= \PP(a \le X \le b)
		= \PP(X \le b) - \PP(X < a)
		\overset{(1)}{=} \PP(X \le b) - \PP(X \le a)
		= F_X(b) - F_X(a)
		= b - a
	\]
	כאשר $(1)$ נובע מרציפות.
\end{example}
הפונקציה שראינו זה עתה היא פונקציה רציפה, ולכן היא אינטגרל של איזושהי פונקציה אחרת, רעיון זה נותן לנו השראה להגדרה נוספת ושימושית מאוד,
\begin{definition}[משתנה מקרי רציף בהחלט]
	משתנה מקרי נקרא \textbf{רציף בהחלט} אם קיימת פונקציה $f_X : \RR \to [0, \infty)$ אינטגרבילית כך שמתקיים
	\[
		F_X(a) = \int_{-\infty}^a f_X(s)\ ds
	\]
\end{definition}
פונקציה $f_X$ כזו היא למעשה המקבילה של פונקציית ההסתברות הבדידה $p(\omega)$ שראינו כבר, והיא מספקת אפיון נוסף להתנהגות ההתפלגות.
\begin{remark}
	רציפות בהחלט גוררת רציפות, זאת שכן האינטגרל של פונקציה הוא פונקציה רציפה.
\end{remark}
\begin{definition}[פונקציית הצפיפות]
	ל־$f_X$ קוראים פונקציית ה\textbf{צפיפות} של $X$.
\end{definition}
\begin{example}
	בדוגמה שראינו קודם מתקיים
	\[
		f_X(s) = \begin{cases}
			1 & 0 \le s \le 1 \\
			0 & \text{else}
		\end{cases}
	\]
\end{example}
\begin{proposition}
	אם $f : \RR \to [0, \infty)$ מקיימת
	\[
		\int_{-\infty}^\infty f(s)\ ds = 1
	\]
	אז קיים משתנה מקרי $X$ (על מרחב כלשהו) עבורו $f$ היא פונקציית צפיפות.
\end{proposition}
\begin{proof}
	נגדיר
	\[
		F(a) = \int_{-\infty}^a f(s)\ ds
	\]
	ונראה ששלוש התכונות הדרושות מתקיימות, כך שהטענה חלה.
\end{proof}
נראה טענה שמקבילה אף יותר את פונקציית הצפיפות לפונקציית ההסתברות הבדידה,
\begin{proposition}
	נניח ש־$X$ משתנה מקרי עם צפיפות $f_X$ אז $\PP(a \le X \le b) = \int_a^b f_X(s)\ ds$.
\end{proposition}
\begin{proof}
	\begin{align*}
		\PP(a \le X \le b)
		& = \PP(X \le b) - \PP(X < a) \\
		& = \PP(X \le b) - \PP(X \le a) \\
		& = F_X(b) - F_X(a) \\
		& = \int_{-\infty}^b f_X(s)\ ds - \int_{-\infty}^a f_X(s)\ ds \\
		& = \int_a^b f_X(s)\ ds
	\end{align*}
\end{proof}
נבחן עתה מספר התפלגויות חשובות.
\begin{definition}[התפלגות רציפה אחידה]
	$X$ מתפלג אחיד על $[a, b]$, ונסמן $X \sim Unif([a, b])$, אם
	\[
		f_X(s) = \begin{cases}
			\frac{1}{b - a} & a \le s \le b \\
			0 & \text{else}
		\end{cases}
	\]
\end{definition}
\begin{definition}[התפלגות מעריכית]
	$X$ מתפלג מעריכית אם פרמטר $\lambda$, ונסמן $X \sim Exp(\lambda)$, אם
	\[
		f_X(s) = \begin{cases}
			\lambda e^{-\lambda s} & 0 \le s \\
			0 & s < 0
		\end{cases}
	\]
\end{definition}
נצטרך להראות שהגדרה זו בכלל תקפה על־ידי בדיקת האינטגרל לפי התנאי שמצאנו קודם,
\[
	\int_{-\infty}^\infty f_X(s)\ ds
	= \int_0^\infty \lambda e^{-\lambda s}\ ds
	= -e^{\lambda s} \mid_{s = 0}^{s = \infty}
	= 0 - (-1)
	= 1
\]
\begin{definition}[התפלגות נורמלית סטנדרטית]\label{standard_normal_distribution}
	$X$ מתפלג נורמלי סטנדרטי, אם
	\[
		f_X(s) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{s^2}{2}}
	\]
\end{definition}
כאשר ראינו באינפי 3 את ערך אינטגרל זה ובהתאם את ההצדקה להגדרה זו.
\begin{example}
	נניח ש־$X \sim Exp(\lambda)$, ואנו רוצים למצוא $a$ עבורו $\PP(X \le a) = \frac{1}{2}$, ערך זה נקרא גם חציון.
	\[
		\int_{-\infty}^{\infty} f_X(s)\ ds
		= \int_{0}^{\infty} \lambda e^{-\lambda s}\ ds
		= -e^{-\lambda a} - (-e^0)
		= 1 - e^{-\lambda a}
		= \frac{1}{2}
		\implies -\lambda a = - \ln 2
	\]
	ולכן $a = \frac{\ln 2}{\lambda}$.
\end{example}
\begin{example}
	נניח ש־$X \sim Unif([0, 1])$ ואנו רוצים למצוא צפיפות של $Y = X^2$. \\
	נחשב את $F_Y$ ונגזור,
	\[
		F_Y(a)
		= \PP(Y \le a)
		= \PP(X^2 \le a)
		= \PP(-\sqrt{a} \le X \le \sqrt{a})
		= \int_{-\sqrt{a}}^{\sqrt{a}} f_X(s)\ ds
		= \int_0^{\sqrt{a}} 1\ ds
		= \sqrt{a}
	\]
	ולכן
	\[
		F_Y(a) = \begin{cases}
			1 & a \ge 1 \\
			\sqrt{a} & 0 \le a \le 1 \\
			0 & a \le 0
		\end{cases}
	\]
	אם נבחר $f_Y = F_y'$ נקבל צפיפות עבור $Y$,
	\[
		f_Y(a)
		= \begin{cases}
			\frac{1}{2 \sqrt{a}} & 0 \le a \le 1 \\
			0 & \text{else}
		\end{cases}
	\]
\end{example}

\section{שיעור 21 --- 14.1.2025}
\subsection{מעבר לעולם הרציף}
\begin{definition}[תוחלת רציפה]
	$X$ משתנה מקרי רציף בעל פונקציית צפיפות $f_X$ אז
	\[
		\EE(X)
		= \int_{-\infty}^{\infty} s f_X(s)\ ds
	\]
	ונבחין שעל האינטגרל להתכנס בהחלט כדי שנוכל לומר שהתוחלת מתכנסת.
\end{definition}
נבחין שבעולם של תורת המידה הגדרה זו, יחד עם ההגדרה הבדידה, הן מקרים פרטיים של הגדרה רחבה יותר. לא נראה אותה במסגרת הקורס.
\begin{proposition}
	אם $Y = g(X)$ אז
	\[
		\EE(Y)
		= \int_{-\infty}^{\infty} g(s) f_X(s)\ ds
	\]
	טענה זו נכונה גם כאשר $Y$ לא רציף בהחלט (רציף).
\end{proposition}
כל התכונות שראינו עד היום על תוחלות, שונות, שונות משותפת וכן הלאה שראינו למשתנים מקריים בדידים נכונים גם עברו המקרה הרציף.
כולל אי־שוויון מרקוב, אי־שוויון צ'בישב, אי־שוויון הופדינג ואף צ'רנוף.
\begin{example}
	$X \sim Unif([0, 1])$, אז $f_X(s) = \begin{cases}
		1 & 0 \le s \le 1 \\
		0 & \text{else}
	\end{cases}$, ולכן
	\[
		\EE(X)
		= \int_{-\infty}^{\infty} s f_X(s)\ ds
		= \int_0^1 s\ ds
		\left. \frac{s^2}{2} \right\rvert_{s = 0}^{s = 1}
		= \frac{1}{2}
	\]
	וכן גם
	\[
		\EE(X^2)
		= \int_{-\infty}^{\infty} s^2 f_X(s)\ ds
		= \int_0^1 s^2\ ds
		= \left. \frac{s^3}{3} \right\rvert_{s = 0}^{s = 1}
		= \frac{1}{3}
	\]
	ונסיק
	\[
		\var(X)
		= \EE(X^2) - {\EE(X)}^2
		= \frac{1}{3} - \frac{1}{2^2}
		= \frac{1}{12}
	\]
	באופן דומה כאשר $t \ne 0$ אז,
	\[
		M_X(t)
		= \EE(e^{tX})
		= \int_{-\infty}^{\infty} e^{ts} f_X(s)\ ds
		= \int_0^1 e^{ts}\ ds
		= \left. \frac{e^{ts}}{t} \right\rvert_{s = 0}^{s = 1}
		= \frac{e^t - 1}{t}
	\]
	וכן $t = 0$ גורר ש־$M_X(t) = 1$.
\end{example}

\subsection{צפיפות משותפת}
\begin{definition}[וקטור מקרי רציף]
	$(X, Y)$ וקטור מקרי עם צפיפות $f_{X Y}$ אם
	\[
		\PP((X, Y) \in D)
		= \int_D f_{X Y}(s, t)\ dt\ ds
	\]
\end{definition}
וניזכר במשפט פוביני שיאפשר לנו לחשב את האינטגרלים הללו:
\begin{theorem}[משפט פוביני]
	תהי $f : D \to \RR$ עבור $D [a, b] \times [c, d] \subseteq \RR^2$ אז
	\[
		\int_D f(s, t)\ ds\ dt
		= \int_a^b \left( \int_c^d f(s, t)\ dt \right)\ ds
		= \int_c^d \left( \int_a^b f(s, t)\ ds \right)\ dt
	\]
\end{theorem}
אם $D$ היא קטע מורכב יותר, לדוגמה מהצורה $D = \{ (x, y) \in \RR^2 \mid a \le x \le b, c(x) \le b \le d(x)$ עבור פונקציות $c, d : [a, b] \to \RR$ מתאימות, אז נוכל לחשב
\[
	\int_D f(x, y)\ ds\ dt
	= \int_a^b \int_{c(s)}^{d(s)} f(s, t)\ dt\ ds
\]
\begin{example}
	נגדיר
	\[
		f_{X Y}(s, t)
		= \begin{cases}
			6s & 0 \le s, 0 \le t, s + t \le 1 \\
			0 & \text{else}
		\end{cases}
	\]
	ונרצה לראות אם היא מגדירה פונקציית צפיפות,
	\[
		\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X Y}(s, t)\ dt\ ds
		= \int_0^1 \int_0^{1 - t} 6s\ ds\ dt
		= \int_0^1 \left. 3s^2 \right\rvert_{s = 0}^{s = 1 - t}\ dt
		= \int_0^1 3{(1 - t)}^2\ dt
		= \left. - {(1 - t)}^3 \right\rvert_{t = 0}^{t = 1}
		= 0 - {(-1)}^3
		= 1
	\]
	והתנאי ההכרחי לפונקציית צפיפות אכן מתקיים, וזוהי אכן פונקציית צפיפות.
\end{example}
\begin{remark}
	התוחלת של וקטור מקרי באופן מאוד דומה תהיה
	\[
		\EE(g(X, Y))
		= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(s, t) f_{X Y}(s, t)\ dt\ dt
	\]
\end{remark}
\begin{proposition}[צפיפות שולית]
	אם $X$ ו־$Y$ הם משתנים מקריים רציפים בעלי צפיפות משותפת $f_{X Y}$, אז
	\[
		f_X(s)
		= \int_{-\infty}^{\infty} f_{X Y}(s, t)\ dt
	\]
\end{proposition}
\begin{proof}
	\[
		\PP(X \le a)
		= \int_{-\infty}^a \int_{-\infty}^{\infty} f_{X Y}(s, t)\ ds\ dt
	\]
	אבל מההגדרה יש התכנסות ולכן יש הצדקה להגדרה
	\[
		f_X(s)
		= \int_{-\infty}^{\infty} f_{X Y}(s, t)\ dt
	\]
	ומתקבל
	\[
		\PP(X \le a)
		= \int_{-\infty}^a f_X(s)\ ds
	\]
	כפי שרצינו.
\end{proof}
\begin{definition}[חוסר תלות במשתנים מקריים רציפים]
	$X$ ו־$Y$ בלתי־תלויים אם
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \PP(Y \in T)
	\]
\end{definition}
זוהי ההגדרה המלאה גם עבור המקרה הבדיד, אך כמו במקרה הבדיד ישנה הגדרה שקולה שתהיה לנו לעזר,
\begin{definition}[הגדרה שקולה לאי־תלות משתנים מקריים רציפים]
	אם $X$ ו־$Y$ משתנים מקריים רציפים בעלי צפיפות משותפת $f_{X Y}$ המקיימת
	\[
		f_{X Y}(s, t) = f_X(s) f_Y(t)
	\]
	אז $X$ ו־$Y$ בלתי־תלויים.
\end{definition}
\begin{example}
	נגדיר
	\[
		f_{X Y}(s, t)
		= \begin{cases}
			1 & 0 \le s, t \le 1 \\
			0 & \text{else}
		\end{cases}
	\]
	אז
	\[
		f_X(s)
		= \int_{-\infty}^{\infty} f_{X Y}(s, t)
		= \begin{cases}
			\int_{0}^{1} 1\ dt = 1 & 0 \le s \le 1 \\
			0 & \text{else}
		\end{cases}
	\]
	ומצאנו שאכן תכונת חוסר התלות חלה.
\end{example}

\section{תרגול 11 --- 16.1.2025}
\subsection{משתנים מקריים רציפים בהחלט}
\begin{exercise}
	יהי $X$ משתנה מקרי רציף בהחלט עם פונקציית צפיפות
	\[
		f_X(t) = \begin{cases}
			c(2t - t^2) & t \in [0, 2] \\
			0 & \text{else}
		\end{cases}
	\]
	חשבו את $c$ וחשבו את ההתפלגות המצטברת.
\end{exercise}
\begin{solution}
	נשתמש בתנאי ההכרחי לפונקציית צפיפות,
	\[
		1
		= \PP(X \in \RR)
		= \int_{-\infty}^{\infty} f_X(t)\ dt
		= \int_{0}^{2} c (2t - t^2)\ dt
		= \left. c (t^2 - \frac{1}{3} t^3) \right\rvert_{t = 0}^{t = 2}
		= c \cdot \frac{4}{3}
	\]
	ולכן $c = \frac{3}{4}$ בלבד.
	
	נעבור לחישוב פונקציית ההתפלגות המצטברת,
	\[
		F_X(s)
		= \PP(X \le s)
		= \int_{-\infty}^{s} f_X(t)\ dt
	\]
	כשאר $s \in [0, 2]$,
	\[
		F_X(s)
		= \int_{0}^{s} \frac{3}{4} (2t - t^2)\ dt
		= \frac{3}{4} (s^2 - \frac{1}{3} s^3)
	\]
	אם $s < 0$ אז $F_X(s) = 0$ ואם $s > 2$ אז $F_X(s) = 1$.
\end{solution}
\begin{example}
	נניח ש־$X \sim Exp(\lambda)$, אז
	\[
		F_X(t)
		= \int_{-\infty}^{t} f_X(x)\ dx
		= \int_{0}^{t} \lambda e^{-\lambda x}\ dx
		= \left. -e^{-\lambda x} \right\rvert_{x = 0}^{x = t}
		= 1 - e^{-\lambda t}
	\]
\end{example}
\begin{example}
	זמן שיחת טלפון ממוצעת מתפלגת מעריכית $\lambda = \frac{1}{10}$, מה ההסתברות ששיחה נמשכה יותר מעשר דקות? \\
	אנו מחפשים את $\PP(X > 10) = 1 - \PP(X \le 10) = 1 - 1 - e^{-\frac{1}{10} \cdot 10} = e^{-1}$. \\
	נניח שהשיחה נמשכה יותר מ־10 דקות ונחשב את ההסתברות שהיא נמשכה יותר מ־20 דקות,
	\[
		\PP(X > 20 \mid X > 10)
		= \frac{\PP(X > 20, X > 10)}{\PP(X > 10)}
		= \frac{e^{-2}}{e^{-1}}
		= e^{-1}
	\]
	אנו רוצים לבדוק אם תכונת חוסר הזיכרון חלה על התפלגות זו, נבדוק.
	\[
		\PP(X > s + t \mid X > t)
		= \frac{e^{-\lambda(s + t)}}{e^{-\lambda t}}
		= e^{-\lambda s}
		= \PP(X > s)
	\]
\end{example}
\begin{example}
	נגדיר $Y = [x]$ וכן $X \sim Exp(\lambda)$, אז $\supp Y = \NN \cup \{ 0 \}$,
	\[
		\PP(Y = n)
		= \PP([x] = n)
		= \PP(n \le X \le n + 1)
		= \int_{n}^{n + 1} \lambda e^{-\lambda t}\ dt
		= e^{-\lambda n} - e^{-\lambda (n + 1)}
		= e^{-\lambda n} (1 - e^{-\lambda}) - 1
		\sim Geo(1 - e^{-\lambda})
	\]
	משתנה מקרי גאומטרי ומשתנה מקרי מעריכי הם אם כך בעלי קשר, לא במקרה שניהם בעלי ההתפלגויות חסרות הזיכרון היחידות.
\end{example}
\begin{exercise}
	נניח ש־$X \sim Exp(\lambda)$ ונגדיר $Z = \log X$, חשבו את ההתפלגות של $Z$.
\end{exercise}
\begin{solution}
	יהי $t \in \RR$, אז
	\[
		F_X(t)
		= \PP(Z < t)
		= \PP(\log X < t)
		= \PP(X < e^t)
		= F_X(e^t)
		= 1 - e^{-\lambda e^t}
	\]
	וכן נחשב את פונקציית הצפיפות, $ f_X(t) = F_X'(t) = e^{-\lambda e^t} \cdot \lambda e^t$.
\end{solution}
\begin{example}
	נחשב את התוחלת של $X \sim Exp(\lambda)$,
	\[
		\EE(X)
		= \int_{-\infty}^{\infty} t f_X(t)\ dt
		= \int_{0}^{\infty} t \lambda e^{-\lambda t}\ dt
		= \frac{1}{\lambda} \int_{0}^{\infty} u e^{-u}\ du
		= \frac{1}{\lambda} ({\left( - u e^{-u} \right)}_{u = 0}^{u = \infty} + \int_{0}^{\infty} e^{-u}\ du)
		= \frac{1}{\lambda} (0 + (1 - 0))
		= \frac{1}{\lambda}
	\]
	באופן דומה גם
	\[
		\EE(X^2)
		= \int_{0}^{\infty} t^2 e^{-\lambda t}\ dt
		= \frac{1}{\lambda^2} \int_{0}^{\infty} u^2 e^{-u}\ du
		= \frac{1}{\lambda^2} ({\left(-u^2 e^{-u}\right)}_{u = 0}^{u = \infty} + 2 \int_{0}^{\infty} u e^{-u}\ duoo)
		= \frac{2}{\lambda^2}
		\implies
		\var(X)
		= \frac{1}{\lambda^2}
	\]
\end{example}

\section{שיעור 22 --- 16.1.2025}
\subsection{אי־תלות  במשתנים מקריים רציפים}
בשיעור הקודם דיברנו ואפיינו אי־תלות משתנים מקריים רציפים, נראה עתה דוגמה למקרים כאלה.
\begin{example}
	נניח ש־$X, Y \sim Exp(1)$ בלתי־תלויים ונרצה לחשב את $\PP(Y \ge 2X)$.
	נובע מההגדרה של התפלגות מעריכית,
	\[
		f_{X Y}(s, t)
		= f_X(s) f_Y(t)
		= \begin{cases}
			e^{-s - t} & t, s \ge 0 \\
			0 & \text{else}
		\end{cases}
	\]
	נעבור לחישוב ההתפלגות שאנו מחפשים,
	\[
		\PP(Y \ge 2X)
		= \int_{0}^{\infty} \int_{2s}^{\infty} e^{-s - t}\ dt\ ds
		= \int_{0}^{\infty} e^{-s} \int_{2s}^{\infty} e^{-t}\ dt\ ds
	\]
	וכן
	\[
		e^{-s} \int_{2s}^{\infty} e^{-t}\ dt
		= \left. -e^{-t} \right\rvert_{t = 2s}^{t = \infty}
		= 0 - (-e^{-2s})
		= e^{-2s}
	\]
	ובהתאם
	\[
		\int_{0}^{\infty} e^{-s} \int_{2s}^{\infty} e^{-t}\ dt\ ds
		= \int_{0}^{\infty} e^{-s} e^{-2s}\ ds
		= \int_{0}^{\infty} e^{-3s}\ ds
		= \left. -\frac{1}{3} e^{-3s} \right\rvert_{s = 0}^{s = \infty}
		= -\frac{1}{3}(0 - 1)
		= \frac{1}{3}
	\]
\end{example}

\subsection{התפלגות נורמלית}
\begin{proposition}[נוסחת הקונבולוציה]
	אם $X$ ו־$Y$ בעלי צפיפות $f_X$ ו־$f_Y$ בלתי־תלויים בהתאמה אז למשתנה המקרי $Z = X + Y$ יש צפיפות
	\[
		f_Z(t)
		= \int_{-\infty}^{\infty} f_X(s) f_Y(t - s)\ ds
	\]
	ניזכר שבמקרה הבדיד,
	\[
		\PP(Z = t)
		= \sum_{s \in \RR} \PP(X = s) \PP(Y = t - s)
	\]
\end{proposition}
את הטענה הזו לא נוכל להוכיח למקרה הרציף.
\begin{theorem}[משפט הגבול המרכזי]\label{central_limit_theorem}
	אם $\{ X_i \mid i < \NN \}$ משתנים מקריים בלתי־תלויים שווי־התפלגות עם $\EE(X_i) = 0, \var(X_i) = 1$ לכל $i \in \NN$ ונגדיר
	\[
		Z_n = \frac{\sum_{i = 1}^{n} X_i}{\sqrt{n}}
	\]
	(אז $\EE(Z_n) = 0, \var(Z_n) = 1$)
	אז
	\[
		\PP(Z_n \le a) \xrightarrow[n \to \infty]{} \int_{-\infty}^{a} \frac{1}{\sqrt{2\pi}} e^{-\frac{s^2}{2}}\ ds
	\]
	כלומר התפלגות זו שווה עבור $\PP(Z \le a)$ עבור $Z \sim N(0, 1)$
\end{theorem}
הכוונה היא שהמשפט הוא המרכזי, לא הגבול.
נראה את ההגדרה המלאה להתפלגות נורמלית,
\begin{definition}[התפלגות נורמלית]
	$Z \sim N(\mu, \sigma^2)$ אם הצפיפות שלו מקיימת
	\[
		f_Z(s) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{{(s - \mu)}^2}{2\sigma^2}\right)
	\]
	כאשר $N(0, 1)$ נקראת ההתפלגות הנורמלית הסטנדרטית. \\
	אם $\mu$ משתנה, אז נקבל הזזה של ההתפלגות, אם נשנה את $\sigma^2$ אז נשנה את השונות, הרדיוס, הריווח של ההתפלגות.
\end{definition}
\begin{example}
	נניח ש־$a > 0$ וש־$b \in \RR$, נניח ש־$X \sim N(0, 1)$ ו־$Y = aX + b$, אנו יודעים שמתקיים
	\[
		f_X(s)
		= \frac{1}{\sqrt{2 \pi}} \exp(-\frac{s^2}{2})
	\]
	ונרצה לחשב את $f_Y(t)$,
	\[
		F_Y(t)
		= \PP(Y \le t)
		= \PP(aX + b \le t)
		= \PP(X \le \frac{t - b}{a})
		= \int_{-\infty}^{\frac{t - b}{a}} \frac{1}{\sqrt{2 \pi}} \exp(-\frac{s^2}{2})\ ds
		= F_X(\frac{t - b}{a})
	\]
	אבל
	\[
		F_Y'(t)
		= F_X'\left(\frac{t - b}{a}\right) \cdot \frac{1}{a}
		= \frac{1}{\sqrt{2\pi a^2}} \exp\left(- \frac{{(t - b)}^2}{2a^2}\right)
	\]
\end{example}
\begin{example}
	נחשב את התוחלת של התפלגות נורמלית, נשתמש באי־זוגיות ונקבל,
	\[
		\EE(X)
		= \int_{-\infty}^{\infty} s \frac{1}{\sqrt{2 \pi}} \exp\left(- \frac{s^2}{2}\right)\ ds
		= 0
	\]
	באופן דומה על־ידי אינטגרציה בחלקים נובע
	\[
		\var(X)
		= \EE(X^2)
		= \int_{-\infty}^{\infty} s^2 \frac{1}{\sqrt{2\pi}} \exp\left(- \frac{s^2}{2}\right)\ ds
		= 1
	\]
\end{example}
\begin{conclusion}
	אם $Y \sim N(\mu, \sigma^2)$ אז $\EE(Y) = \mu$ ו־$\var(Y) = \sigma^2$.
\end{conclusion}
\begin{proposition}
	אם $X_i \sim N(\mu_i, \sigma_i^2)$ עבור $i \in [2]$ כך שהם בלתי־תלויים, אז
	\[
		X_1 + X_2
		\sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
	\]
\end{proposition}
ההוכחה מושארת כתרגיל ומופיעה בספר.

\subsection{התפלגות סדרת התפלגויות}
נדבר על התכנסות בהתפלגות של משתנים מקריים, נבחין שהשם מוזר קצת כי הוא קופץ מעל המשתנים המקריים ועובר ישר למשתנים המקריים, זאת שכן במקרים אלה הם לא מעניינים אותנו באופן ישיר.
\begin{definition}[התכנסות משתנים מקריים בהתפלגות]\label{convergence_in_distribution}
	${\{X_i\}}_{i \in \NN}$ סדרת משתנים מקריים, $Z$ משתנה מקרי רציף. \\
	נאמר ש־$X_n \xrightarrow{d} Z$ ($X_n$ מתכנסת בהתפלגות ל־$Z$) אם לכל $a \in \RR$,
	\[
		F_{X_n}(a) \xrightarrow[n \to \infty]{} F_Z(a)
	\]
\end{definition}
\begin{example}
	נניח ש־$X_n \sim U([n])$ אז אם $Y_n = \frac{X_n}{n}$ אז $Y_n \xrightarrow{d} Z$ כאשר $Z \sim([0, 1])$. \\
	נבחין שמההגדרה $Y_n \sim U(\{\frac{1}{n}, \frac{2}{n}, \dots, \frac{n}{n}\})$.
	נגדיר
	\[
		f_Z(s)
		= \begin{cases}
			1 & 0 \le s \le 1 \\
			0 & \text{else}
		\end{cases},
		\qquad
		F_Z(a)
		= \begin{cases}
			0 & a \le 0 \\
			a & 0 \le a \le 1 \\
			1 & \text{else}
		\end{cases}
	\]
	אם $a \le 0$ אז $F_Y(a) = 0 \to F_Z(a) = 0$.
	אם $a \ge 1$ אז $F_Y(a) = 1 \to F_Z(a) = 1$.
	אם $0 < a < 1$ אז
	\[
		F_{Y_n}(a)
		= \PP(Y_n \le a)
		= \PP(X_n \le n \cdot a)
		\le \frac{\lceil n \cdot a \rceil}{n}
		\xrightarrow{n \to \infty} a
	\]
\end{example}
נעבור לבחון את הקשר שבין התפלגות גאומטרית ומעריכית.
\begin{example}
	נניח ש־$X_n \sim Geo(\frac{\lambda}{n})$ עבור $\lambda > 0$.
	עבור $a > 0$,
	\begin{align*}
		F_{Y_n}(a)
		& = \PP(Y_n \le a) \\
		& = \PP(X_n \le na) \\
		& = 1 - \PP(X_n > nk) \\
		& = 1 - \sum_{k = \lfloor nk \rfloor + 1}^{\infty} \PP(X_n = k) \\
		& = 1 - \sum_{k = \lfloor nk \rfloor + 1}^{\infty} {(1 - \frac{\lambda}{n})}^{k - 1} \frac{\lambda}{n} \\
		& = 1 - {(1 - \frac{\lambda}{n})}^{k - 1} \frac{\lambda}{n} \sum_{l = 1}^{\infty} {(1 - \frac{\lambda}{n})}^l \\
		& = 1 - {(1 - \frac{\lambda}{n})}^{\lfloor na \rfloor} \\
		& = 1 - {({(1 - \frac{\lambda}{n})}^n)}^{\lfloor a \rfloor} \\
		& \to 1 - e^{-\lambda a}
	\end{align*}
	זאת אומרת ש־$F_Z(a) = 1 - e^{-\lambda a}$ ו־$Z \sim Exp(\lambda)$.
\end{example}

\section{שיעור 23 --- 21.1.2025}
\subsection{התכנסות משתנים מקריים}
\begin{proposition}
	$X_n$ סדרת משתנים מקריים כך ש־$\EE(X_n) = 0$ לכל $n \in \NN$, ו־$\var(X_n) \xrightarrow{n \to \infty} 0$, אז $X_n \xrightarrow{d} 0$.
\end{proposition}
\begin{proof}
	תרגיל, כאשר זוהי תוצאה של צ'בישב.
\end{proof}
\begin{proposition}
	אם $X_n$ משתנים מקריים בדידים הנתמכים על $\NN$, ו־$X$ גם נתמך על הטבעיים,
	אז $X_n \xrightarrow{d} X$ אם ורק אם $\PP(X_n = k) \xrightarrow{n \to \infty} \PP(X = k)$ לכל $k \in \NN$.
\end{proposition}
הראינו שאם $X_n \sim Bin(n, \frac{\lambda}{n})$ ו־$X \sim Poi(\lambda)$ אז מתקיימת הטענה ו־$X_n \xrightarrow{d} X$.
\begin{proof}
	נניח ש־$\supp X = \NN$.
	עבור $k < a < k + 1$ צריך לבדוק שמתקיים $F_{X_k}(a) = \PP(X_n \le a) = \sum_{i = 1}^{k} \PP(X_n = i)$,
	ושבאותו אופן $F_X(a) = \sum_{i = 1}^{k} \PP(X = i)$, לכן צריך להראות שלכל $k$,
	$\sum_{i = 1}^{k} \PP(X_n = i) \xrightarrow{n \to \infty} \sum_{i = 1}^{k} \PP(X = i)$ אם ורק אם $\PP(X_n = i) \xrightarrow{n \to \infty} \PP(X = i)$.

	הכיוון השני מיידי, ולכן נעבור להוכחת הכיוון הראשון, באינדוקציה על $k$.
	עבור $k = 1$ הטענה נובעת מיד, ואז נניח שהטענה נכונה עבור $k \in \NN$ ונבדוק את $k + 1$,
	אנו כבר יודעים שמתקיים
	\[
		\sum_{i = 1}^{k} \PP(X_n = i) + \PP(X_n = k + 1)
		\xrightarrow{m \to \infty} \sum_{i = 1}^{k} \PP(X = i) + \PP(X = k + 1)
	\]

	נראה הוכחה פשוטה אף יותר,
	\[
		\sum_{i = 1}^{k + 1} \PP(X_n = i) - \sum_{i = 1}^{k} \PP(X_n = i)
		\xrightarrow{n \to \infty}
		\sum_{i = 1}^{k + 1} \PP(X = i) - \sum_{i = 1}^{k} \PP(X = i)
	\]
	אבל למעשה מכאן נובע ישירות $\PP(X_n = k + 1) \to \PP(X = k + 1)$.
\end{proof}
ניזכר במשפט\ \ref{central_limit_theorem} ונראה לו דוגמה.
\begin{example}
	נניח $X_n \sim U([\sigma])$ בלתי־תלויים ואנו רוצים להעריך את
	\[
		\PP(3500 \le \sum_{i = 1}^{1000} X_i \le 3700)
	\]

	נתחיל בנרמול, נגדיר $Y_i = \frac{X_i - \frac{7}{2}}{\sqrt{\frac{35}{12}}}$, לכן נובע $\EE(Y_i) = 0, \var(Y_i) = 1$.
	נציין שגם $X_i = \sqrt{\frac{35}{12}} Y_i + \frac{7}{2}$, ובהתאם
	\begin{align*}
		\PP(3500 \le \sqrt{\frac{35}{12}} \sum_{i = 1}^{1000} Y_i + 3500 \le 3700)
		& = \PP(0 \le \sqrt{\frac{35}{12}} \sum_{i = 1}^{1000} Y_i \le 200) \\
		& = \PP(0 \le \sum_{i = 1}^{1000} Y_i \le \sqrt{\frac{12}{35}} 200) \\
		& = \PP(0 \le \frac{1}{\sqrt{1000}} \sum_{i = 1}^{1000} Y_i \le \frac{200}{\sqrt{\frac{35}{12}} \sqrt{1000}}) \\
		& \approx \int_0^1 \frac{1}{\sqrt{2 \pi}} e^{-\frac{s^2}{2}}\ ds
	\end{align*}
\end{example}
\begin{example}
	נרצה לחשב את הגבול של $e^{-n} \sum_{k = 0}^{n} \frac{n^k}{k!}$.
	נוכל כמובן להשתמש בטור טיילור ולנסות לחשב את הערך הזה ישירות, אך נראה דרך נוספת.
	למעשה שאלה זו שקולה לחלוטין ל־$\PP(X \le n)$ עבור $X \sim Poi(n)$.
	ניזכר שגם אם $X \sim Poi(\lambda_1)$ וכן $Y \sim Poi(\lambda_2)$ אז $X + Y \sim Poi(\lambda_1 + \lambda_2)$ אז נסיק שאם $X_i \sim Poi(1)$ עבור $1 \le i \le n$ נקבל
	\[
		Poi(n) \sim \sum_{i = 1}^{n} X_i
	\]
	ולכן נוכל לנסח מחדש את השאלה כגבול $\PP(\sum_{i = 1}^{n} X_i \le n)$ עבור $X_i$ בלתי־תלויים המתפלגים פואסונית $1$.
	נבחין שגם $\EE(X_i) = 1$ ולכן ערך זה שקול לערך
	\[
		\PP(\sum_{i = 1}^{n} (X_i - 1) \le 0)
	\]
	נגדיר $Y_i = X_i - 1$ בלתי־תלויים כך ש־$\EE(Y_i) = 0$ ו־$\var(Y_i) = 1$.
	אז ממשפט הגבול המרכזי נסיק
	\[
		\PP(\sum_{i = 1}^{n} (X_i - 1) \le 0)
		= \PP(\frac{\sum_{i = 1}^{n} (X_i - 1)}{\sqrt{n}} \le 0)
		\to \PP(Z \le 0)
		= \frac{1}{2}
	\]
\end{example}

\subsection{סדרות של מאורעות}
נעבור לדבר על העולם של סדרות של מאורעות, במטרה לאפיין את ההתנהגות של המאורעות האלה ולבנות כלים שיאפשרו לנו לשאול שאלות מוכללות על הסדרות, נעבור עוד רגע להגדיר שאלות כאלה באופן פורמלי ומדויק.
\begin{definition}[מאורעות מיוחדים עבור סדרת מאורעות]
	תהי ${\{A_n\}}_{n = 1}^\infty$ סדרת מאורעות. \\
	נגדיר
	\[
		\limsup_{n \to \infty} A_n = \{ A_n \text{ infinitely often} \} = \bigcap_{n = 1}^\infty \bigcup_{k = n}^\infty A_k
		= \{ \omega \in \Omega \mid \forall n \in \NN, \exists k \ge n, \omega \in A_k \}
	\]
	עבור המאורע שקורה בסדרה אינסוף פעמים.
	כלומר המקרה שאינסוף פעמים הם התרחשו, לדוגמה שאינסוף פעמים מטבע נפל על עץ.

	נגדיר גם
	\[
		\liminf_{n \to \infty} A_n
		= \{ A_n \text{ eventually} \}
		= \bigcup_{n = 1}^\infty \bigcap_{k = n}^\infty A_k
		= \{ \omega \in \Omega \mid \exists n \in \NN, \forall k \ge n, \omega \in A_k \}
	\]
	עבור מאורע שיקרה מתישהו.
\end{definition}
נסתכל על משתנים מקריים שמייצגים את המאורעות האלה, במטרה לנסות להבין את ההגדרות האלה.
\begin{exercise}
	נגדיר $1_{A_n}$, המשתנה המקרי המציין, אז נובע
	\[
		1_{\limsup_{n \to \infty} A_n}
		= \limsup_{n \to \infty} 1_{A_n}
	\]
	ובאופן דומה
	\[
		1_{\liminf_{n \to \infty} A_n}
		= \liminf_{n \to \infty} 1_{A_n}
	\]
\end{exercise}
\begin{remark}
	מדה־מורגן נובע
	\[
		\limsup_{n \to \infty} A_n^C
		= {(\liminf_{n \to \infty} A_n)}^C
	\]
\end{remark}

\begin{lemma}[הלמה של פאטו]
	עבור סדרה מאורעות $\{ A_n \}$ מתקיים,
	\[
		\PP(\liminf_{n \to \infty} A_n)
		\le 
		\liminf_{n \to \infty} \PP(A_n)
	\]
	וכן באופן דומה,
	\[
		\PP(\limsup_{n \to \infty} A_n)
		\ge 
		\limsup_{n \to \infty} \PP(A_n)
	\]
\end{lemma}
\begin{proof}
	נוכיח את אחד מהחסמים ואת השני נקבל מההערה.
	\[
		\PP(\liminf_{n \to \infty} A_n)
		= \PP( \bigcup_{n = 1}^\infty \bigcap_{k = n}^\infty A_k)
	\]
	ולכן נגדיר $B_n = \bigcap_{k = n}^\infty A_k$.
	זוהי כמובן סדרה עולה, זאת שכן ככל שאנו מתקדמים בסדרה אנו חותכים, כלומר $A_n \cap B_{n + 1} = B_n$.
	לכן נובע מ־\ref{probability_continuous_probability_function_theorem} כי
	\[
		\PP(\bigcup_{n = 1}^\infty B_n)
		= \lim_{n \to \infty} \PP(B_n)
		\le \liminf_{n \to \infty} \PP(A_n)
	\]
	כאשר המעבר האחרון נובע מהתכנסות הגבול והגבול התחתון.
\end{proof}

\section{תרגול 12 --- 23.1.2025}
\subsection{צפיפות משותפת}
\begin{exercise}
	נניח ש־$X, Y \sim Exp(1)$ בלתי־תלויים (כלומר מתקיים $f_X \cdot f_Y = f_{X, Y}$ לכל פונקציות צפיפות שנבחר). \\
	חשבו את ההתפלגות והצפיפות של המשתנה המקרי $X - Y$.
\end{exercise}
\begin{solution}
	נוכל לפתור את השאלה על־ידי שימוש בקונבולוציה, אך ננסה לפתור ישירות.
	אנו רוצים לחשב את $F_{X - Y}(t)$ עבור $t \in \RR$, לפי ההגדרה נובע
	\begin{align*}
		F_{X - Y}(t)
		& = \iint_{\{X - Y < t\}} f_{X, Y}(x, y)\ dx\ dy \\
		& = \iint_{\{X - Y < t\}} f_X(x) f_Y(y)\ dx\ dy \\
		& = \iint_{\{X - Y < t\}} e^{-x} e^{-y}\ dx\ dy \\
		& = \int_{-\infty}^{\infty} e^{-x} \int_{-x - t}^{\infty} e^{-y}\ dy\ dx \\
		& = \int_{-\infty}^{\infty} e^{-y} \int_{-\infty}^{y + t} e^{-x}\ dx\ dy
	\end{align*}
	נבחין כי ראינו פה שתי דרכים שונות להשתמש במשפט פוביני על התחום הנתון.
	נוכל להשתמש בכל אחד מהביטויים לפי נוחות, במקרה הזה לפי חיוביות ושליליות של הביטויים $y + t$ ו־$-x - t$.
	כאשר $t < 0$ נחשב,
	\[
		\int_{-\infty}^{\infty} e^{-x} \int_{-x - t}^{\infty} e^{-y}\ dy\ dx
		= \int_{-\infty}^{\infty} e^{-x} (e^{-x + t})\ dx
		= \int_{-\infty}^{\infty} e^{-2x + t}\ dx
		= \left. -\frac{1}{2} e^{-2x + t} \right\rvert_{x = 0}^{x = \infty}
		= \frac{1}{2} e^t
	\]
	כאשר $t < 0$ נקבל,
	\[
		\int_{-\infty}^{\infty} e^{-y} \int_{-\infty}^{y + t} e^{-x}\ dx\ dy
		= \int_{0}^{\infty} e^{-y} \int_{0}^{y + t} e^{-x}\ dx\ dy
		= \int_{0}^{\infty} e^{-y} (1 - e^{-y - t})\ dy
		= \left. \frac{1}{2} e^{-2y - t} - e^{-y} \right\rvert_{y = 0}^{y = \infty}
		= 1 - \frac{1}{2} e^{-t}
	\]
	ולכן נובע
	\[
		F_{X - Y}(t)
		= \begin{cases}
			1 - \frac{1}{2} e^{-t} & t \ge 0 \\
			\frac{1}{2} e^t & t < 0
		\end{cases}
	\]
	ובהתאם לזה נגזור ונקבל גם את פונקציית הצפיפות,
	\[
		f_{X - Y}(t)
		= F_{X - Y}'(t)
		= \begin{cases}
			\frac{1}{2} e^{-t} & t \ge 0 \\
			\frac{1}{2} e^t & t < 0
		\end{cases}
	\]
\end{solution}
\begin{exercise}
	יהי $i \in [n]$ ונגדיר $X_i \sim Unif([0, i])$ ונגדיר $Y_i = X_i \cdot X_{i + 1}$, כאשר $Y = \sum Y_i$,
	חשבו את $\EE(Y)$ ואת $\var(Y)$.
\end{exercise}
\begin{solution}
	נשתמש בלינאריות התוחלת,
	\[
		\EE(Y_i)
		= \EE(X_i) \cdot \EE(X_{i + 1})
		= \frac{1}{2} \cdot \frac{1}{2}
		= \frac{1}{4}
	\]
	ובהתאם
	\[
		\EE(Y)
		= \frac{n - 1}{4}
	\]
	נעבור לשונות,
	\[
		\var(Y)
		= \cov(Y, Y)
		= \sum \var(Y_i) + 2 \sum_{i < j} \cov(Y_i, Y_j)
	\]
	אנו גם יודעים ש־$\EE(Y_i) = \EE(Y_i^2) - {\EE(Y_i)}^2$, וכן
	\[
		\EE(Y_i^2)
		= \iint_{\RR^2} x^2 y^2\ dx\ dy
		= \int_0^1 \int_0^1 x^2 y^2\ dx\ dy
		= \frac{1}{3} \cdot \frac{1}{3}
		= \frac{1}{9}
		\implies \var(Y_i)
		= \frac{7}{144}
	\]
	ונותר לחשב את $\cov(Y_i, Y_j)$. אם $|i - j| \ge 2$ אז $\cov(Y_i, Y_j) = 0$.
	נחשב גם את
	\[
		\cov(Y_i, Y_{i + 1})
		= \EE(Y_i Y_{i + 1}) - \EE(Y_i) \EE(Y_{i + 1})
	\]
	וכן
	\[
		\EE(Y_i Y_{i + 1})
		= \EE(X_i X_{i + 1}^2 X_{i + 2})
		= \iiint_{{[0, 1]}^3} x y^2 z\ dx\ dy\ dz
		= \left( \int_0^1 x\ dx \right) \left( \int_0^1 y^2\ dy \right) \left( \int_0^1 z\ dz \right)
		= \frac{1}{2} \cdot \frac{1}{3} \cdot \frac{1}{2}
		= \frac{1}{12}
	\]
	נסיק אם כך ש־$\cov(Y_i, Y_{i + 1}) = \frac{1}{12} - \frac{1}{16} = \frac{1}{48}$.
	קיבלנו ש־$\var(Y) = (n - 1) \frac{7}{144} + 2(n - 2) \frac{1}{48}$.
\end{solution}

\section{שיעור 24 --- 23.1.2025}
\subsection{סדרות מאורעות --- המשך}
\begin{example}
	נניח ש־$\PP(A_n) = \frac{1}{2}$ בלתי־תלויים.
	אז
	\[
		\PP(\bigcup_{n = 1}^\infty \bigcap_{k = n}^\infty A_k)
		= \lim_{n \to \infty} \PP(\bigcap_{k = n}^\infty A_k)
		= 0
		\le \liminf_{n \to \infty} \PP(A_n)
		= \frac{1}{2}
	\]
	כלומר הלמה של פאטו לא מועילה לנו במיוחד במקרה זה.
	באופן דומה
	\[
		\PP(\limsup_{n \to \infty} A_n)
		= 1 \ge \limsup_{n \to \infty} \PP(A_n)
		= \frac{1}{2}
	\]
	נניח שכל $A_n$ זהים, $\forall n, A_n = A_1$.
\end{example}
\begin{lemma}[הלמה הראשונה של בורל־קנטלי]
	אם $\{A_n\}$ סדרת מאורעות ומתקיים $\sum_{n = 1}^{\infty} \PP(A_n) < \infty$ אז $\PP(\limsup_{n \to \infty} A_n) = 0$.
\end{lemma}
נבחן לא הוכחה רשמית, אבל את הרעיון שלה, נסתכל על $X = \sum_{n = 1}^{\infty} 1_{A_n}$, א
\[
	\EE(X)
	= \sum_{n = 1}^{\infty} \PP(A_n) < \infty
\]
וכן
\[
	\PP(\limsup_{n \to \infty} A_n)
	= \PP(X = \infty)
	\le \frac{\EE(X)}{\infty}
	= 0
\]
ועתה נעבור להוכחה אמיתית.
\begin{proof}
	\[
		\PP(\limsup_{n \to \infty} A_n)
		= \PP(\bigcap_{n = 1}^\infty \bigcup_{k = n}^\infty A_k)
		= \lim_{n \to \infty} \PP(\bigcup_{k = n}^\infty A_k)
		\le \lim_{n \to \infty} \sum_{k = n}^{\infty} \PP(A_k)
		= 0
	\]
	כאשר אי־השוויון נובע מחסם האיחוד, והשוויון האחרון נובע מהנתון $\sum_{n = 1}^{\infty} \PP(A_n) < \infty$.
\end{proof}
נרצה למצוא גרסה הפוכה ללמה, אך ניסיון להפוך את התנאים יוביל לטענה שאיננה נכונה, במקום זאת נראה את הלמה הבאה,
\begin{lemma}[הלמה השנייה של בורל־קנטלי]
	אם $A_n$ סדרת מאורעות בלתי־תלויים ו־$\sum_{n = 1}^{\infty} \PP(A_n) = \infty$ אז $\PP(\limsup_{n \to \infty} A_n) = 1$.
\end{lemma}
\begin{proof}
	אנו רוצים לחסום מלמטה ולכן נשתמש במשלים, נרצה למצוא את איפוס המשלים, לכן,
	\[
		\PP(\liminf_{n \to \infty} A_n^C)
		= \PP(\bigcup_{n = 1}^\infty \bigcap_{k = n}^\infty A_k^C)
		= \lim_{n \to \infty} \PP(\bigcap_{k = n}^\infty A_k^C)
	\]
	אבל לכל $N \in \NN$,
	\[
		\PP(\bigcap_{k = n}^\infty A_k^C)
		\ge \PP(\bigcap_{k = n}^N A_k^C)
		= \prod_{k = n}^N (1 - \PP(A_k))
		\le \prod_{k = n}^N e^{-\PP(A_k)}
		= e^{-\sum_{k = n}^N \PP(A_k)}
		\xrightarrow{N \to \infty} 0
	\]
\end{proof}
\begin{example}
	אם $A_n$ בלתי־תלויים ו־$\PP(A_n) = p_n$ אז
	\[
		\PP(\limsup_{n \to \infty} A_n) = 0 \iff \sum_{n = 1}^{\infty} p_n < \infty
	\]
	וכן
	\[
		\PP(\limsup_{n \to \infty} A_n) = 1 \iff \sum_{n = 1}^{\infty} p_n = \infty
	\]
	נגדיר $B_n = A_n \cap A_{n + 1}$, אם $\sum_{n = 1}^{\infty} p_n p_{n + 1} < \infty$ אז $\PP(\limsup_{n \to \infty} B_n) = 0$.
	אם $\sum_{n = 1}^{\infty} p_n p_{n + 1} = \infty$ אז $\PP(\limsup_{n \to \infty} B_n) = 1$.
	אם ${\{ B_{2n} \}}_{n \in \NN}$ בלתי־תלויים ו־$\sum_{n \in 2\NN} p_n p_{n + 1} = \infty$ או $\sum_{n \in 2\NN + 1} p_n p_{n + 1} = \infty$, \\
	אז $\PP(\limsup_{n \to \infty} B_{2n}) = 1$ או $\PP(\limsup_{n \to \infty} B_{2n + 1}) = 1$.
\end{example}
\begin{example}
	נגדיר $X_n \sim Unif(\{-1, 1\})$ וכן $Y_n = \sum_{k = 1}^{n} X_k$.
	מצאנו בתרגיל שמתקיים $\PP(Y_{3n} = 0) \le e^{-cn}$ עבור איזשהו $c > 0$.
	נסיק אם כך,
	\[
		\sum_{n = 1}^{\infty} \PP(Y_n = 0) < \infty
	\]
	ומהלמה הראשונה נובע $\PP(\limsup_{n \to \infty} \{ Y_n = 0 \}) = 0$.
	זה נכון אם ורק אם $\PP(\exists n \forall k \ge n Y_k \ne 0) = 1$.
\end{example}

\subsection{התכנסות של סדרות משתנים מקריים}\label{almost_surely_convergence}
בהגדרה\ \ref{convergence_in_distribution} דיברנו על התכנסות של משתנים מקריים בהקשר ההתפלגות שלהם, עתה נראה התכנסות במובן חזק יותר.
\begin{definition}[התכנסות סדרת משתנים מקריים כמעט תמיד]
	נאמר ש־$X_n \xrightarrow{a.s.} X$ אם
	\[
		\PP(\{ \omega \in \Omega \mid X_n(\omega) \to X(\omega) \}) = 1
	\]
\end{definition}
נבחן דוגמה למקרה זה.
\begin{example}
	נבחר $X_n = 1_{A_n}$ עבור $A_n$ בלתי־תלויים.
	נגדיר גם $\PP(A_n) = p_n$, כך ש־$p_n \le \frac{1}{2}$ לכל $n \in \NN$.
	באילו תנאים $X_m \xrightarrow{a.s.} X$ ולאיזה $X$?
	מתי בסדרה $1_{A_n}(\omega)$ יש $0$ החל ממקום מסוים?
	זהו למעשה ${(\limsup_{n \to \infty} A_n)}^C$.
	אם $\sum_{n = 1}^{\infty} p_n < \infty$ אז $\PP(\limsup_{n \to \infty} A_n) = 0$ ולכן $X_m \xrightarrow{a.s.} 0$.
	אם $\sum p_n = \infty$ אז $\PP(\limsup_{n \to \infty} A_n) = 1$ ואז $X_n$ לא מתכנסת כמעט תמיד.
\end{example}
\begin{theorem}[החוק החזק של המספרים הגדולים]
	אם $X_n$ משתנים מקריים בלתי־תלויים שווי־התפלגות עם $\EE(X_n) = \mu$ אז
	\[
		\frac{\sum_{i = 1}^{n} X_i}{n} \xrightarrow{a.s.} \mu
	\]
\end{theorem}
\begin{proof} (תחת ההנחה $|X_n| \le M$ כמעט תמיד)
	יהי $\epsilon > 0$,
	נרצה לבדוק את ההסתברויות,
	\[
		\PP(\frac{\sum_{i = 1}^{n} X_i}{n} \ge \mu + \epsilon),
		\qquad
		\PP(\frac{\sum_{i = 1}^{n} X_i}{n} \le \mu - \epsilon)
	\]
	אבל קיבלנו ששני אלה חסומים על־ידי $e^{-cn}$.
	על־ידי שימוש בטענה זו,
	\[
		\{ \frac{\sum_{i = 1}^{n} X_i}{n} \to \mu \}
		= \{ \forall k \in \NN, \exists N, \forall n \ge N, \left\lvert \frac{\sum_{i = 1}^{n} X_i}{n} - \mu \right\rvert < \frac{1}{k} \}
	\]
	ואנו רוצים להראות שלכל $k$ שנבחר מתקיים $\PP(\exists N \forall n \ge N \left\lvert \frac{\sum_{i = 1}^{n} X_i}{n} - \mu \right\rvert < \frac{1}{k}) = 1$.
	באופן שקול
	\[
		\PP(\bigcup_{N = 1}^\infty \bigcap_{n = N}^\infty A_k^n) = 1
		\iff
		\PP(\limsup_{n \to \infty} {(A_k^n)}^C) \overset{(1)}{=} 0
	\]
	כאשר $(1)$ נובע מהלמה הראשונה.
\end{proof}
\begin{conclusion}
	אם $X_n$ בלתי־תלויים שווי־התפלגות עם תוחלת $\EE(X_n) = \mu \ne 0$ אז $Y_n = \sum_{i = 1}^{n} X_i$ הוא הילוך חולף.
	זאת שכן מהחוק החזק $\frac{Y_n}{n} \to \mu \ne 0$.
\end{conclusion}

\section{תרגול 13 --- 30.1.2025}
\subsection{המשפט הגדול המרכזי}
נדבר על התפלגות נורמלית סטנדרטית,\ \ref{standard_normal_distribution}, ונתחיל בסימון,
\begin{notation}
	אם $X \sim N(0, 1)$, אז נסמן $\Phi(t) = F_X(t)$.
\end{notation}
נבהיר שמשפט הגבול המרכזי,\ \ref{central_limit_theorem}, הוא משפט מרכזי שמסביר למה התפלגות נורמלית מופיעה בכל כך הרבה מקומות.
המשפט למעשה גורס שתקנון של התפלגויות ישאף להתנהג כמו התפלגות נורמלית.
נעבור לדוגמה,
\begin{exercise}
	מטילים $n$ קוביות הוגנות באופן בלתי־תלוי.
	העריכו את ההסתברות שסכום הקוביות נמצא בטווח $I = [\frac{7}{2}n - \sqrt{n}, \frac{7}{2} + \sqrt{n}]$.
\end{exercise}
\begin{solution}
	נגדיר $X_i \sim U([6])$ וכן $X = \sum X$, אז
	\[
		\EE(X_i) = \frac{7}{2},
		\quad
		\sigma^2 = \var(X_i) = \frac{35}{12}
	\]
	נגדיר $Y_i = \frac{X_i - \frac{7}{2}}{\sigma}$.
	$Y_i$ עומדים בתנאי משפט הגבול המרכזי ולכן אם $n \to \infty$ אז
	\[
		\frac{\sum Y_i}{\sqrt{n}} \xrightarrow{d} N(0, 1)
	\]
	אנו רוצים להעריך את $\PP(X \in I) = \PP(-\frac{\sqrt{n}}{\sigma} \le \sum Y_i \le \frac{\sqrt{n}}{\sigma})$.
	ממשפט הגבול המרכזי נוכל להסיק,
	\[
		\PP(X \in I)
		= \PP(\frac{-1}{\sigma} \le \frac{\sum Y_i}{\sqrt{n}} \le \frac{1}{\sigma})
		\approx \Phi(\frac{1}{\sigma}) - \Phi(\frac{-1}{\sigma})
		\approx 0.44
	\]
\end{solution}
נעבור לדוגמה שהופיעה במבחן של אמיר,
\begin{exercise}
	נניח ש־$X \sim Poi(10000)$,
	תנו הערכה ל־$\PP(X \le 9800)$. \\
	\textbf{רמז:} תנו הערכתכם בצורת אינטגרל.
\end{exercise}
\begin{solution}
	נגדיר $X_i \sim Poi(1)$ אז אנו יודעים ש־$\sum_{i = 1}^{10000} X_i \overset{d}{=} X$.
	אנו יודעים גם ש־$\EE(X_i) = 1$ וכן ש־$\var(X_i) = 1$ ולכן נגדיר $Y_i = X_i - 1$.
	מ־\ref{central_limit_theorem} וההנחה ש־$n = 10000$ מספיק גדול נסיק,
	\[
		\frac{\sum Y_i}{\sqrt{10000}}
		\overset{d}{\approx} N(0, 1)
	\]
	לכן
	\[
		\PP(X \le 9800)
		= \PP(\sum X_i \le 9800)
		= \PP(\frac{\sum Y_i}{100} \le -2)
		= \Phi(-2)
		= \frac{1}{\sqrt{2\pi}} - \int_{-\infty}^{-2} \exp(\frac{-t^2}{2})\ dt
		\approx 0.0228
	\]
\end{solution}

\subsection{התכנסויות}
נדבר על התכנסות\ \ref{almost_surely_convergence}, התכנסות שמאוד מזכירה את ההתכנסות של אינפי.
יש סוג שני של התכנסות,
\[
	X_n \xrightarrow{P} X \implies \forall \epsilon > 0, \lim_{n \to \infty} \PP(|X_n - X| < \epsilon) = 1
\]
לעומתה יש את ההתכנסות\ \ref{convergence_in_distribution}, היא מדברת על התכנסות נקודתית באופן נאיבי.
כל התכנסות גוררת את הבאות אחריה, אך לא הפוך. נשתכנע בזה עכשיו. \\
נסכם ונגיד שהתכנסות כמעט תמיד גוררת התכנסות בהסתברות גוררת התכנסות בהתפלגות.
\begin{example}
	נניח ש־$X_n \sim Ber(\frac{1}{2})$.
	בוודאי שסדרה זו מתכנסת בהתפלגות ל־$Ber(\frac{1}{2})$.

	נראה שהיא לא מתכנסת בהסתברות (הגרסה השנייה),
	\[
		\PP(|X_n - X_m| \ge \epsilon)
		\le \PP(|X_n - X_m| \ge \frac{\epsilon}{2}) + \PP(|X_m - X| \ge \frac{\epsilon}{2})
	\]
	לכל $\epsilon$ קטן כרצוננו.
	מהצד השני עבור $\epsilon < 1$,
	\[
		\PP(|X_n - X_m| \ge \epsilon)
		= \PP(X_n = 0, X_m = 1) + \PP(X_n = 1, X_m = 0)
		= \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
	\]
	על־ידי שימוש באי־תלות, וזו כמובן סתירה, בשורה הראשונה הראינו שההסתברות קטנה כרצוננו, ואז הצלחנו לחסום אותה על־ידי $\frac{1}{2}$.
\end{example}
\begin{example}
	\begin{align*}
		\{ \lim_{n \to \infty} X_n = X \}
		& = \{ \omega \in \Omega \mid \forall \epsilon \exists N \forall n > N, |X_n(\omega) - X(\omega)| < \epsilon \} \\
		& = \bigcap_{\epsilon > 0} \bigcup_{N} A_{N, \epsilon} = \{ \omega \in \Omega \mid \forall n > N, |X_n(\omega) - X(\omega)| < \epsilon \} \\
		& = \liminf_{n \to \infty} A_{N, \epsilon}
	\end{align*}
	ובהתאם
	\[
		\PP(\liminf_{n \to \infty} A_{N, \epsilon}) = 1 \iff X_n \xrightarrow{a.s.} X
	\]
	וכן,
	\[
		\liminf_{n \to \infty} \PP(A_{N, \epsilon}) = 1 \iff X_i \xrightarrow{P} X
	\]
	זוהי הלמה של פאטו.
\end{example}
\begin{example}
	נרצה למצוא $(X_n)$ מתכנסת בהסתברות ולא מתכנסת כמעט תמיד. \\
	נגדיר $X_n$ בלתי־תלויים עם התפלגות,
	\[
		\PP(X_n = n) = \frac{1}{n},
		\qquad
		\PP(X_n = 0) = 1 - \frac{1}{n}
	\]
	ולכן עבור $\epsilon > 0$,
	\[
		\PP(X_n > \epsilon) \to \frac{1}{n} \to 0
	\]
	ולכן $X_n \xrightarrow{D} 0$, כלומר הסדרה מתכנסת בהסתברות למשתנה המקרי הקבוע $0$.

	נגדיר $B_{n, \epsilon} = \{ X_n > \epsilon \}$ ולכן $\PP(B_{n, \epsilon}) = \frac{1}{n}$ ובהתאם $\sum_{n = 1}^{\infty} \PP(B_{n, \epsilon}) = \infty$.
	מהלמה השנייה של בורל־קנטלי נסיק,
	\[
		\PP(\limsup_{n \to \infty} B_{n, \epsilon}) = 1
	\]
	ולכן גם
	\[
		\PP(\limsup_{n \to \infty} B_{n, \epsilon}^C) = 0
	\]
	ונובע ש־$X_n$ לא מתכנס כמעט תמיד למשתנה מקרי קבוע $0$.
	לכן גם לא מתכנס כמעט תמיד.
\end{example}

\section{שיעור 25 --- 30.1.2025}
\subsection{התכנסויות של סדרות משתנים מקריים}
ניזכר שראינו שלושה סוגים של התכנסויות, התכנסות כמעט תמיד,\ \ref{almost_surely_convergence},
את הסוג השני נגדיר עתה,
\begin{definition}[התכנסות בהסתברות]
	תהי סדרת משתנים מקריים $X_n$, אז נאמר ש־$X_n \xrightarrow{P} X$, או שסדרת המשתנים המקריים מתכנסת בהסתברות ל־$X$, אם,
	\[
		\forall \epsilon > 0,
		\PP(|X_n - X| > \epsilon) \xrightarrow{n \to \infty} 0
	\]
\end{definition}
הסוג השלישי הוא התכנסות בהתפלגות,\ \ref{convergence_in_distribution}.
\begin{example}
	נניח ש־${(X_n)}_{n = 1}^\infty$ בלתי־תלויים בעלי תוחלת $\mu$ ושונות $\sigma^2$. \\
	מהחוק החזק של המספרים הגדולים נובע $\frac{\sum{i = 1}^n X_i}{n} \xrightarrow{a.s.} \mu$. \\
	החוק החלש גורר ש־$\frac{\sum_{i = 1}^{n} X_i}{n} \xrightarrow{P} \mu$. \\
	משפט הגבול המרכזי גורר ש־$\frac{\left( \sum_{i = 1}^{n} X_i \right) - n\mu}{\sqrt{n} \sigma} \xrightarrow{d} Z$,
	כאשר $Z \sim N(0, 1)$.
\end{example}
\begin{theorem}[יחס סוגי ההתכנסות]
	אם $X_n \xrightarrow{a.s.} X$ אז $X_n \xrightarrow{P} X$,
	אם $X_n \xrightarrow{P} X$ אז $X_n \xrightarrow{d} X$,
	כאשר הכיוון ההפוך איננו נכון.

	אם $X_n \xrightarrow{d} C$ עבור משתנה מקרי קבוע $C$, אז $X_n \xrightarrow{P} C$ (אבל לא מעבר).
\end{theorem}
\begin{proof}[הוכחת גרירה של כמעט תמיד להסתברות]
	יהי $\epsilon > 0$.
	נבחין כי $\lim_{n \to \infty} X_n = X$ אז קיים $k \in \NN$ כך שלכל $n > k$, $|X_n - X| < \epsilon$.
	לכן
	\[
		\{ \lim_{n \to \infty} X_n = X \}
		\subseteq \bigcup_{k = 1}^\infty \bigcap_{n = k}^\infty \{ |X_n - X| < \epsilon \}
		= \liminf_{n \to \infty} \{ |X_n - X| \le \epsilon \}
	\]
	ממונוטוניות נובע,
	\[
		1 \overset{(1)}{=}
		\PP(\{ \lim_{n \to \infty} X_n = X \})
		\le \PP(\liminf_{n \to \infty} \{ |X_n - X| \le \epsilon \})
		\overset{(2)}{=} \liminf_{n \to \infty} \PP(|X_n - X| \le \epsilon)
	\]
	כאשר
	\begin{enumerate}
		\item $X_n \xrightarrow{a.s.} X$
		\item הלמה של פאטו
	\end{enumerate}
	ולכן $\lim_{n \to \infty} \PP(|X_n - X| \ge \epsilon) = 0$.
\end{proof}
\begin{example}
	נראה שאכן לא בהכרח $X_n \xrightarrow{P} X$ מעיד ש־$X_n \xrightarrow{a.s.} X$.
	גם כאן וגם בדוגמה הטובה יותר מהתרגול $X$ הוא קבוע.
	תהי ${(Y_n)}_{n = 1}^\infty$ בלתי־תלויים כך ש־$Y_n \sim Geo(\frac{1}{2})$, ונסמן $X_n = \frac{Y_n}{\log_2 n}$, ונגדיר $X = 0$. \\
	נראה ש־$X_n \xrightarrow{P} X$,
	יהי $\epsilon > 0$,
	\[
		\PP(|X_n - X| > \epsilon)
		= \PP(Y_n > \epsilon \log_2 n)
		= {\left(\frac{1}{2}\right)}^{\lfloor \epsilon \log_2 n \rfloor}
		\xrightarrow{n \to \infty} 0
	\]
	כאשר המהלך האחרון נובע מהתפלגות גאומטרית ישירות. \\
	נעבור להראות שלא מתקיים $X_n \xrightarrow{a.s.} X$.
	נטען ש־$X_n > 1$ אינסוף פעמים בהסתרות 1,
	\[
		\PP(X_n > 1)
		= \frac{1}{2^{\lfloor \log_2 n \rfloor}}
		\ge \frac{1}{2^{\log_2 n}}
		= \frac{1}{n}
	\]
	ולכן
	\[
		\sum_{n = 1}^{\infty} \PP(X_n > 1) = \infty
	\]
	ומהלמה השנייה של בורל־קנטלי נובע $\PP(\bigcap_{n = 1}^\infty \bigcup_{k = n}^\infty X_n > 1) = 1$,
	לכן $\PP(\lim_{n \to \infty} X_n = X) = 0 \ne 1$.
\end{example}
נעבור לחלק הבא של הוכחת המשפט.
\begin{proof}[הוכחת הסתברות גורר בהתפלגות]
	תהי $t$ נקודת רציפות של $F_X$, לכל $n \in \NN$ ולכל $\epsilon > 0$ נרצה להראות שמתקיים
	\[
		F_X(t - \epsilon) - \PP(|X_n - X| > \epsilon)
		\le F_X(t)
		\le F_X(t + \epsilon) + \PP(|X_n - X| > \epsilon)
	\]
	עבור החלק הראשון של אי־השוויון,
	\begin{align*}
		F_X(t)
		& = \PP(X_n \le t) \\
		& = \PP(X_n \le t, X \le t + \epsilon) + \PP(X_n \le t, X > t + \epsilon) \\
		& \le \PP(X \le t + \epsilon) + \PP(|X_n - X| > \epsilon) \\
		& = F_X(t + \epsilon) + \PP(|X_n - X| > \epsilon)
	\end{align*}
	ממונוטוניות והסתברות שלמה.
	עבור החלק השני של אי־השוויון,
	\begin{align*}
		F_X(t - \epsilon)
		& = \PP(X \le t - \epsilon) \\
		& = \PP(X \le t - \epsilon, X_n \le t) + \PP(X \le t - \epsilon, X_n > t) \\
		& \le \PP(X_n \le t) + \PP(|X_n - X| > \epsilon) \\
		& = F_X(t) - \PP(|X_n - X| > \epsilon)
	\end{align*}
	ולכן נובע ש־$F_X(t) \ge F_X(t - \epsilon) - \PP(|X_n - X| > \epsilon)$.
	מצאנו אי־שוויונות החוסמים את $F_X(t)$ ולכן כאשר $n \to \infty$ נובע
	\[
		F_X(t - \epsilon)
		\le \liminf_{n \to \infty} F_{X_n}(t)
		\le \limsup_{n \to \infty} F_{X_n}(t)
		\le F_X(t + \epsilon)
	\]
	וכאשר $\epsilon \to 0$ ובשימוש ברציפות של $t$ ב־$F_X$ נובע ש־$\lim_{n \to \infty} F_{X_n}(t) = F_X(t)$.
\end{proof}
נראה עתה דוגמה למקרה שיש התכנסות בהתפלגות אבל לא בהסתברות.
הסיבה הטכנית היא שייתכן ש־$X_n$ ו־$X$ לא מוגדרים על אותו מרחב הסתברות.
\begin{example}
	נניח ש־$X_n, X$ כולם בלתי־תלויים ומתפלגים $Unif([0, 1])$, אז $X_n \xrightarrow{d} X$ שכן $F_{X_n}(t) = F_X(t)$ לכל $t$,
	אבל $\PP(|X_n - X| > \frac{1}{4}) \ge \PP(X_n \le \frac{1}{2}, X \ge \frac{3}{4}) = \frac{1}{8} \not\to 0$.
\end{example}
\begin{proposition}
	אם $X = C$ משתנה מקרי קבוע ו־$X_n \xrightarrow{d} C$ אז $X_n \xrightarrow{P} C$.
\end{proposition}
\begin{proof}
	יהי $\epsilon > 0$,
	\[
		\PP(|X_n - X| \le \epsilon)
		= \PP(X_n \le C + \epsilon) - \PP(X_n < C - \epsilon)
		\ge \PP(X_n \le C + \epsilon) - \PP(X_n \le C - \epsilon)
		= F_{X_n}(C + \epsilon) - F_{X_n}(C - \epsilon)
	\]
	אבל $F_X(t) = F_C(t) = \begin{cases}
		0 & t < C \\
		1 & t \ge C
	\end{cases}$
	לכן שגם $C + \epsilon$ וגם $C - \epsilon$ נקודות רציפות של $F_X$,
	ולכן
	\[
		\PP(|X_n - X| \le \epsilon)
		\ge F_{X_n}(C + \epsilon) - F_{X_n}(C - \epsilon)
		\xrightarrow{n \to \infty} F_X(C + \epsilon) - F_X(C - \epsilon)
		= 1 - 0
	\]
	כפי שרצינו.
\end{proof}

\subsection{תכונות של התכנסות}
\begin{proposition}
	הטענות הבאות נכונות עבור התכנסות כמעט תמיד,
	\begin{enumerate}
		\item אם $X_n \xrightarrow{a.s.} X$ וכן $Y_n \xrightarrow{a.s.} Y$ אז $X_n + Y_n \xrightarrow{a.s.} X + Y$
		\item אם $X_n \xrightarrow{a.s.} X$ ו־$f \in C(\RR)$ אז $f(X_n) \xrightarrow{a.s.} f(X)$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item מתקיים
			\[
				\{ \lim_{n \to \infty} X_n + Y_n = X + Y \}
				\supseteq \{ \lim_{n \to \infty} X_n = X \} \cap \{ \lim_{n \to \infty} Y_n = Y \}
			\]
			צריך להוכיח שמתקיים $\PP(\lim_{n \to \infty} X_n = X, \lim_{n \to \infty} Y_n = Y) = 1$,
			נראה שמתקיים
			\begin{align*}
				\PP({\{ \lim_{n \to \infty} X_n = X \} \cap \{ \lim_{n \to \infty} Y_n = Y \}}^C)
				& = \PP({\{ \lim_{n \to \infty} X_n = X \}}^C \cup {\{ \lim_{n \to \infty} Y_n = Y \}}^C) \\
				& \le \PP({\{ \lim_{n \to \infty} X_n = X \}}^C) + \PP({\{ \lim_{n \to \infty} Y_n = Y \}}^C) \\
				& = 0 + 0
			\end{align*}
		\item $\{ \lim_{n \to \infty} f(X_n) = f(X) \} \supseteq \{ \lim_{n \to \infty} X_n = X \}$ ולכן
			\[
				\PP(\lim_{n \to \infty} f(X_n) = f(X))
				\ge \PP(\lim_{n \to \infty} X_n = X)
				= 1
			\]
			וסיימנו.
	\end{enumerate}
\end{proof}
\begin{example}
	נניח ש־${(X_n)}_{n = 1}^\infty$ בלתי־תלויים ומתפלגים $Unif([0, 1])$, נראה שמתקיים
	\[
		{(X_1 \cdots X_n)}^{\frac{1}{n}}
		\xrightarrow{a.s.} \frac{1}{e}
	\]
	נבחין כי ${(X_1 \cdots X_n)}^{\frac{1}{n}} = {(e^{\log X_1} \cdots e^{\log X_n})}^{\frac{1}{n}} = \exp(\frac{1}{n} \sum_{i = 1}^{n} \log X_i)$,
	אם מתקיים $\frac{1}{n} \sum_{i = 1}^{n} \log X_i \xrightarrow{a.s.} -1$ אז מהטענה שהוכחנו עתה הטענה נובעת.
	נוכיח ש־$-\log X_i \sim Exp(1)$,
	\[
		\PP(-\log X_i > t)
		= \PP(X_i < e^{-t})
		= e^{-t}
	\]
	זאת שכן $X_i \sim U([0, 1])$ ולכן,
	\[
		\EE(\log X_i) = -1
	\]
	ומהחוק החזק נובע
	\[
		\frac{1}{n} \sum_{i = 1}^{n} \log X_i
		\xrightarrow{a.s.} -1
	\]
\end{example}

\section{סיכום תוצאות}

\subsection{התפלגויות בדידות}
\begin{otherlanguage}{english}
	\begin{center}
		\begin{tabular}{c | c c c c c c} % chktex 44
			$X \sim$ & Parameters & $\supp X$ & $\PP(X = s)$ & $\EE(X)$ & $\var(X)$ & $M_X(t)$ \\
			\hline % chktex 44
			$U([n])$ & $n \in \NN$ & $[n]$ & $\frac{1}{n}$ & $\frac{n + 1}{2}$ & $\frac{n^2 - 1}{12}$ & $\frac{e^{nt} - e^{2t}}{n(1 - e^t)}$ \\
			\hline % chktex 44
			$Ber(p)$ & $0 \le p \le 1$ & $\{0, 1\}$ & $\tiny\begin{cases} p & s = 1 \\ 1 - p & s = 0 \end{cases}$ & $p$ & $p(1 - p)$ & $p e^t + (1 - p)$ \\
			\hline % chktex 44
			$Bin(n, p)$  & $n \in \NN, 0 \le p \le 1$ & $\NN \cup \{0\}$ & $\binom{n}{s} {(1 - p)}^{n - s} p^s$ & $np$ & $np(1 - p)$ & ${(pe^t + (1 - p))}^n$ \\
			\hline % chktex 44
			$Geo(p)$ & $0 \le p \le 1$ & $\NN$ & ${(1 - p)}^{s - 1} p$ & $\frac{1}{p}$ & $\frac{1 - p}{p^2}$ & $\frac{p e^t}{1 - (1 - p) e^t}$ \\
			\hline % chktex 44
			$Poi(\lambda)$ & $0 < \lambda$ & $\NN \cup \{0\}$ & $e^{-\lambda} \frac{\lambda^s}{s!}$ & $\lambda$ & $\lambda$ & $\exp(\lambda(e^t - 1))$ \\
		\end{tabular}
	\end{center}
\end{otherlanguage}

\subsection{התפלגויות רציפות}
\begin{otherlanguage}{english}
	\begin{center}
		\begin{tabular}{c | c c c c c c c} % chktex 44
			$X \sim$ & Parameters & $\supp X$ & $f_X(s)$ & $F_X(s)$ & $\EE(X)$ & $\var(X)$ & $M_X(t)$ \\
			\hline % chktex 44
			$Unif([a, b])$ & $a \le b$ & $s \in [a, b]$ & $\tiny\begin{cases} \frac{1}{b - a} & s \in [a, b] \\ 0 & \text{else} \end{cases}$ & $\tiny\begin{cases} 0
						   & s < a \\ \frac{s - a}{b - a} & a \le s < b \\ 1 & s > b \end{cases}$
						   & $\frac{a + b}{2}$ & $\frac{{(b - a)}^2}{12}$ & $\tiny\begin{cases} \frac{e^{tb} - e^{ta}}{t(b - a)} & t \ne 0 \\ 1 & t = 0 \end{cases}$ \\
			\hline % chktex 44
			$Exp(\lambda)$ & $\lambda > 0$ & $0 \le s$ & $-\lambda e^{\lambda s}$ & $1 - e^{-\lambda}$ & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ & $\frac{\lambda}{\lambda - t}$ \\
			\hline % chktex 44
			$N(0, 1)$ & --- & $\RR$ & $\frac{1}{\sqrt{2 \pi}} \exp(-\frac{s^2}{2})$ & $\Phi(s)$ & $0$ & $1$ & --- \\
			\hline % chktex 44
			$N(\mu, \sigma^2)$ & $\sigma^2 \ge 0$ & $\RR$ & $\frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{{(s - \mu)}^2}{2 \sigma^2})$ & $\Phi(\frac{s - \mu}{\sigma})$ & $\mu$ & $\sigma^2$ & ---
		\end{tabular}
	\end{center}
\end{otherlanguage}

\listoftheorems[title=הגדרות ומשפטים,ignoreall,show={theorem,definition},swapnumber,onlynamed={proposition}]
\end{document}

\input{../article_base.tex}
\title{תורת ההסתברות 1 --- סיכום}
\setcounter{secnumdepth}{2}
% chktex-file 9
% chktex-file 17

\hypersetup{}
\begin{document}
\maketitle
\maketitleprint{}

\tableofcontents

\section{שיעור 1 --- 29.10.2024}

\subsection{מבוא הקורס}
% אורי גורביץ' הוא המרצה, הקורס הוא מבוא להסתברות. הוא חצי חירש תתמודד עם זה במקרה הצורך.
נלמד לפי ספר שעוד לא יצא לאור שנכתב על־ידי אורי עצמו, הוא עוד לא סופי ויש בו בעיות ואי־דיוקים, תשיג את הספר הזה.
כן יש הבדל בין הקורס והספר אז לא לסמוך על הסדר שלו גם כשאתה משיג אותו, אבל זו תוספת מאוד נוחה.
יש סימון של כוכביות לחומר מוסף, כדאי לעבור עליו לקראת המבחן כי זה יתן לנו עוד אינטואיציה והעמקה של ההבנה.

נשים לב כי ענף ההסתברות הוא ענף חדש יחסית, שהתפתח הרבה אחרי שאר הענפים הקלאסיים של המתמטיקה, למעשה רק לפני 400 שנה נשאלה על־ידי נזיר במהלך חקר של משחק אקראי השאלה הראשית של העולם הזה, מה ההסתברות של הצלחה במשחק.

נעבור לדבר על פילוסופיה של ההסתברות.
מה המשמעות של הטלת מטבע מבחינת הסתברות?
ישנה הגישה של השכיחות, שמציגה הסתברות כתוצאה במקרה של חזרה על ניסוי כמות גדולה מאוד של פעמים.
יש כמה בעיות בזה, לרבות חוסר היכולת להגדיר במדויק אמירה כזו, הטיות שנובעות מפיזיקה, מטבעות הם לא מאוזנים לדוגמה.
הבעיה הראשית היא שלא לכל בעיה אפשר לפנות בצורה כזאת.
ישנה גישה נוספת, היא הגישה האוביקטיבית או המתמטית, הגישה הזו בעצם היא תרגום בעיה מהמציאות לבעיה מתמטית פורמלית.
לדוגמה נשאל את השאלה מה ההסתברות לקבל 6 בהגרלה של כל המספרים מ־1 עד מיליון.
השיטה ההסתברותית קובעת שאם אני רוצה להוכיח קיום של איזשהו אוביקט, לפעמים אפשר לעשות את זה על־ידי הגרלה של אוביקט כזה והוכחה שיש הסתברות חיובית שהוא יוגרל, וזו הוכחה שהוא קיים.
מה התחזיות שינבעו מתורת ההסתברות? לדוגמה אי־אפשר לחזות הטלת מטבע בודדת, אבל היא כן נותנת הבנה כללית של הטלת 1000 מטבעות, הסתברויות קטנות מספיק יכולות להיות זניחות ובמקרה זה נוכל להתעלם מהן.
לפחות בתחילת הקורס נדבר על תרגום של בעיות מהמציאות לבעיות מתמטיות, זה אומנם חלק פחות ריגורזי, אבל הוא כן חשוב ליצירת קישור בין המציאות לבין החומר הנלמד.

דבר אחרון, ישנה השאלה הפילוסופית של האם באמת יש הסתברות שכן לא בטוח שיש אקראיות בטבע, הגישה לנושא מבחינה פיזיקלית קצת השתנתה בעת האחרונה וקשה לענות על השאלה הזאת.
יש לנו תורות פיזיקליות שהן הסתברותיות בעיקרן, כמו תורת הקוונטים, תורה זו לא סתם הסתברותית, אנחנו לא מנסים לפתור בעיות הסתברותיות אלא ממש משתמשים במודלים סטטיסטיים כדי לתאר מצב בעולם.
לדוגמה נוכל להסיק ככה מסקנה פשוטה שאם מיכל גז נפתח בחדר, יהיה ערבוב של הגז הפנימי ושל אוויר החדר, זוהי מסקנה הסתברותית.
החלק המדהים הוא שתורת הקוונטים מניחה חוסר דטרמניזם כתכונה יסודית ועד כמה שאפשר לראות יש ניסויים שמוכיחים שבאמת יש חוסר ודאות בטבע.
דהינו שברמה העקרונית הפשוטה באמת אין תוצאה ודאית בכלל למצבים כאלה במציאות.

\subsection{מרחבי מדגם ופונקציית הסתברות}
\begin{definition}[מרחב מדגם]
	מרחב מדגם הוא קבוצה לא ריקה שמהווה העולם להסתברות. \\*
	נסמנה $\Omega$.
	איבר במרחב המדגם נסמן ב־$\omega \in \Omega$ על־פי רוב.
\end{definition}
נוכל להגיד שמרחב במדגם הוא הקבוצה של האיברים שעליה אנחנו שואלים בכלל שאלות, זהו הייצוג של האיברים או המצבים שמעניינים אותנו.
בהתאם נראה עכשיו מספר דוגמות שמקשרות בין אובייקטים שאנו דנים בהם בהסתברות ובהגדרה פורמלית של מרחבי מדגם עבורם.
\begin{example}[מרחבי הסתברות שונים]
	נראה מספר דוגמות למצבים כאלה:
	\begin{itemize}
		\item הטלת מטבע תוגדר על־ידי $\Omega = \{ H, T \}$.
		\item הטלת שלושה מטבעות תהיה באופן דומה $\Omega = {\{H, T\}}^3$.
		\item הטלת קוביה היא $\Omega = [6] = \{ 1, \dots, 6 \}$.
		\item הטלת מטבע ואז אם יוצא עץ (H) אז מטילים קוביה ואם יוצא פלי (T) אז מטילים קוביה עם 8 פאות. \\*
			במקרה זה נסמן $\Omega = \{ H1, H2, H3, \dots, H6, T1, \dots, T8 \} = \{H, T \} \times \{1, \dots, 8 \}$ כאשר הכוונה פה היא לזוג סדור $\langle H, 1 \rangle$.
		\item ערבוב חפיסת קלפים, במקרה זה מרחב המדגם שלנו יהיה סימון של הקלפים כרשימה מספרית בלבד, דהינו $\Omega = S_{52}$. \\*
			נוכל גם לסמן במקום את $\Omega = {\{1, \dots, 52 \}}^{52}$, זהו סימון זהה.
	\end{itemize}
\end{example}
בדוגמה זו קל במיוחד לראות שכל איבר בקבוצה מתאר מצב סופי כלשהו, ואנו יכולים לשאול שאלות הסתברותיות מהצורה מה הסיכוי שנקבל $\omega$ מסוים מתוך $\Omega$, זאת ללא התחשבות בבעיה שממנה אנו מגיעים.
נבחן עתה גם דוגמות למקרים שבהם אין לנו מספר סופי של אפשרויות, למעשה מקרים אלה דומים מאוד למקרים שראינו עד כה.
\begin{example}[מרחבי מדגם לא סופיים]
	מטילים מטבע עד שיוצא $H$, אז מרחב המדגם הוא $\Omega = \NN \cup \{ \infty \}$. \\*
	באופן דומה נוכל לבחון מדידת זמן התפרקות חלקיק, היא $\Omega = \RR_+ \cup \{ \infty \}$.
\end{example}
\begin{definition}[פונקציית הסתברות נקודתית]
	יהי מרחב מדגם $\Omega$ ותהי $p : \Omega \to [0, \infty)$ פונקציה כך שמתקיים
	\[
		\sum_{\omega \in \Omega} p(\omega) = 1
	\]
	אז פונקציה זו נקראת \textbf{פונקציית הסתברות}.
\end{definition}
למעשה פונקציית הסתברות היא מה שאנחנו נזהה עם הסתברות במובן הפשוט, פונקציה זו מגדירה לנו לכל סיטואציה ממרחב המדגם מה הסיכוי שנגיע אליה, כך לדוגמה אם נאמר שהטלת מטבע תגיע בחצי מהמקרים לעץ ובחצי השני לפלי,
אז זו היא פונקציית ההסתברות עצמה, פונקציה שמחזירה חצי עבור עץ וחצי עבור פלי, נראה מספר דוגמות.
\begin{example}[פונקציית הסתברות להטלת מטבע]
	נגדיר $\Omega = \{ H, T \}$ ויהי $0 \le \alpha \le 1$, נגדיר $p(H) = \alpha, p(T) = 1 - \alpha$.
\end{example}
\begin{example}[פונקציית הסתברות אינסופית]
	נגדיר $\Omega = \NN \cup \{ \infty \}$ ו־$p(\omega) = \begin{cases}
		2^{-\omega} & \omega \in \NN \\
		0 & \omega = \infty
	\end{cases}$.
	בדוגמה זו נקבל $\sum_{n = 1}^{\infty} 2^{-n} = 1$ ולכן זו אכן פונקציית הסתברות.
\end{example}
נבחין כי הדוגמה האחרונה מתארת לנו התפלגות של דעיכה, זאת אומרת שלדוגמה אם קיים חלקיק עם זמן מחצית חיים של יחידה אחת, פונקציית הסתברות זו תניב לנו את הסיכוי שהוא התפרק לאחר כמות יחידות זמן כלשהי.
\begin{example}
	נגדיר $\Omega = \NN$ ו־$p(\omega) = \frac{1}{\omega(\omega + 1)}$, נבחין כי אכן $\sum_{n = 1}^{\infty} \frac{1}{n(n + 1)} = 1$.
\end{example}
\begin{definition}[תומך]
	התומך של $p$ הוא $\supp(p) = \{ \omega \in \Omega \mid p(\omega) > 0 \}$. \\*
	נבחין כי התומך הוא למעשה קבוצת האיברים שאפשרי לקבל לפי פונקציית ההסתברות, כל שאר המצבים מקבלים 0, משמעו הוא שאין אפשרות להגיע אליו.
\end{definition}
\begin{remark}
	נבחין כי תמיד $\mathcal{F} \subseteq \mathcal{P}(\Omega)$.
\end{remark}
\begin{definition}[מאורע]
	מאורע הוא תת־קבוצה של מרחב המדגם, קבוצת כל המאורעות תסומן $\mathcal{F}$.
	עבור מאורע $A$ המאורע המשלים מסומן ב־$A^C = \Omega \setminus A$.
\end{definition}
\begin{definition}[פונקציית הסתברות]
	נגדיר עתה פונקציית הסתברות שאיננה נקודתית.
	יהי מרחב מדגם $\Omega$ וקבוצת מאורעות $\mathcal{F}$. \\*
	תהי $\PP : \mathcal{F} \to [0, \infty)$ המקיימת את התכונות הבאות:
	\begin{enumerate}
		\item $\PP(\Omega) = 1$
		\item לכל ${\{A_i\}}_{i = 1}^\infty \subseteq \mathcal{F}$ סדרת מאורעות שונים מתקיים
			\[
				\sum_{i \in \NN} \PP(A_i) = \PP(\bigcup_{i \in \NN} A_i)
			\]
			דהינו, הפונקציה סכימה בתת־קבוצות בנות מניה.
	\end{enumerate}
	לפונקציה כזו נקרא \textbf{פונקציית ההסתברות} על $(\Omega, \mathcal{F})$.
\end{definition}
\begin{proposition}
	תהי $p$ פונקציית הסתברות נקודתית על $\Omega$ אז נגדיר פונקציית הסתברות $\PP_p$ על־ידי
	\[
		\PP_p(A) = \sum_{\omega \in A} p(\omega)
	\]
	אז $\PP_p$ היא פונקציית הסתברות.
\end{proposition}
\begin{proof}
	נוכיח ששתי התכונות של פונקציית הסתברות מתקיימות.
	\[
		\PP_p(A) = \sum_{\omega \in A} p(\omega) \ge 0
	\]
	שכן זהו סכום אי־שלילי מהגדרת $p$,
	בנוסף נקבל מההגדרה של $p$ כי
	\[
		\PP_p(\Omega) = \sum_{\omega \in \Omega} p(\omega) = 1
	\]
	וקיבלנו כי התכונה הראשונה מתקיימת. \\*
	תהי ${\{A\}}_{i = 1}^\infty \in \mathcal{F}$, אז נקבל
	\[
		\sum_{i \in \NN} \PP_p(A_i) = \sum_{i \in \NN} \left( \sum_{\omega \in A_i} p(\omega) \right) = \sum_{\omega \in \bigcup_{i \in \NN} A_i} p(\omega) = \PP_p(\bigcup_{i \in \NN} A_i)
	\]
	ולכן גם התכונה השנייה מתקיימת וקיבלנו כי $\PP_p$ היא אכן פונקציית הסתברות.
\end{proof}
נשים לב כי בעוד פונקציית הסתברות נקודתית מאפשרת לנו לדון בהסתברות של איבר בודד בקבוצות בנות מניה, פונקציית הסתברות למעשה מאפשרת לנו לדון בהסתברות של מאורעות, הם קבוצות של כמה מצבים אפשריים, ובכך להגדיל את מושא הדיון שלנו.
מהטענה האחרונה גם נוכל להסיק שבין שתי ההגדרות קיים קשר הדוק, שכן פונקציית הסתברות נקודתית גוררת את קיומה של פונקציית הסתברות כללית.

\section{תרגול 1 --- 31.10.2024}
המתרגל הוא אמיר, amir.behar@mail.huji.ac.il

\subsection{מרחבי הסתברות סופיים ובני־מניה}
ניזכר בהגדרה למרחב הסתברות, המטרה של הגדרה זו היא לתאר תוצאות אפשריות של מצב נתון.
\begin{definition}[מרחב הסתברות]
	מרחב הסתברות הוא קבוצה $(\Omega, \mathcal{F}, \PP)$ כאשר $\PP : \mathcal{F} \to [0, 1]$, כך שמתקיים
	\begin{enumerate}
		\item חיוביות: $\forall A \in \mathcal{F}, \PP(A) \ge 0$
		\item נרמול: $\PP(\Omega) = 1$
		\item סיגמא־אדיטיביות: $\forall {\{ A_i \}}_{i = 1}^\infty \in \mathcal{F}, (\forall i, j \in \NN, i \ne j \implies A_i \cap A_j = \emptyset) \implies \sum_{i \in I} \PP(A_i) = \PP(\bigcup_{i \in I} A_i )$
	\end{enumerate}
\end{definition}
\begin{exercise}
	יהי $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, $A, B \in \mathcal{F}$, הוכיחו
	\[
		\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)
	\]
\end{exercise}
\begin{proof}
	נבחין כי $\PP(A) = \PP(A - (A \cap B)) + \PP(A \cap B)$ וגם $\PP(B) = \PP(B - (A \cap B)) + \PP(A \cap B)$. נוכל אם כן לסכום ולקבל
	\[
		\PP(A) + \PP(B) = \PP(A - (A \cap B)) + \PP(A \cap B) + \PP(B - (A \cap B)) + \PP(A \cap B)
		= \PP(A \cup B) + \PP(A \cap B)
	\]
	נבחין כי השוויון האחרון נובע מהזרות של קבוצות אלה.
\end{proof}
לאורך פרק זה נגדיר מעתה שמתקיים $\Omega$ סופית, $\mathcal{F} = 2^\Omega$ ואף נגדיר כי ההסתברות אחידה, דהינו $\forall A \enspace \PP(A) = \frac{|A|}{|\Omega|}$, זה כמובן שקול לטענה
\[
	\forall \omega, \omega' \in \Omega, \PP(\{\omega\}) = \PP(\{\omega'\})
\]
\begin{exercise}
	מטילים קוביה הוגנת, מה ההסתברות שיצא מספר זוגי?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = [6] = \{1, \dots, 6\}$, עם $\PP$ אחידה. \\*
	נרצה לחשב את $A = \{2, 4, 6\}$ ולכן נקבל $\PP(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = \frac{1}{2}$.
\end{solution}
\begin{exercise}
	מטילים מטבע הוגן שלוש פעמים, מה ההסתברות שיצא עץ בדיוק פעמיים, ומה ההסתברות שיצא עץ לפחות פעמיים?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = \{ TTT, TTP, TPT, PTT, \dots \}$. \\*
	עבור המקרה הראשון נגדיר $A = \{ TTP, TPT, PTT \}$, ולכן נקבל שההסתברות היא $\PP(A) = \frac{3}{8}$. \\*
	במקרה השני נקבל $B = A \cup \{ TTT \}$ ולכן $P(B) = \frac{1}{2}$.
\end{solution}
\begin{exercise}
	מטילים קוביה הוגנת $n$ פעמים.
	\begin{enumerate}
		\item מה ההסתברות שתוצאת ההטלה הראשונה קטנה מ־4?
		\item מה ההסתברות שתוצאת ההטלה הראשונה קטנה שווה מתוצאת ההטלה השנייה?
		\item מה ההסתברות שיצא 1 לפחות פעם אחת?
	\end{enumerate}
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {[6]}^n = \{ (x_1, \dots, x_n) \mid x_i \in [6] \}$.
	\begin{enumerate}
		\item נגדיר $A = \{ (x_1, \dots, x_n) \in \Omega \mid x_1 < 4 \}$ ולכן $\PP(A) = \frac{3 \cdot 6^{n - 1}}{6^n} = \frac{1}{2}$.
		\item נגדיר $B = \{ (x_1, \dots, x_n) \in \Omega \mid x_1 \le x_2 \} = \bigcup_{i = 1}^6 \{ (x_1, i, x_3, \dots, x_n) \in \Omega \mid x_i \le i \}$, ולכן נקבל
			\[
				\PP(B) = \sum \PP(B_i) = \sum \frac{i \cdot 6^{n - 2}}{6^n} = \frac{\sum_{i = 1}^{6} i}{6^2} = \frac{6 \cdot 7}{6^2 \cdot 2} = \frac{7}{12}
			\]
		\item הפעם $C = \{ (x_1, \dots, x_n) \in \Omega \mid \exists i, x_i = 1 \}$, בהתאם $C^C = \{ (x_1, \dots, x_n) \in \Omega \mid \forall i, x_1 \ne 1 \}$. \\*
			לכן נקבל $\PP(C^C) = \frac{5^n}{6^n} \implies \PP(C) = 1 - \frac{5^n}{6^n}$.
	\end{enumerate}
\end{solution}
\begin{exercise}
	חמישה אנשים בריאים וחמישה אנשים חולי שפעת עומדים בשורה. מה ההסתברות שחולי השפעת נמצאים משמאל לאנשים הבריאים?
\end{exercise}
\begin{solution}
	נגדיר $\Omega$ ככל הסידורים של $0, 1$ כשיש חמישה מכל סוג.
	לכן נקבל $|\Omega| = \binom{10}{5}$, שכן $\Omega = \{ X \subset [10] \mid |X| = 5 \}$. \\*
	המאורע הפעם הוא $A = \{ \{ 1, 2, 3, 4, 5 \} \}$ ובהתאם $\PP(A) = \frac{5! 5!}{10!}$.

	נוכל גם להגדיר $\Omega = S_{10}$ כאשר חמשת המספרים הראשונים מייצגים בריאים וחמשת האחרונים מייצגים חולים. \\*
	במקרה זה נקבל $A = \{ \pi \in \Omega \mid \pi(\{1, 2, 3, 4, 5\}) \subseteq \{1, 2, 3, 4, 5\} \}$ ולכן $|A| = 5! 5! $ וכך נקבל $\PP(A) = \frac{5! 5!}{10!}$.
\end{solution}

\section{שיעור 2 --- 31.10.2024}
\subsection{השלמה לטורים דו־מימדיים}
נגדיר הגדרה שדרושה לצורך ההרצאה הקודמת כדי להיות מסוגלים לדון בסכומים אינסופיים בני־מניה.
\begin{definition}[סכום קבוצת בת־מניה]
	אם ${\{a_i\}}_{i \in I}$ ו־$a_i \ge 0$ לכל $i \in I$ אז נגדיר
	\[
		\sum_{i \in I} a_i = \sup \left\{ \sum_{i \in J} \mid J \subseteq I, J \text{ is finite} \right\}
	\]
\end{definition}

\subsection{תכונות של פונקציות הסתברות}
נעבור עתה לבחון פונקציות הסתברות ואת תכונותיהן, נתחיל מתרגיל שיוצק תוכן לתומך של פונקציית הסתברות:
\begin{exercise}
	הוכיחו כי אם $\sum_{i \in I} a_i < \infty$ ו־$a_i \ge 0$ לכל $i \in I$ אז $|\{ i \in I \mid a_i < 0 \}| \le \aleph_0$.
	במילים אחרות הוכיחו כי התומך של $a$ הוא בן־מניה.
\end{exercise}
בשיעור הקודם ראינו את ההגדרה והטענה הבאות:
\begin{definition}[פונקציית הסתברות מתאימה לנקודתית]
	בהינתן פונקציית הסתברות נקודתית $p$ נגדיר
	\[
		\PP_p(A) = \sum_{\omega \in A} p(\omega)
	\]
\end{definition}
\begin{proposition}
	$\PP_p$ היא פונקציית הסתברות.
\end{proposition}
טענה זו בעצם יוצרת קשר בין פונקציות הסתברות לפונקציות הסתברות נקודתיות, ומאפשרת לנו לחקור את פונקציות ההסתברות לעומק באופן פשוט הרבה יותר. נשתמש עתה בכלי זה.
\begin{definition}[מרחב הסתברות בדיד]
	אם $\PP$ פונקציית הסתברות כך שקיימת פונקציית הסתברות נקודתית $p$ כך ש$\PP = \PP_p$, אז נאמר ש־$\PP$ היא בדידה ו־$(\Omega, \mathcal{F}, \PP)$ \textbf{מרחב הסתברות בדיד}.
\end{definition}
\begin{proposition}
	יש פונקציות הסתברות שאינן בדידות.
	בפרט, עבור מדגם ההסתברות $\Omega = [0, 1]$ קיימת פונקציית הסתברות $\PP$ המקיימת
	\[
		\forall a, b \in \RR, 0 \le a \le b \le 1 \implies \PP([a, b]) = b - a
	\]
\end{proposition}
\begin{example}
	ידוע כי $\sum_{n \in \NN} \frac{1}{n^2} = \frac{\pi^2}{6} < \infty$ ולכן נוכל להגדיר $\Omega = \NN$ ו־$p(n) = \frac{1}{\frac{\pi^2}{6} n^2}$, הגדרה זו תניב ש־$\sum_{n \in \NN} p(n) = 1$ ולכן זו פונקציית הסתברות.
	נחשב את $\PP_p(A)$ עבור $A = 2\NN$:
	\[
		\PP_p(A) = \sum_{n \in A} p(n) = \sum_{k \in \NN} p(2k) = \frac{1}{\frac{\pi^2}{6} {(2k)}^2} = \frac{6}{\pi^2} \frac{1}{4} \sum_{k \in \NN} \frac{1}{k^2} = \frac{1}{4}
	\]
	נסביר, הגדרנו פונקציית הסתברות של דעיכה, דהינו שככל שהמספר שאנו מבקשים גדול יותר כך הוא פחות סביר באופן מעריכי (לדוגמה זמן מחצית חיים), ואז שאלנו כמה סביר המאורע שבו נקבל מספר זוגי.
\end{example}
\begin{theorem}[תכונות פונקציית הסתברות]
	$\PP$ פונקציית הסתברות על $(\Omega, \mathcal{F})$, אז
	\begin{enumerate}
		\item $\PP(\emptyset) = 0$
		\item אם $I$ קבוצה סופית ו־${\{A_i\}}_{i \in I}$ מאורעות זרים בזוגות, אז $\PP(\bigcup_{i \in I} A_i) = \sum_{i \in I} \PP(A_i)$
		\item אם $A \subseteq B$ מאורעות אז $\PP(A) \le \PP(B)$
		\item $\PP(A) \le 1$ לכל מאורע $A$
		\item לכל מאורע $A$ מתקיים $\PP(A^C) = 1 - \PP(A)$
	\end{enumerate}
\end{theorem}
\begin{proof}
	נוכיח את התכונות
	\begin{enumerate}
		\item נראה כי $\PP(\emptyset) = \sum_{i = 1}^\infty \PP(\emptyset)$ שכן כל איחוד של קבוצות ריקות הוא זר, לכן אילו $\PP(\emptyset) \ne 0$ נקבל ישר סתירה, נסיק כי $\PP(\emptyset) = 0$ בלבד.
		\item נגדיר $A_i = \emptyset$ לכל $i > n$ ונשתמש בסיגמא־אדיטיביות ונקבל
			\[
				\PP(\bigcup_{i \in I} A_i)
				= \PP(\bigcup_{i \in \NN} A_i)
				= \sum_{i \in \NN} \PP(A_i)
				= \sum_{i \in I} \PP(A_i)
			\]
		\item נשתמש בתכונה 2 על $B, B \setminus A$, אלו הן קבוצות זרות כמובן, אם נגדיר $D = A \cup (B \setminus A)$ נקבל $\PP(D) = \PP(A) + \PP(B \setminus A) \ge \PP(A)$.
		\item נובע ישירות מתכונה 3 ומ־$A \subseteq \Omega$.
		\item ניזכר כי $A^C = \Omega \setminus A$ ולכן $\Omega = A \cup A^C$ ונקבל $1 = \PP(\Omega) = \PP(A) + \PP(A^C)$.
	\end{enumerate}
\end{proof}
נעבור עתה לאפיון של פונקציות הסתברות בדידות, נבין מתי הן כאלה ומתי לא.
\begin{theorem}[תנאים שקולים לפונקציית הסתברות בדידה]
	אם $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, התנאים הבאים שקולים:
	\begin{enumerate}
		\item $\PP$ היא פונקציית הסתברות בדידה
		\item $\PP$ נתמכת על קבוצות בנות־מניה, כלומר קיימת קבוצה $A \in \mathcal{F}$ בת־מניה כך ש־$\PP(A) = 1$
		\item $\sum_{\omega \in \Omega} \PP(\{\omega\}) = 1$
		\item לכל מאורע $A \in \mathcal{F}$ מתקיים $\PP(A) = \sum_{\omega \in A} \PP(\{\omega\})$
	\end{enumerate}
\end{theorem}
\begin{proof}
	$1 \implies 2$:
	נניח ש־$\PP = \PP_p$ עבור $p : \Omega \to [0, \infty)$ פונקציית הסתברות נקודתית.
	נסתכל על $\supp(p) = \{ \omega \in \Omega \mid p(\omega) > 0 \}$, לפי הגדרת הסכום והתרגיל נובע ש־$A = \supp(p)$ בת־מניה.
	נקבל
	\[
		\PP(A) = \sum_{\omega \in A} p(\omega) = \sum_{\omega \in \Omega} p(\omega) = \PP(\Omega) = 1
	\]

	$2 \implies 4$:
	נניח ש־$\PP(S) = 1$ עבור $S$ בת־מניה. לכן $\PP(S^C) = 0$.
	נראה כי $A$ הוא איחוד זר $A = (A \cap S) \cup (A \cap S^C)$ ולכן נקבל
	\[
		\PP(A) = \PP(A \cap S) + \PP(A \cap S^C) = \PP(A \cap S) + 0 = \sum_{\omega \in A \cap S} \PP(\{\omega\}) = \sum_{\omega \in A} \PP(\{\omega\})
	\]

	$4 \implies 3$:
	אם נבחר $A = \Omega$ נקבל את טענה 3.

	$3 \implies 1$:
	נגדיר $p : \Omega \to [0, \infty)$ על־ידי $p(\omega) = \PP(\{ \omega \})$, נקבל $\sum_{\omega \in \Omega} p(\omega) = 1$ ולכן $p$ היא פונקציית הסתברות נקודתית.
	מהתרגיל והגדרת הסכום נובע ש־$S = \supp(p)$ היא בת־מניה ומתקיים $\PP(S^C) = 0$, אז לכל $A \in \mathcal{F}$ מתקיים
	\[
		\PP(A) = \PP(A \cap S) + \PP(A \cap S^C) = \PP(A \cap S) = \sum_{\omega \in A \cap S} \PP(\{ \omega \}) = \sum_{\omega \in A} \PP(\{\omega\}) = \sum_{\omega \in A} p(\omega) = \PP_p(A)
	\]
\end{proof}

\subsection{פרדוקס יום ההולדת}
פרדוקס יום ההולדת הוא פרדוקס מוכר הגורס כי גם בקבוצות קטנות יחסית של אנשים, הסיכוי שלשני אנשים שונים יהיה תאריך יום הולדת זהה הוא גבוה במידה משונה.
הפרדוקס נקרא כך שכן לכאורה אין קשר בין מספר הימים בשנה לבין הסיכוי הכל־כך גבוה שמצב זה יקרה, נבחן עתה את הפרדוקס בהיבט הסתברותי.

נניח שכל תאריכי יום ההולדת הם סבירים באותה מידה ונבחן את הפרדוקס.
נגדיר $\Omega = {[365]}^k$ עבור $k$ מספר האנשים בקבוצה נתונה כלשהי.
$p(\omega) = \frac{1}{{365}^k}$ לכל $\omega \in \Omega$.
נקבל $\PP(A) = \PP_p(A) = \frac{|A|}{365^k}$.
נרצה לחשב את $A$ כמאורע שיש לפחות שני אנשים שיש להם יום הולדת באותו יום, דהינו שיש שני ערכים זהים ברשימת המספרים, נגדיר $A = \{ \omega \in \Omega \mid \exists 1 \le i \ne j \le k, \omega_i = \omega_j \}$.
בשל המורכבות נבחן את המשלים $A^C$, נקבל $|A^C| = 365 \cdot 364 \cdots (365 - (k - 1)) = \frac{365!}{(365 - (i - 1))!}$.
נציב ונחשב:
\[
	\PP(A^C) = \frac{|A^C|}{365^k} = \prod_{i = 1}^k \frac{365 - (i - 1)}{365} = \prod_{i = 1}^k (1 - \frac{i - 1}{365})
\]
מהנוסחה שקיבלנו נראה שמההצבה $k = 23$ נקבל שההסתברות היא בערך $\frac{1}{2}$, דהינו בקבוצה של 23 אנשים יש סבירות של חצי שלפחות שניים יחגגו יום הולדת באתו יום.

\section{שיעור 3 --- 5.11.2024}

\subsection{מכפלת מרחבי הסתברות בדידים}
ניזכר תחילה במרחבי הסתברות אחידים
\begin{definition}[מרחב הסתברות אחיד]
	מרחב הסתברות אחיד הוא $(\Omega, \mathcal{F}, \PP_p)$ המקיים $p(\omega_1) = p(\omega_2)$ לכל $\omega_1, \omega_2 \in \Omega$.
\end{definition}
\begin{conclusion}
	$\PP_p(A) = \frac{|A|}{|\Omega|}$
\end{conclusion}
נבחין כי במקרים מסוימים ההסתברות שלנו מורכבת משני מאורעות בלתי תלויים, במקרים אלה נרצה להגדיר מכפלה של מרחבי ההסתברות.
\begin{definition}[מרחב מכפלת הסתברויות]
	אם $(\Omega_1, \mathcal{F}_1, \PP_{p_1})$ ו־$(\Omega_2, \mathcal{F}_2, \PP_{p_2})$ מרחבי הסתברות בדידים
	נגדיר $q : \Omega_1 \times \Omega_2 \to [0, \infty)$ על־ידי $q(\omega_1, \omega_2) = p(\omega_1) \cdot p(\omega_2)$.
\end{definition}
\begin{proposition}
	$q$ פונקציית הסתברות נקודתית.
\end{proposition}
\begin{proof}
	נשתמש ישירות בהגדרה ונחשב
	\[
		\sum_{(\omega_1, \omega_2) \in \Omega_1 \times \Omega_2} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in \Omega_1, \omega_2 \in \Omega_2} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in \Omega_1} \left( \sum_{\omega_2 \in \Omega_2} p_1(\omega_1) p_2(\omega_2) \right)
		= \sum_{\omega_1 \in \Omega_1} p_1(\omega_1)
		= 1
	\]
\end{proof}
עתה כשהוכחנו טענה זו, יש לנו הצדקה אמיתית להגדיר את $(\Omega_1 \times \Omega_2, \mathcal{F}_{1,2}, \PP_q)$ כמרחב הסתברות, ונקרא לו מרחב מכפלה.
\begin{proposition}
	אם $(\Omega_1, \mathcal{F}_1, \PP_{p_1})$ ו־$(\Omega_2, \mathcal{F}_2, \PP_{p_2})$ מרחבי הסתברות אחידים,
	אז מרחב המכפלה $(\Omega_1 \times \Omega_2, \mathcal{F}_{1,2}, \PP_q)$ אחיד אף הוא.
\end{proposition}
\begin{proof}
	\[
		q(\omega_1, \omega_2) = p_1(\omega_1) p_2(\omega_2)
		= \frac{1}{|\Omega_1|} \cdot \frac{1}{|\Omega_2|}
		= \frac{1}{|\Omega_1 \times \Omega_2|}
	\]
\end{proof}
\begin{definition}[מאורע שוליים ומאורע מכפלה]
	במרחב מכפלה המאורעות מהצורה $A \times \Omega_2$ או $\Omega_1 \times A$ נקראים שוליים. \\*
	מאורע מהצורה $A \times B$ נקרא מאורע מכפלה.
\end{definition}
\begin{proposition}
	במרחב מכפלה $\PP_q(A \times B) = \PP_{p_1}(A) \cdot \PP_{p_2}(B)$.
	בפרט $\PP_q(A \times \Omega_2) = \PP_{p_1}(A)$.
\end{proposition}
\begin{proof}
	\[
		\sum_{(\omega_1, \omega_2) \in A \times B} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in A, \omega_2 \in B} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in A} \left( \sum_{\omega_2 \in B} p_1(\omega_1) p_2(\omega_2) \right)
		= \sum_{\omega_1 \in A} p_1(\omega_1) \PP_{p_2}(B)
		= \PP_{p_1}(A) \PP_{p_2}(B)
	\]
\end{proof}
\begin{example}
	בהינתן $n$ הטלות מטבע כלשהו, מה ההסתברות שיצאו $k$ עצים?

	עבור ההטלה הראשונה, $\Omega_1 = \{0, 1\}$. עוד נגדיר $p(1) = \alpha, p(0) = 1 - \alpha$ עבור $0 \le \alpha \le 1$ כלשהו. \\*
	בהתאם נקבל $\Omega = {\{0, 1\}}^n$, וכן
	\[
		q(\omega_1, \dots, \omega_n) = \prod_{i = 1}^n p(\omega_i)
		= \prod_{i = 1}^n \alpha^{\omega_i} \cdot {(1 - \alpha)}^{1 - {\omega_i}}
		= \alpha^{\sum_{i = 1}^n \omega_i} {(1 - \alpha)}^{n - \sum_{i = 1}^n \omega_i}
	\]

	נבחין כי היינו יכולים לתאר את המקרה הזה ממש על־ידי $q(\omega) = \alpha^\omega \cdot {(1 - \alpha)}^{1 - \omega}$.

	נעבור עתה לבחינת המאורע
	\[
		A = \{ (\omega_1, \dots, \omega_n) \in \Omega \mid \sum_{i = 1}^{n} \omega_i = k \}
	\]
	נקבל מהביטוי שמצאנו כי
	\[
		\PP_q(A)
		= \sum_{(\omega_1, \dots, \omega_n) \in A} q(\omega_1, \dots, \omega_n)
		\sum_{\sum_{i = 1}^n \omega_i = k} \alpha^{\sum_{i = 1}^n \omega_i} {(1 - \alpha)}^{n - \sum_{i = 1}^n \omega_i}
		= |A| \alpha^k {(1 - \alpha)}^{n - k}
		= \binom{n}{k} \alpha^k {(1 - \alpha)}^{n - k}
	\]
\end{example}
\begin{example}
	נבחן עתה את המקרה של הטלות הוגנות ובחינת המקרה שחצי מההטלות לפחות יצאו עץ,
	זאת־אומרת שנבחן את הדוגמה הקודמת כאשר $n = 2m, k = m$, ו־$\alpha = \frac{1}{2}$.
	מנוסחת סטרלינג שאנחנו לא מכירים $m! \simeq \sqrt{2\pi m} {(\frac{m}{e})}^m$ ואז נוכל להסיק
	\[
		\PP_q(A)
		= \binom{2m}{m} \frac{1}{2^m}
		\simeq \frac{\sqrt{4\pi m} {(\frac{2m}{e})}^{2m}}{{(\sqrt{2\pi m} {(\frac{k}{e})}^m)}^2 2^{2m}}
		= \frac{\sqrt{4\pi m}}{2\pi m}
		= \frac{1}{\sqrt{\pi m}}
	\]
\end{example}

\subsection{ניסויים דו־שלביים}
נניח $(\Omega_1, \mathcal{F}_1, \PP_{p_1})$ מרחב הסתברות בדידה עבור הניסוי הראשון, ונניח שיש מרחב הסתברות בדידה עבור הניסוי השני כך שלכל תוצאה בניסוי הראשון, פונקציית ההסתברות תשתנה בהתאם בניסוי השני.
לכל $\omega_1 \in \Omega_1$ יש פונקציית הסתברות נקודתית $p_{\omega_1} : \Omega_2 \to [0, \infty)$.
נגדיר את מרחב הניסוי הדו־שלבי $(\Omega_1 \times \Omega_2, \mathcal{F}_{1, 2}, \PP_q)$,
כאשר $q(\omega_1, \omega_2) = p_1(\omega_1) \cdot p_{\omega_1}(\omega_2)$.
\begin{proposition}
	$\PP_q$ פונקציית הסתברות.
\end{proposition}
\begin{proof}
	\[
		\sum_{(\omega_1, \omega_2) \in \Omega_1 \times \Omega_2} q(\omega_1, \omega_2)
		= \sum_{\omega_1 \in \Omega_1} \left( \sum_{\omega_2 \in \Omega_2} p_1(\omega_1) p_{\omega_1}(\omega_2) \right)
		= \sum_{\omega_1 \in \Omega_1} p_1(\omega_1) \left( \sum_{\omega_2 \in \Omega_2} p_{\omega_1}(\omega_2) \right)
		= \sum_{\omega_1 \in \Omega_1} p_1(\omega_1)
		= 1
	\]
\end{proof}
\begin{example}
	$\Omega_1 = \{H, T\}$ ו־$\Omega_2 = \{1, \dots, 8\}$, נגדיר $p_1(H) = p_1(T) = \frac{1}{2}$.
	עוד נגדיר
	\[
		p_H(\omega_2) = \begin{cases}
			\frac{1}{6} & 1 \le \omega_2 \le 6 \\
			0 & \text{else}
		\end{cases},
		\qquad
		p_T(\omega_2) = \frac{1}{8}
	\]
	מהגדרה זו נקבל
	\[
		q(\omega_1, \omega_2) = \begin{cases}
			\frac{1}{12} & \omega_1 = H, \omega_2 \in [6] \\
			0 & \omega_1 = H, \omega_2 \in \{7, 8\} \\
			\frac{1}{16} & \omega_1 = T, \omega_2 \in [8]
		\end{cases}
	\]
\end{example}
\begin{theorem}[חסם האיחוד]
	אם $A, B$ מאורעות אז $\PP(A \cup B) \le \PP(A) + \PP(B)$.
\end{theorem}
\begin{proof}
	\[
		\PP(A \cup B)
		= \PP(A \uplus (B \setminus A))
		= \PP(A) + \PP(B \setminus A)
		\le \PP(A) + \PP(B)
	\]
\end{proof}
נוכל להשתמש בחסם האיחוד כדי להוכיח גרסה כללית יותר של המשפט:
\begin{theorem}[אי־שוויון בול]
	אם $A_1, \dots, A_k$ מאורעות, אז $\PP(\bigcup_{i = 1}^k A_i) \le \sum_{i = 1}^k \PP(A_i)$.
\end{theorem}
\begin{example}
	נחזור לבחון את פרדוקס יום ההולדת, הפעם נבחן גרסה כללית יותר של הרעיון.
	נגדיר $\Omega = {[m]}^k$ עם הסתברות אחידה.
	נגדיר גם $A = \{ \omega \in \Omega \mid \exists 1 \le i < j \le k, \omega_i = \omega_j \}$.
	אנו רוצים את ההסתברות $\PP(A) = \frac{|A|}{|\Omega|}$, אז נבחן את המשלים
	\[
		A^C = \{ \omega \in \Omega \mid \forall 1 \le i, j \le k, i \ne j \implies \omega_i \ne \omega_j \}
	\]
	נחשב
	\[
		|A^C| = m (m - 1) \cdots (m - (k - 1))
	\]
	בהתאם
	\[
		\PP(A^C)
		= \frac{\prod_{i = 0}^{k - 1} (m - i)}{m^k}
		= \prod_{i = 0}^{k - 1} \frac{m - i}{m^k}
		= \prod_{i = 0}^{k - 1} (1 - \frac{i}{m})
	\]
	נזכור ש־$\forall x \in \RR, 1 + x \le e^x$, ונוכל לקבל
	\[
		\prod_{i = 0}^{k - 1} (1 - \frac{i}{m})
		\le \prod_{i = 0}^{k - 1} e^{-\frac{i}{m}}
		= \exp(- \frac{1}{m} \sum_{i = 0}^{k - 1} i)
		= e^{- \frac{k(k - 1)}{2m}}
	\]
	כאשר $k$ גדול ביחס ל־$\sqrt{2m}$ מקבלים חסם קרוב ל־$0$.

נגדיר הפעם $A = \bigcup_{\substack{i \ne j \\ i, j \in [k]}} A_{ij}$ עבור $A_{ij} = \{ \omega \in \Omega \mid \omega_i = \omega_j \}$.
	וגם
	\[
		i \ne j \implies \PP(A_{ij}) = \frac{|A_{ij}|}{m^k} = \frac{m \cdot m^{k - 2}}{m^k} = \frac{1}{m}
	\]
	ועתה
	\[
		\PP(A)
		\le \sum_{\substack{i \ne j \\ i, j \in [k]}} \PP(A_{ij})
		= \sum_{\substack{i \ne j \\ i, j \in [k]}} \frac{1}{m}
		= \binom{k}{2} \frac{1}{m}
		= \frac{k (k - 1)}{2m}
	\]
	לכן אם $k$ קטן ביחס ל־$\sqrt{2m}$ אז ההסתברות ליום־הולדת משותף קטנה.
\end{example}

\section{תרגול 2 --- 7.11.2024}

\subsection{פתרון שאלות הסתברותיות}
נתחיל בבחינת טענה שימושית לביצוע חישובי הסתברות:
\begin{proposition}[נוסחת ההסתברות השלמה]
	יהי $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, $\mathcal{A}$ חלוקה בת־מניה של $\Omega$, לכל $B \in \mathcal{F}$ מתקיים
	\[
		\PP(B) = \sum_{A \in \mathcal{A}} \PP(A \cap B)
	\]
	נניח שיש מרחב הסתברות ויש חלוקה בת מניה של המרחב, אז לכל מאורע ההסתברות שלו היא הסכום על החלוקה על החיתוך של החלוקה ו־$A$.
\end{proposition}
\begin{proof}
	נשים לב כי $B = \biguplus{A \in \mathcal{A}} B \cap A$ איחוד זר, ולכן הטענה נובעת מסיגמא־אדיטיביות.
\end{proof}
\begin{exercise}
	קוביה מוטה בעלת 6 פאות עם הסתברות נקודתית $p(i) = \frac{i}{21}$ מוטלת 5 פעמים. \\*
	מה ההסתברות שתוצאת ההטלה הראשונה התקבלה פעם אחת ויחידה?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {[6]}^5$ ונגדיר $\PP(x_1, \dots, x_5) = p(x_1) \cdots p(x_5)$.
	אנו רוצים לחשב את
	\[
		B = \{ (x_1, \dots, x_5) \in \Omega \mid \forall j \ne 1, x_j \ne x_1 \}
	\]
	נגדיר חלוקה $\mathcal{A} = \{ A_1, \dots, A_6 \}$ של $\Omega$ כך ש־$A_i = \{ (i, x_2, \dots, x_5) \in \Omega \mid 1 \le x_j \le 6 \}$. \\*
	נקבל
	\[
		\PP(B \cap A_i) = \frac{i}{21} \cdot {(1 - \frac{i}{21})}^4
	\]
	על־ידי שימוש בנוסחת ההסתברות השלמה נקבל
	\[
		\PP(B) = \sum_{i = 1}^{6} \PP(B \cap A_i) = \sum_{i = 1}^{6} \frac{i}{21} {(1 - \frac{i}{21})}^4
	\]
\end{solution}
נראה עתה דוגמה לשימוש בחסם האיחוד בן־המניה, אותו נראה בהרצאה הבאה
\begin{proposition}[חסם האיחוד הבן־מניה]
	אם $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות ו־${\{A_i\}}_{i = 1}^\infty \subseteq \mathcal{F}$ אז מתקיים
	\[
		\PP(\bigcup_{i = 1}^\infty A_i) \le \sum_{i \in \NN} \PP(A_i)
	\]
\end{proposition}
\begin{exercise}
	משלשלים $k$ פתקי הצבעה בין $n$ קלפיות. \\*
	מה ההסתברות שאין קלפי עם יותר מפתק אחד?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = \{ (x_1, \dots, x_n) \mid 0 \le x_i, x_1 + \cdots + x_n = k \}$.
	נחשב ונקבל $|\Omega| = \binom{n + k - 1}{k - 1}$. \\*
	נגדיר את המאורע, $A = \{ (x_1, \dots, x_n) \in \Omega \mid x_i \le 1 \}$. \\*
	ננסה לחסום את המשלים,
	\[
		\Omega \setminus A = \{ (x_1, \dots, x_n) \in \Omega \mid \exists i, x_i \ge 2 \}
	\]
	אם נגדיר $A_i = \{ (x_1, \dots, x_n) \in \Omega \mid x_i \ge 2 \}$ אז נוכל להגדיר
	\[
		\Omega \setminus A = \bigcup_{i \in [n]} A_i
	\]
	נחשב את ההסתברות של כל $A_i$, מתקבל $|A_i| = \binom{n + k - 3}{k - 3}$ מהשיקול של סכימת הפתרונות השלמים תוך התעלמות משני פתקים. \\*
	לכן
	\[
		\PP(A_i) = \frac{|A_i|}{|\Omega|} = \frac{\binom{n + k - 3}{k - 3}}{\binom{n + k - 1}{k - 1}} = \frac{k(k - 1)}{(k + n - 1)(k + n - 2)}
	\]
	מחסם האיחוד נובע
	\[
		\PP(\Omega - A) \le \sum_{i = 1}^{n} \frac{k(k - 1)}{(k + n - 1)(k + n - 2)} = n \cdot \frac{k(k  -1)}{(n + k - 1)(n + k - 2)}
	\]
	ועל־ידי מעבר למשלים שוב נוכל להסיק $\PP(A) \ge 1 - n \cdot \frac{k(k  -1)}{(n + k - 1)(n + k - 2)}$. \\*
	נזכור כי אנו מנסים להבין את המגמה כאשר המספרים מאוד גדולים, לכן נבחן את המקרה ש־$n \to \infty$, אז נובע $\PP(A) \xrightarrow[n \to \infty]{} 1$, \\*
	דהינו כאשר יש כמות קלפיות הולכת וגדלה הסיכוי שיהיה פתק יחיד בכל אחת (מספר הפתקים לא משתנה) הולך וגדל ומתקרב לסיכוי מלא.
\end{solution}
נראה עתה דוגמה לשימוש במרחבי ניסוי דו־שלביים:
\begin{exercise}
	מה ההסתברות שנגריל מספר $m$ בין $1$ ל־$n$,
	ואז נגריל עוד מספר והוא יהיה בין $1$ לבין $m$?
\end{exercise}
\begin{solution}
	נבנה פונקציית הסתברות עבור הניסוי השני, נניח שבניסוי השני קיבלנו $m$:
	\[
		p_m(k) = \begin{cases}
			\frac{1}{m} & k \le m \\
			0 & k > m
		\end{cases},
		\qquad
		q(m, k) = \begin{cases}
			\frac{1}{mn} & k \le m \\
			0 & k > m
		\end{cases}
	\]
	נגדיר $A_k$ המאורע שתוצאת ההגרלה השניה היא $k$, לכן
	\[
		\PP(A_k)
		= \PP(\{(m, k) \in \Omega \mid m \le k \})
		= \sum_{m = 1}^{n} q(m, k)
		= \sum_{m = k}^{n} \frac{1}{mn}
	\]
	נבחין כי המעבר האחרון אכן תקין, שכן קיבענו את המשתנה השני, זאת אומרת שעכשיו במקום להסתכל על מספר שיותר קטן ממספר אחר, אנו בוחנים את המספר החוסם מלמעלה, המספר הגדול יותר. \\*
	לדוגמה
	\[
		\PP(A_n) = \frac{1}{n^2},
		\qquad
		\PP(A_1) = \sum_{m = 1}^{n} \frac{1}{mn} = \frac{1}{n} \sum_{m = 1}^n \frac{1}{m} \approx \frac{\log n}{n}
	\]
\end{solution}
נבחן דוגמה ספציפית כהמשך של השאלה הזו, הפעם נגדיר $m = n / 2$:
\begin{example}
	נגדיר $B_{n / 2}$ להיות המאורע בהתחלה השניה ו־$B$ שבהגרלה השניה יצא מספר גדול מ־$n / 2$
	\[
		\PP(B_{n / 2}) = \PP(\bigcup_{k \ge n / 2}^n A_k)
		= \frac{1}{n} \sum_{k \ge \frac{n}{2}}^n \sum_{m = k}^n \frac{1}{m}
		= \frac{1}{n} \sum_{m = \lceil \frac{n}{2}\rceil}^{n} \frac{\frac{n}{2} + 1 - n + m}{m}
	\]
	כמו בשאלה הקודמת, גם הפעם נרצה להבין מגמה כללית, ולכן נבדוק את הביטוי כאשר $n$ שואף לאינסוף, דהינו שהמספרים שאפשר להגדיל הולכים וגדלים בכמותם:
	\[
		\lim_{n \to \infty} \PP(B_{n / 2})
		= \lim_{n \to \infty} \frac{1}{n} \sum_{m = \lceil \frac{n}{2}\rceil}^{n} \frac{1 + m - \frac{n}{2}}{m}
	\]
	נבחין כי $\sum_{m = 1}^n \frac{1}{m} = \log(n) + e + o(\frac{1}{m})$ ולכן
	\begin{align*}
		\lim_{n \to \infty} \frac{1}{n} \sum_{m = \lceil \frac{n}{2}\rceil}^{n} \frac{1 + m - \frac{n}{2}}{m}
		& = \lim_{n \to \infty} \frac{1}{2} + \frac{n}{2n} (\log(n) - \log(\frac{n}{2}) + o(\frac{1}{n})) + \frac{1}{n} (\log(n) - \log(\frac{n}{2}) + o(\frac{1}{n})) \\
		& = \frac{1}{2} + \frac{1}{n} \log 2
	\end{align*}
\end{example}

\section{שיעור 4 --- 7.11.2024}
בשיעור הקודם דיברנו על מרחבי מכפלה וניסויים דו־שלביים.
ברור לנו כי על־ידי שרשור דומה לתהליך של ניסוי דו־שלבי נוכל לבנות ניסוי רב־שלבי.
עוד דיברנו על חסם האיחוד, הטענה כי $\PP(\bigcup_{i = 1}^n A_i) = \sum_{i = 1}^n \PP(A_i)$.
השימוש של חסם האיחוד מאפשר לנו לפשט חישובים שבהם אנחנו רוצים הבנה כללית של ההתנהגות של מרחב ההסתברות.

\subsection{חסמי איחוד ורציפות}
\begin{definition}[סדרת מאורעות עולה]
	סדרת מאורעות ${\{A_n\}}_{n = 1}^\infty$ נקראת עולה אם $A_n \subseteq A_{n + 1}$ לכל $n \in \NN$.
\end{definition}
\begin{notation}
	נסמן $A_\infty = \bigcup_{n \in \NN} A_n$.
\end{notation}
\begin{theorem}[משפט רציפות פונקציית ההסתברות]
	אם ${\{A_n\}}_{n = 1}^\infty$ סדרת מאורעות עולה אז
	\[
		\PP(A_\infty) = \lim_{n \to \infty} \PP(A_n)
	\]
\end{theorem}
המשפט נקרא כך בשל ההקבלה שלו לקונספט של רציפות בפונקציות רגילות, עבור $f : \RR \to \RR$ היא רציפה ב־$a$ אם ורק אם לכל סדרה $x_n \to a$ מתקיים $f(x_n) \to f(a)$.
\begin{proof}
	נגדיר $B_n = A_n \setminus A_{n - 1}$ כאשר $B_1 = A_1 \setminus \emptyset = A_1$. \\*
	נראה כי מתקיים $\biguplus_{n = 1}^m B_n = A_m$ איחוד זר: \\*
	$\bigcup_{n = 1}^m B_n = A_m$ כי לכל $\omega \in A_m$ יש $n$ מינימלי כך ש־$\omega \in A_n$, אבל $\omega \notin A_{n - 1}$, לכן נוכל להסיק כי $\omega \in A_n \setminus A_{n - 1} = B_n$. \\*
	אם $\omega \in B_n = A_n \setminus A_{n - 1}$ אז $\omega \notin A_{n - 1}$ ולכן $\omega \notin A_k$ לכל $k < n$.
	מסיגמא־אדיטיביות נסיק
	\[
		\sum_{n = 1}^{m} \PP(B_n) = \PP(A_m)
	\]
	וגם
	\[
		\sum_{n = 1}^{\infty} \PP(B_n)
		= \PP(\biguplus_{n = 1}^\infty B_n)
		= \PP(\bigcup_{m = 1}^\infty \left(\biguplus_{n = 1}^\infty B_n\right))
		= \PP(\bigcup_{m = 1}^\infty A_m)
	\]
	מצד שני מהגדרת הגבול
	\[
		\sum_{n = 1}^{\infty} \PP(B_n)
		= \lim_{m \to \infty} \sum_{n = 1}^{m}  \PP(B_n)
		= \lim_{m \to \infty} \PP(A_m)
	\]
\end{proof}
\begin{definition}[סדרת מאורעות יורדת]
	נגדיר סדרת מאורעות ${\{A_n\}}_{n = 1}^\infty$ כך שמתקיים $A_{n + 1} \subseteq A_n$ לכל $n \in \NN$.
\end{definition}
נוכל להסיק מהעובדה שמשלים של סדרה עולה הוא סדרה יורדת ונקבל
\begin{proposition}
	\[
		\PP(\bigcap_{n \in \NN} A_n) = \lim_{n \to \infty} \PP(A_n)
	\]
\end{proposition}
\begin{proposition}[חסם האיחוד הבן־מניה]
	אם ${\{A_n\}}_{n = 1}^\infty$ סדרת מאורעות אז מתקיים
	\[
		\PP(\bigcup_{n = 1}^\infty A_n) \le \sum_{n \in \NN} \PP(A_n)
	\]
\end{proposition}
\begin{proof}
	נגדיר $B_m = \bigcup_{n = 1}^m A_n$, זוהי סדרה עולה ולכן
	\[
		\PP(\bigcup_{n = 1}^\infty A_n)
		= \PP(\bigcup_{m = 1}^\infty B_m)
		= \lim_{m \to \infty} \PP(B_m)
		\le \lim_{m \to \infty} \sum_{n = 1}^{m} \PP(A_n)
		= \sum_{n = 1}^{\infty} \PP(A_n)
	\]
\end{proof}

\subsection{עיקרון ההכלה וההדחה}
\begin{proposition}
	אם $A, B$ מאורעות אז
	\[
		\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)
	\]
\end{proposition}
\begin{proof}
	נגדיר $C = A \setminus B, D = A \cap B, E = B \setminus A$, נקבל
	\[
		A = C \uplus D,
		\quad
		B = D \uplus E,
		\quad
		A \cup B = C \uplus D \uplus E
	\]
	ונקבל
	\[
		\PP(A) = \PP(C) + \PP(D),
		\quad
		\PP(D \cup B) = \PP(D) + \PP(E)
	\]
	ולכן
	\[
		\PP(A \cup B) = \PP(C) + \PP(D) + \PP(E)
	\]
\end{proof}
\begin{theorem}[הכלה והפרדה לשלושה מאורעות]
	עבור שלושה מאורעות $A, B, C$:
	\[
		\PP(A \cup B \cup C) = \PP(A) + \PP(B) + \PP(C) - (\PP(A \cap B) + \PP(A \cap C) + \PP(B \cap C)) + \PP(A \cap B \cap C)
	\]
\end{theorem}
\begin{theorem}[הכלה והפרדה ל־n מאורעות]
	יהיו $A_1, \dots, A_n$ מאורעות, אז
	\[
		\PP(\bigcup_{i = 1}^n A_i)
		= \sum_{i = 1}^{n} \PP(A_i) - \sum_{i = 1}^{n} \sum_{j = 1}^{i - 1} \PP(A_i \cap A_j) + \sum_{i = 1}^{n} \sum_{j = 1}^{i - 1} \sum_{k = 1}^{j - 1} \PP(A_i \cap A_j \cap A_k) + \dots
	\]
	אם נגדיר $A_I = \bigcap_{i \in I} A_i$ לכל $I \subseteq [n]$ אז נקבל
	\[
		\PP(\bigcup_{n = 1}^n A_i)
		= \sum_{k = 1}^{n} {(-1)}^{k + 1} \sum_{\substack{I \subseteq [i] \\ |I| = k}} \PP(A_I)
		= \sum_{\emptyset \ne I \subseteq [n]} {(-1)}^{|I| + 1} \PP(A_I)
	\]
\end{theorem}
את משפט זה נוכיח בהמשך הקורס. \\*
נראה דוגמה לבעיה קלאסית במקרים אלה.
\begin{exercise}[בעיית ההתאמה]
	מחלקים $n$ מעטפות ל־$n$ תיבות דואר, אחת לכל תיבה, מה ההסתברות שאף מכתב לא הגיע ליעדו?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = S_n$ מרחב אחיד.
	$A = \{ \omega \in \Omega \mid \forall i, \omega(i) \ne i \}$. \\*
	נבחן את המשלים, $A^C = \{ \omega \in \Omega \mid \exists i, \omega(i) = i \} = \bigcup_{i = 1}^n A_i$ עבור $A_i = \{ \omega \in \Omega \mid \omega(i) = i \}$.
	נחשב
	\[
		\PP(A_i) = \frac{|A_i|}{|\Omega|} = \frac{(n - 1)!}{n!} = \frac{1}{n}
	\]
	במקרה של חיתוך $\PP(A_i \cap A_j)$ עבור $j < i$ נקבל
	\[
		\PP(A_i \cap A_j) = \frac{|A_i \cap A_j|}{|\Omega|} = \frac{(n - 2)!}{n!} = \frac{1}{n(n - 1)}
	\]
	נוכל להמשיך את התהליך הזה, ונקבל
	\[
		\PP(A_I) = \frac{|\bigcap_{i \in I} A_i|}{|\Omega|}
		= \frac{(n - |I|)!}{n!}
		= \frac{1}{n (n - 1) (n - 2) \cdots (n - (I + 1))}
	\]
	כעת נותר להשתמש בנוסחה להכלה והדחה, ונקבל
	\[
		\PP(\bigcup_{i = 1}^n A_i)
		= \sum_{k = 1}^{n} {(-1)}^{k + 1} \sum_{\substack{I \subseteq [n] \\ |I| = k}} \frac{(n - k)!}{n!}
		= \sum_{k = 1}^{n} {(-1)}^{k + 1} \binom{n}{k} \frac{(n - k)!}{n!}
		= \sum_{k = 1}^{n} \frac{{(-1)}^{k + 1}}{k!}
	\]
	נשים לב כי רצינו לחשב את המשלים למאורע, לכן
	\[
		\PP(A)
		= 1 - \PP(\bigcup_{i = 1}^n A_i)
		= 1 + \sum_{k = 1}^{n} \frac{{(-1)}^k}{k!}
		= \sum_{k = 0}^{n} \frac{{(-1)}^k}{k!}
		\xrightarrow[n \to \infty]{} e^{-1}
	\]
	נקבל שאוסף התמורות ללא נקודת שבת הוא
	\[
		|A^n| = n! \sum_{l = 0}^{n} \frac{{(-1)}^l}{l!}
	\]
	נגדיר קבוצה חדשה
	\[
		D_k = \{ \omega \in S_n \mid \exists i, \omega(i) = i \}
		= \biguplus_{\substack{I \subseteq [n] \\ |I| = k }} D_I
	\]
	ונבחין כי
	\[
		D_I = \{ \omega \in S_n \mid \forall i \in I, \omega(i) = i, \forall i \notin I, \omega(i) \ne i \}
	\]
	ולכן
	\begin{align*}
		\PP(D_k)
		& = \sum_{\substack{I \subseteq [n] \\ |I| = k }} \PP(D_I) \\
		& = \sum_{\substack{I \subseteq [n] \\ |I| = k }} \frac{|D_I|}{n!} \\
		& = \sum_{\substack{I \subseteq [n] \\ |I| = k }} \frac{(n - k)! \sum_{l = 0}^n \frac{{(-1)}^l}{l!}}{n!} \\
		& = \binom{n}{k} \frac{(n - k)!}{n!} \sum_{l = 0}^n \frac{{(-1)}^l}{l!} \\
		& = \frac{1}{k!} \sum_{l = 0}^n \frac{{(-1)}^l}{l!} \\
		& \xrightarrow[n \to \infty]{} \frac{e^{-1}}{k!}
	\end{align*}
\end{solution}

\section{שיעור 5 --- 12.11.2024}

\subsection{הסתברות מותנית}
\begin{definition}[הסתברות מותנית]
	$A, B$ מאורעות, ההסתברות המותנית של $A$ בהינתן $B$ תוגדר להיות
	\[
		\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
	\]
\end{definition}
\begin{example}
	אם מטילים שתי קוביות מאוזנות, מה ההסתברות שיצא 3 בקוביה הראשונה בהינתן שהסכום הוא 8?

	נגדיר כמובן $\Omega = {[6]}^2$, וכן נגדיר $A = \{ (3, i) \in \omega \mid 1 \le i \le 6 \}$ וכן $B = \{ (2, 6), (3, 5), (4, 4), (5, 3), (6, 2) \}$.
	\[
		\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
		= \frac{\frac{1}{36}}{\frac{5}{36}}
		= \frac{1}{5}
	\]
\end{example}
\begin{proposition}
	נקבע מאורע $B$ עם הסתברות $\PP(B) > 0$, נגדיר $\PP_B(A) = \PP(A \mid B)$, דהינו $\PP_B : \mathcal{F} \to [0, \infty)$. \\*
	אז $\PP_B$ היא פונקציית הסתברות.
\end{proposition}
\begin{proof}
	$\PP_B(A)$ היא אי־שלילית. \\*
	נראה גם
	\[
		\PP_B(\Omega) = \frac{\PP(\Omega \cap B)}{\PP(B)} = 1
	\]
	ולבסוף
	\[
		\PP_B(\biguplus_{i \in I} A_i)
		= \frac{(\PP_B(\biguplus_{i \in I} A_i )) \cap B}{\PP(B)}
		= \frac{\PP_B(\biguplus_{i \in I} A_i \cap B)}{\PP(B)}
		= \sum_{i \in I} \frac{\PP(A_i \cap B)}{\PP(B)}
		= \sum_{i \in I} \PP_B(A_i)
	\]
\end{proof}
\begin{proposition}
	יהיו $C, B$ מאורעות המקיימים $\PP(B \cap C) > 0$, נסמן $\PP' = \PP_B$ ו־$\PP'' = \PP_C'$. \\*
	אז לכל מאורע $A$ מתקיים $\PP''(A) = \PP(A \mid B \cap C)$ או בחילוף סימונים $\PP'' = \PP_{B \cap C}$.
\end{proposition}
\begin{proof}
	\[
		\PP''(A)
		= \PP_C'(A)
		= \frac{\PP'(A \cap C)}{\PP'(C)}
		= \frac{\PP_B(A \cap C)}{\PP_B(C)}
		= \frac{\frac{\PP(B \cap (A \cap C))}{\PP(B)}}{\frac{\PP(B \cap C)}{\PP(B)}}
		= \PP_{B \cap C}(A)
	\]
\end{proof}
מצאנו כי התניה חוזרת היא אסוציאטיבית ולכן נוכל לדבר על הסתברות מותנית בכמה מאורעות ללא התייחסות לסדר שלהם, למעשה התנייה מותנית היא קומוטטיבית כפי שאפשר לראות בהוכחה.
\begin{conclusion}[נוסחת ההסתברות השלמה בהסתברות מותנית]
	נניח ש־${\{A_i\}}_{i \in \NN}$ חלוקה בת־מניה של $\Omega$ ו־$B$ מאורע כלשהו, אז
	\[
		\PP(B) = \sum_{i \in \NN} \PP(A_i) \PP(B \mid A_i)
	\]
\end{conclusion}
\begin{proof}
	\[
		\PP(A_i) \PP(B \mid A_i) = \PP(A_i) \frac{\PP(B \cap A_i)}{\PP(A_i)} = \PP(B \cap A_i)
	\]
	ולכן
	\[
		\biguplus_{i \in \NN} (B \cap A_i) = B
		\implies \PP(B) = \sum_{i \in \NN} \PP(B \cap A_i)
	\]
\end{proof}
\begin{lemma}[כלל בייס]
	אם $A, B$ מאורעות עם הסתברות חיובית אז
	\[
		\PP_A(B) = \frac{\PP(B)}{\PP(A)} \PP_B(A)
	\]
\end{lemma}
\begin{proof}
	ישירות מהגדרה נסיק
	\[
		\PP_A(B)
		= \frac{\PP(A \cap B)}{\PP(A)}
		= \frac{\PP(B)}{\PP(A)} \cdot \frac{\PP(A \cap B)}{\PP(B)}
		= \frac{\PP(B)}{\PP(A)} \PP_B(A)
	\]
\end{proof}
\begin{conclusion}[כלל השרשרת]
	\[
		\PP(A \cap B) = \PP(A) \PP(B \mid A)
	\]
\end{conclusion}
\begin{exercise}
	מטילים מטבע הוגן. אם יוצא עץ נוסעים לתל־אביב ואם יוצא פלי אז ונסעים לחיפה.
	כשנוסעים לתל־אביב יש הסתברות של אחוז אחד לפנצ'ר, ובנסיעה לחיפה יש הסתברות של 2 אחוז לפנצ'ר. \\*
	מה ההסתברות לפנצ'ר ומה ההסתברות שנסעו לתל־אביב?
\end{exercise}
\begin{solution}
	נגדיר $A$ הוא עץ או לנסוע לתל־אביב ו־$B$ ההסתרות שיהיה פנצ'ר, בהתאם
	\[
		\PP(A^C) = \PP(A) = \frac{1}{2},
		\qquad
		\PP(B \mid A) = 0.01,
		\PP(B \mid A^C) = 0.02
	\]
	בהתאם
	\[
		\PP(B) = \PP(A) \PP(B \mid A) + \PP(A^C) + \PP(B \mid A^C)
		= \frac{1}{2} 0.01 + \frac{1}{2} 0.02 = 0.015
	\]
	באשר לשאלה השנייה נקבל
	\[
		\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)} = \frac{\PP(A)}{\PP(B)} \PP(B \mid A) = \frac{\frac{1}{2}}{0.015} \cdot 0.01 = \frac{1}{3}
	\]
\end{solution}
נבחין כי התוצאה יצאה מאוד אלגנטית כתוצאה מהמטבע ההוגן, אילו הוא היה לא הוגן היינו מקבלים חישוב שונה במקצת, אך תקף באותה המידה.
\begin{example}[מונטי הול]
	יש שלוש דלתות, בוחרים אחת, מנחה פותח דלת שלא נבחרה ומאחוריה אין כלום, מה שאומר שמאחורי אחת הדלתות הסגורות יש אוצר ובאחרות יש עז.
	המנחה מציע לכם להחליף את הדלת שבחרתם.

	קשה למדל את הבעיה הזו, שכן חסר תיאור והגדרה, אז נאמר שהגרלנו מספר ב־$[3]$, נניח שבחרנו $1$, נניח שהמנחה גם במכוון תמיד בוחר דלת ריקה.
	נוסיף את ההנחה שאם האוצר מאחורי דלת 1 אז המנחה פותח את 2 או 3, וההסתברויות שוות.
	
	נעבור להגדרה, $A_i$ המאורע שהאוצר ב־$i$ ו־$B_i$ היא שהמנחה פותח את דלת $i$.
	מההנחות שלנו נובע $\PP(B_3 \mid A_2) = 1, \PP(B_2 \mid A_3) = 1, \PP(B_3 \mid A_1) = \PP(B_2 \mid A_1) = \frac{1}{2}$.
	לבסוף נניח כי $\PP(A_i) = \frac{1}{3}$ לכל שלוש הדלתות. \\*
	נרצה לחשב את $\PP(A_1 \mid B_2)$:
	\[
		\PP(A_1 \mid B_2)
		= \frac{\PP(A_1)}{\PP(B_2)} \cdot \PP(B_2 \mid A_1)
		= \frac{\frac{1}{6}}{\PP(B_2)}
	\]
	וגם
	\[
		\PP(B_2) = \PP(A_1) \PP(B_2 \mid A_1) + \PP(A_2) \PP(B_2 \mid A_2) + \PP(A_3) \PP(B_2 \mid A_3) = \frac{1}{3} \cdot \frac{1}{2} + \frac{1}{3} \cdot 0 + \frac{1}{3} \cdot 1 = \frac{1}{2}
	\]
\end{example}

\section{תרגול 3 --- 14.11.2024}

\subsection{הסתברות מותנית}
\begin{exercise}
	מטילים זוג קוביות הוגנות ושונות. נתון שסכום תוצאותיהן גדול מעשר,
	מה ההסתברות שבהטלה השנייה יצא 6?
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {[6]}^2$ עם $\PP$ אחידה. \\*
	עוד נגדיר $A = \{ (x, y) \in \Omega \mid x + y > 10 \}$ וכן $B = \{ (x, 6) \in \Omega \}$, לכן
	\[
		\PP(B \mid A)
		= \frac{\PP(A \cap B)}{\PP(A)}
		= \frac{\frac{|A \cap B|}{|\Omega|}}{\frac{|A|}{|\Omega|}}
		= \frac{|A \cap B|}{|A|} = \frac{2}{3}
	\]
\end{solution}
\begin{exercise}
	אדם מחפש מכתב, זכור לו במעורפל בהסתברות $0 \le p \le 1$ שהניח אותו באחת ממגירות שולחן העבודה. \\*
	בשולחן $n$ מגירות והאדם חיפש ב־$k$ המגירות הראשונות ולא מצא את המכתב. \\*
	מה ההסתברות שהמכתב בשולחן?
\end{exercise}
\begin{solution}
	נגדיר $A$ להיות המאורע שהמכתב בשולחן ו־$B_k$ המכתב לא באף אחת מ־$k$ המגירות הראשונות. אנו מחפשים את $\PP(A \mid B_k)$. \\*
	לכן
	\[
		\PP(A \mid B_k)
		= \frac{\PP(A \cap B_k)}{\PP(B_k)}
	\]
	עוד אנו יודעים כי
	\[
		\PP(A) = p,
		\PP(B_k) = 1 - \frac{kp}{n}
	\]
	אזי
	\[
		\frac{\PP(A \cap B_k)}{\PP(B_k)}
		= \frac{\frac{(n - k)p}{n}}{\frac{n - kp}{n}}
		= \frac{(n - k) p}{n - kp}
	\]
\end{solution}
\begin{exercise}
	האדם הוא מתודי והחליט להפסיק את החיפוש אם ההסתברות שהמכתב בשולחן קטנה מ־$\frac{1}{4}$. \\*
	נניח ש־$p = \frac{3}{4}$ ושיש 10 מגירות, כמה מגירות תיבדקנה לכל היותר עד שהאדם יפסיק את החיפוש?
\end{exercise}
\begin{solution}
	\[
		\frac{1}{4} > \PP(A \mid B_k) = \frac{(10 - k) \frac{3}{4}}{10 - \frac{3k}{4}} \iff k > \frac{89}{11}
	\]
	נבדוק לכל היותר 8 מגירות.
\end{solution}

\subsection{ניסוי דו־שלבי על־ידי הסתברות מותנית}
\begin{proposition}
	נניח שנתון ניסוי דו־שלבי על $\Omega_1 \times \Omega_2$ עם פונקציית הסתברות נקודתית $p$ על $\Omega_1$ ולכל $\omega \in \Omega_1$ גם $p_\omega$ היא פונקציית הסתברות נקודתית על $\Omega_2$. \\*
	אם $\PP$ היא פונקציה על $\Omega_1 \times \Omega_2$ המקיימת
	\[
		\PP(\{a, x\}) = p(a),
		\qquad
		\PP(\{ x, b \} \mid \{ (a, x) \}) = p_a(b)
	\]
	אז $\PP$ היא פונקציית הסתברות יחידה המתאימה לניסוי הדו־שלבי.
\end{proposition}
\begin{proof}
	יהי $(a, b) \in \Omega_1 \times \Omega_2$, מכלל השרשרת נובע
	\[
		\PP(\{(a, b)\})
		= \PP(\{(a, x)\}) \cdot \PP(\{(x, b)\} \mid \{(a, x)\})
		= p(a) \cdot p_a(b)
		= q(a, b)
	\]
	עבור $q$ פונקציית הסתברות נקודתית של $\PP$.
\end{proof}
נבחין שוב כי בעוד כל ניסוי דו־שלבי, ניתן לבחון אותו כניסוי מותנה, הכיוון ההפוך לא בהכרח מתקיים; לא כל ניסוי מותנה הוא ניסוי דו־שלבי. \\*
נבחן דוגמות לשימוש בקשר זה.
\begin{exercise}
	בשוק ישנם שלושה סוגי מחשבים. חצי מסוג ראשון, 30\% מסוג שני ו־20\% מסוג שלישי. \\*
	הסיכוי שמחשב מסוג ראשון יתקלקל בשנתו הראשונה הוא עשירית, הסיכוי לסוג שני הוא חמישית והסיכוי למחשב מהסוג השלישי הוא $\frac{1}{20}$. \\*
	קונים מחשב באקראי מבין מחשבי השוק, מה ההסתברות שהוא יתקלקל בשנתו הראשונה?
\end{exercise}
\begin{solution}
	נסמן $C_j$ הוא המאורע שקנינו מחשב מסוג $j$ ו־$B$ המאורע שהמחשב התקלקל בשנתו הראשונה. \\*
	עוד נתון $\PP(C_1) = \frac{l}{2}, \PP(C_2) = \frac{3}{10}, \PP(C_3) = \frac{1}{5}$. \\*
	נתונים לנו גם $\PP(B \mid C_1) = \frac{1}{10}, \PP(B \mid C_2) = \frac{1}{5}, \PP(B \mid C_3) = \frac{1}{20}$.
	מנוסחת ההסתברות השלמה נובע
	\[
		\PP(B) = \PP(B \mid C_1) \PP(C_1) + \PP(B \mid C_2) \PP(C_2) + \PP(B \mid C_3) \PP(C_3)
	\]
\end{solution}
\begin{exercise}
	במבחן אמריקאי לכל שאלה 4 אפשרויות ובדיוק 1 נכונה.
	סטודנטית ניגשת למבחן עם האסטרטגיה הבאה:
	\begin{itemize}
		\item אם היא יודעת את התשובה היא עונה נכונה.
		\item אם היא לא יודעת את התשובה אז היא בוחרת תשובה אקראית.
	\end{itemize}
	נתון כי הסטודנטית יודעת את התשובה ל־90\% משאלות הבחינה. \\*
	בוחרים שאלה באקראי, ונתון שהסטודנטית ענתה עליה נכון, מה ההסתברות שהיא ידעה את התשובה.
\end{exercise}
\begin{solution}
	נסמן ב־$A$ את המאורע שהסטודנטית ידעה את התשובה, וב־$B$ את המאורע שהסטודנטית ענתה נכון. \\*
	אנו יודעים כי $\PP(A) = \frac{9}{10}$ וגם כי $\PP(B \mid A) = 1, \PP(B \mid A^C) = \frac{1}{4}$.
	\[
		\PP(A \mid B)
		= \frac{\PP(A)}{\PP(B)} \PP(B \mid A)
		= \frac{\frac{9}{10} \cdot 1}{\PP(B \mid A) \cdot \PP(A) + \PP(B \mid A^C) \cdot \PP(A^C)}
		= \frac{\frac{9}{10}}{\frac{9}{10} + \frac{1}{4} \cdot \frac{1}{10}}
		= \frac{\frac{9}{10}}{\frac{37}{40}}
		\approx 0.973
	\]
\end{solution}

\section{שיעור 6 --- 14.11.2024}

\subsection{אי־תלות}
\begin{definition}[מאורעות בלתי־תלויים]
	מאורעות $A, B$ המקיימים $\PP(A \cap B) = \PP(A) \PP(B)$ יקראו \textbf{בלתי־תלויים}.
\end{definition}
\begin{remark}
	נובע שמתקיים
	\[
		\PP(A \mid B) = \PP(A),
		\qquad
		\PP(B \mid A) = \PP(B)
	\]
\end{remark}
\begin{remark}[תזכורת]
	אם $A \subseteq \Omega_1$ ו־$B \subseteq \Omega_2$ ועובדים עם $\PP$ פונקציית הסתברות של מרחב המכפלה $\Omega_1 \times \Omega_2$. \\*
	אז ראינו שמתקיים $\PP(A \times B) = \PP_1(A) \cdot \PP_2(B) = \PP(A \times \Omega_2) \cdot \PP(\Omega_1 \times B)$
\end{remark}
\begin{example}
	מטילים שתי קוביות, אז $\Omega = {[6]}^2$. \\*
	נגדיר $A = \{ 4 \} \times [6]$ המאורע שיצא 4 בקוביה הראשונה ו־$B$ המאורע שסכום הקוביות הוא 7.
	\[
		\PP(A) = \frac{|A|}{|\Omega|} = \frac{1}{6},
		\qquad
		\PP(B) = \frac{|B|}{|\Omega|} = \frac{1}{6}
	\]
	וחישוב חיתוך המאורעות יניב
	\[
		\PP(A \cap B) = \frac{|A \cap B|}{|\Omega|} = \frac{|\{(4, 3)\}|}{36} = \frac{1}{36} = \PP(A) \PP(B)
	\]
	אז המאורעות בלתי־תלויים.
\end{example}
\begin{proposition}
	\begin{enumerate}
		\item לכל מאורע $A$, $A$ ו־$\emptyset$ בלתי־תלויים וכן $A$ ו־$\Omega$ בלתי־תלויים.
		\item אם $A$ ו־$B$ בלתי־תלויים ו־$\PP(B) > 0$ אז $\PP(A \mid B) = \PP(A)$.
		\item אם $A$ ו־$B$ בלתי־תלויים אז גם $A^C$ ו־$B$ בלתי תלויים.
	\end{enumerate}
\end{proposition}
\begin{proof}
	נוכיח את הטענה השלישית
	\[
		\PP(B \cap A^C)
		= \PP(B) - \PP(A \cap B)
		= \PP(B) - \PP(B) \PP(A)
		= \PP(B) (1 - \PP(A))
		= \PP(B) \PP(A^C)
	\]
	במעבר הראשון השתמשנו בנוסחת ההסתברות השלמה על החלוקה $A, A^C$.
\end{proof}
נראה הגדרה לאי־תלות במספר מאורעות, אך לא ההגדרה שאנו רוצים לעבוד איתה.
\begin{definition}[אי־תלות בזוגות]
	אם $A_1, \dots, A_n$ נקראים בלתי תלויים בזוגות אם
	\[
		\forall 1 \le i < j \le n, \PP(A_i \cap A_j) = \PP(A_i) \PP(A_j)
	\]
\end{definition}
\begin{definition}[קבוצה בלתי־תלויה]
	מאורע $A$ נקרא בלתי־תלוי בקבוצת המאורעות $B_1, \dots, B_n$ אם לכל $I \subseteq [n]$ מתקיים
	\[
		\PP(A \mid \bigcap_{i \in I} B_i) = \PP(A)
	\]
	דהינו $A$ ו־$\bigcap_{i \in I} B_i$ בלתי־תלוי.
\end{definition}
\begin{exercise}
	הביאו דוגמה למאורעות $A, B_1, B_2$ כך ש־$A$ ו־$B_1$ בלתי־תלויים וכך גם $A$ ו־$B_2$ אבל $A$ לא בלתי־תלוי בקבוצה $\{B_1, B_2\}$. \\*
	הראו כי אם $A, B_1$ בלתי־תלויים וגם $A, B_2$ בלתי־תלויים וגם $B_1, B_2$ זרים, אז $A$ בלתי־תלוי ב־$\{B_1, B_2\}$.
\end{exercise}
\begin{proposition}
	$A$ בלתי־תלוי ב־$\{B_1, \dots, B_n\}$ אם ורק אם $A$ בלתי תלוי ב־$\{B_1, \dots, B_n, B_1^C, \dots, B_n^C\}$.
\end{proposition}
\begin{proof}
	הכיוון השני הוא טריוויאלי, לכן נוכיח את הכיוון הראשון בלבד. \\*
	נראה ש־$A$ בלתי־תלוי בקבוצה ${\{B_1, \dots, B_n, B_1\}}^C$.
	נרצה להראות שלכל $I \subseteq [n + 1]$ מתקיים ש־$A$ ו־$\bigcap_{i \in I} B_i$ בלתי־תלויים. \\*
	אם $n + 1 \notin I$ אז לפי ההנחה חוסר התלות כבר מתקיים. \\*
	אחרת נגדיר $J = I \setminus \{ n + 1 \}$ ולכן $I = J \uplus \{ n + 1 \}$, ומכאן נובע
	\begin{align*}
		\PP((\bigcap_{i \in I} B_i) \cap A)
		& = \PP((\bigcap_{i \in J} B_i) \cap B_1^C \cap A) \\
		& = \PP(\bigcap_{i \in J} B_i \cap A) - \PP(\bigcap_{i \in J} B_i \cap B_1 \cap A) \\
		& = \PP(\bigcap_{i \in J} B_i) \PP(A) - \PP(\bigcap_{i \in J} B_i \cap B_1) \PP(A) \\
		& = \PP(\bigcap_{i \in J} B_i \cap B_1^C) \PP(A) \\
		& = \PP(\bigcap_{i \in I} B_i) \PP(A)
	\end{align*}
	ומצאנו כי ניתן להוסיף איבר, בשל כך נוכל לבצע את התהליך איטרטיבית ולקבל את המבוקש.
\end{proof}
\begin{definition}[אי־תלות קבוצת מאורעות]
	קבוצת מאורעות $\{A_1, \dots, A_n\}$ נקראת בלתי־תלויה אם לכל $I \subseteq [n]$ מתקיים
	\[
		\PP(\bigcap_{i \in I} A_i) = \prod_{i \in I} \PP(A_i)
	\]
\end{definition}
\begin{conclusion}
	אם $A_1, \dots, A_n$ בלתי־תלויים, אז גם כל תת־קבוצה של מאורעות היא בלתי־תלויה. \\*
	בפרט $A_1, \dots, A_n$ בלתי־תלויים גורר ש־$A_1, \dots, A_n$ בלתי־תלויים בזוגות.
\end{conclusion}
\begin{proposition}
	קבוצת מאורעות $\{A_1, \dots, A_n\}$ בלתי־תלויה אם ורק אם לכל $i \in [n]$ מתקיים ש־$A_i$ בלתי־תלויה ב־$\{A_1, \dots, A_n\} \setminus \{A_i\}$.
\end{proposition}
\begin{proof}
	לכיוון הראשון, נרצה להראות ש־$A_1$ לא תלוי ב־$\{A_2, \dots, A_n\}$, כלומר לכל $I \subseteq \{2, \dots, n\}$ רוצים להראות ש־$\PP(\bigcap_{i \in I} A_i \cap A_1) = \PP(\bigcap_{i \in I} A_i) \PP(A_1)$ על־ידי
	\[
		\PP(\bigcap_{i \in I} A_i \cap A_1)
		= (\prod_{i \in I} \PP(A_i)) \PP(A_1)
		= \PP(\bigcap_{i \in I} A_i) \PP(A_1)
	\]

	נעבור לכיוון השני.
	צריך להראות שלכל $I \subseteq [n]$ מתקיים $\PP(\bigcap_{i \in I} A_i) = \prod_{i \in I} \PP(A_i)$.
	תהי $I \subseteq [n] = \{ i_1, \dots, i_k \}$ כאשר $|I| = k$. \\*
	לפי ההנחה $A_{i_1}$ בלתי־תלוי ב־$\{ A_j \mid j \in [n] \setminus \{ i_1 \}\}$, לכן נקבל באינדוקציה
	\[
		\PP(\bigcap_{l = 1}^k A_{i_l})
		= \PP(A_{i_1} \cap (\bigcap_{l = 2}^k A_{i_l}))
		= \PP(A_{i_1}) \cdot \PP(\bigcap_{l = 2}^k A_{i_l})
		= \PP(A_{i_1}) \PP(A_{i_2}) \PP(\bigcap_{l = 3}^k A_{i_l})
		= \cdots = \PP(A_{i_1}) \cdots \PP(A_{i_k})
	\]
\end{proof}

\section{שיעור 7 --- 19.11.2024}

\subsection{אי־תלות}
נראה הגדרה שקולה לאי־תלות
\begin{definition}[שקולה לאי־תלות]
	$A_1, \dots, A_n$ בלתי־תלויים אם ורק אם
	\[
		\forall I \subseteq [n], \PP((\bigcap_{i \in I} A_i) \cap (\bigcap_{i \in [n] \setminus I} A_i^C))
		= \prod_{i \in I} \PP(A_i) \prod_{i \in [n] \setminus I} \PP(A_i^C)
	\]
\end{definition}
את השקילות של ההגדרות נראה בתרגיל. \\*
מאורעות $A_1, \dots, A_n$ בלתי־תלויים בהינתן $B$ אם הם בלתי־תלויים לפי פונקציית ההסתברות המותנית ב־$B$, $\PP_B$.
\begin{example}
	בוחרים מטבע באקראי משק ומטילים אותו $n$ פעמים. \\*
	$A_i$ יצא עץ בהטלה ה־$i$ בלתי־תלוי בהינתן בחירת המטבע, $B$ המאורע שנבחר מטבע מסוים.
\end{example}
נרצה לנסות לתת הגדרה חדשה עבור מקרים אינסופיים, נראה שיתקיים
\[
	\forall I \subseteq \NN, \PP(\bigcap_{i \in I} A_i = \prod_{i \in I} \PP(A_i))
\]
אבל היא לא מועילה לנו, נגדיר במקום זאת
\begin{definition}[קבוצה בת־מניה בלתי־תלויה]
	$A_1, A_2, \dots$ מאורעות הם בלתי־תלויים אם ורק אם \\*
	לכל קבוצה סופית $I$ מתקיים ${\{A_i\}}_{i \in I}$ קבוצה בלתי־תלויה.
\end{definition}
\begin{remark}[מכפלה אינסופית]
	נגדיר מכפלה אינסופית על־ידי
	\[
		\prod_{i \in \NN} a_i
		= \prod_{i = 1}^\infty a_i
		= \lim_{N \to \infty} \prod_{i = 1}^N a_i
	\]
\end{remark}
\begin{proposition}
	אם $A_1, A_2, \dots$ סדרת מאורעות בלתי־תלויים אז
	\[
		\PP(\bigcap_{i \in \NN} A_i) = \prod_{i \in \NN} \PP(A_i)
	\]
\end{proposition}
\begin{proof}
	נגדיר $B_n = \bigcap_{i = 1}^n A_i$ סדרה יורדת ולכן מרציפות פונקציית ההסתברות נובע
	\[
		\PP(\bigcap_{i \in \NN} A_i)
		= \PP(\bigcap_{n \in \NN} B_n)
		= \lim_{n \to \infty} \PP(B_n)
		= \lim_{N \to \infty} \prod_{i = 1}^N \PP(A_i)
		= \prod_{i = 1}^\infty \PP(A_i)
	\]
\end{proof}
\begin{example}
	אם $A_1, \dots$ בלתי־תלויים ו־$\PP(A_i) = p < 1$ אז $\PP(\bigcap_{i \in \NN} A_i) = 0$. \\*
	לדוגמה בהטלה אינסוף פעמים של מטבע הסיכוי שייצא עץ הוא אפס.
	דוגמה זו קצת בעייתית שכן כלל לא הראינו כי מרחב זה קיים ומוגדר, אבל המשמעות היא שעבור מרחבי מדגם הולכים וגדלים, אז ההסתברות המבוקשת שואפת להיות אפס.
\end{example}

\subsection{משתנים מקריים}
עד כה היינו צריכים לבצע ניתוח מלא של הסיטואציה כדי להגיע למסקנה, גם אם בהרבה מקרים שונים הגענו לבדיוק אותה המסקנה, המטרה של משתנים מקריים הוא לבודד את הרעיון הזה ולתקוף אותו.
\begin{definition}[משתנה מקרי]
	יהי $(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, פונצקיה מ־$\Omega$ ל־$\RR$ נקראת \textbf{משתנה מקרי}.
\end{definition}
\begin{notation}
	על־אף שזו פונקציה, נהוג לסמן משתנים מקריים בסימונים שאנו רגילים שמשמשים למשתנים, לדוגמה $X, Y, Z$.
\end{notation}
\begin{remark}
	השם קצת מטעה, אלו הם לא משתנים, ושווה לחשוב עליהם בתור מצבים מקריים יותר.
\end{remark}
\begin{example}
	נניח $\Omega = \{H, T\}$ הטלת מטבע, ונגדיר את הפונקציה $f : \Omega \to \RR$ על־ידי $f(H) = 2, f(T) = -3$,
	זאת במטרה לייצג שאם יוצא עץ נפסיד שלושה מטבעות ואם מתקבל פלי אז נקבל שני מטבעות.
\end{example}
\begin{example}
	נרצה להטיל שתי קוביות ונרצה לדבר על תוצאת אחת ההטלות, נתחיל ונגדיר $\Omega = {[6]}^2$. \\*
	נגדיר $X_1 : \Omega \to \RR$ על־ידי $X_1(a, b) = a$, ובאופן דומה נגדיר $X_2(a, b) = b$.
	יצרנו פונקציות שמהוות משתנה מקרי עבור ההטלה הראשונה ועבור ההטלה השנייה, נגדיר גם עבור הסכום, $Y(a, b) = a + b$. \\*
	נקבל עתה $Y = X_1 + X_2$, ונבחין בכוח האמיתי של הגדרה זו, יש לנו איזשהו קישור מורכב במרחב ההסתברות ללא עבודה ישירות מול המרחב.
\end{example}
\begin{definition}[משתנה מקרי מושרה ממאורע]
	אם $A$ מאורע אז נגדיר $1_A$ משתנה מקרי על־ידי
	\[
		1_A(\omega) = \begin{cases}
			1, & \omega \in A \\
			0, & \omega \notin A
		\end{cases}
	\]
\end{definition}
\begin{proposition}[תכונות של משתנים מקריים מושרים]
	\begin{enumerate}
		\item $1_{A^C} = 1 - 1_A$
		\item $1_{A \cap B} = 1_A \cdot 1_B$
		\item $1_{A \cup B} = \max\{1_A, 1_B\}$
	\end{enumerate}
\end{proposition}
\begin{example}
	$\Omega = S_n$, $A_i$ המאורע שיש $i$ נקודות שבת. \\*
	נסמן $X_i = 1_{A_i}$ ו־$X = \sum_{i = 1}^n X_i$.
\end{example}
בדוגמות הקודמות ההטלה הראשונה זוגית $\{ (a, b) \in {[6]}^2 \mid a \in \{2, 4, 6\}\}$. נכתוב במקום זאת $X_1 \in \{2, 4, 6\}$.
\begin{definition}[מאורע מושרה ממשתנה מקרי]
	אם $X$ משתנה מקרי ו־$S \in \mathcal{F}_\RR$, המאורע $X \in S$ מוגדר להיות
	\[
		X^{-1}(S) = \{ \omega \in \Omega \mid X(\omega) \in S \}
	\]
	בהתאם נכתוב $\PP(\{x \in S\})$ על־ידי $\PP(X \in S)$, ובאופן דומה נוכל לציין גם את $\PP(X = s), \PP(X \le s)$ ודומים.
\end{definition}
\begin{definition}[פונקציית הסתברות מושרית ממשתנה מקרי]
	$(\Omega, \mathcal{F}, \PP)$ מרחב הסתברות, ויהי $X$ משתנה מקרי. \\*
	נגדיר פונקציה $\PP_X : \mathcal{F}_\RR \to [0, \infty)$ על־ידי
	\[
		\PP_X(S) = \PP(X \in S) = \PP(\{\omega \in \Omega \mid X(\omega) \in S \})
	\]
	$\PP_X$ מכונה ה\textbf{ההתפלגות} של $X$. \\*
	אם $\PP_X$ נתמכת על $S$ (כלומר $\PP_X(S) = 1$) אז אומרים ש־$X$ נתמך על $S$.
\end{definition}
\begin{proposition}
	$\PP_X$ היא פונקציית הסתברות על $(\RR, \mathcal{F}_\RR)$.
\end{proposition}
\begin{proof}
	\[
		\forall S, \PP_X(S) = \PP(X \in S) \ge 0
	\]
	וכן
	\[
		\PP_X(\RR) = \PP(X \in \RR) = \PP(\Omega) = 1
	\]
	ולבסוף סיגמא־אדיטיביות:
	\begin{align*}
		\forall S_1, S_2, \dots, \PP_X(\biguplus_{n \in \NN} S_n)
		& = \PP(X \in \biguplus_{n \in \NN} S_n) \\
		& = \PP(\{\omega \in \Omega \mid X(\omega) \in \biguplus_{n \in \NN} S_n \}) \\
		& = \PP(\biguplus_{n \in \NN} \{ X \in S_n \}) \\
		& = \sum_{n \in \NN} \PP(X \in S_n) \\
		& = \sum_{n \in \NN} \PP_X(S_n)
	\end{align*}
\end{proof}

\section{תרגול 4 --- 21.11.2024}

\subsection{אי־תלות}
נניח מרחב הסתברות $(\Omega, \mathcal{F}, \PP)$.
\begin{exercise}
	בכד שלושה מטבעות, שניים הוגנים ואחד שמוטבע עץ על שני צדדיו. \\*
	שולפים מטבע באקראי ואז מטילים אותו פעמיים. \\*
	האם תוצאת ההטלה הראשונה תלויה בתוצאת ההטלה השנייה?
\end{exercise}
\begin{solution}
	נסמן ב־$A_i$ את המאורע שבהטלה ה־$i$ יצא עץ. \\*
	אנו שואלים אם $A_1, A_2$ הם תלויים, נסמן גם $F$ המאורע ששלפנו מטבע הוגן.
	\[
		\PP(A_1)
		= \PP(A_1 \mid F) \PP(F) + \PP(A_1 \mid F^C) \PP(F^C)
		= \frac{1}{2} \cdot \frac{2}{3} + 1 \cdot \frac{1}{3} = \frac{2}{3}
	\]
	אנו רוצים לבדוק את התלות ולכן נחשב
	\[
		\PP(A_1 \cap A_2)
		= \PP(A_1 \cap A_2 \mid F) \PP(F) + \PP(A_1 \cap A_2 \mid F^C) \PP(F^C)
		= \frac{1}{4} \cdot \frac{2}{3} + 1 \cdot \frac{1}{3} = \frac{1}{2} \ne \frac{4}{9} = \PP(A_1) \cdot \PP(A_2)
	\]
	ולכן הם תלויים.
\end{solution}
\begin{exercise}
	נגדיר $\Omega = \NN$ ו־$\PP(\{n\}) = \frac{1}{c \cdot n^2}$, כאשר $c = \sum_{n \in \NN} \frac{1}{n^2} = \frac{\pi^2}{6}$. \\*
	נגדיר $\forall k \in \NN, A_k = k \NN = \{ kn \mid n \in \NN \}$. \\*
	האם ${\{A_k\}}_{k \in \NN}$ היא תלויה?
\end{exercise}
\begin{solution}
	\[
		\PP(A_k) = \sum_{n \in \NN} \PP(\{k_n\})
		= \sum_{n \in \NN} \frac{1}{c k^2 n^2}
		= \frac{1}{c k^2} \sum_{n \in \NN} \frac{1}{n^2}
		= \frac{1}{k^2}
	\]
	ולכן
	\[
		\PP(A_2 \mid A_4) = 1 \ne \frac{1}{4} = \PP(A_2)
	\]
	ולכן המאורעות תלויים ובכלל הקבוצה לא בלתי־תלויה.
\end{solution}
\begin{exercise}
	נגדיר $P$ קבוצת המספרים הראשוניים, האם ${\{A_p\}}_{p \in P}$ בלתי־תלויה?
\end{exercise}
\begin{solution}
	יהיו $p_1, \dots, p_m \in P$ ראשוניים, אז מהמשפט היסודי של האריתמטיקה (או פירוק לגורמים ראשוניים)
	\[
		A_{p_1} \cap \cdots \cap A_{p_m} = A_{p_1 \cdots p_m}
	\]
	ולכן
	\[
		\PP(A_{p_1} \cap \cdots \cap A_{p_m}) = \PP(A_{p_1 \cdots p_m}) = \frac{1}{{(p_1 \cdots p_m)}^2}
		= \frac{1}{p_1^2} \cdots \frac{1}{p_m^2} = \PP(A_{p_1}) \cdots \PP(A_{p_m})
	\]
	נגדיר גם $B = \bigcap_{p \in P} A_p^C = \{ 1 \}$, ונחשב
	\[
		\frac{6}{\pi^2} = \frac{1}{c} = \PP(B) = \prod_{p \in P} (1 - \frac{1}{p^2})
	\]
\end{solution}
\begin{conclusion}
	נוכל להסיק מסקנה משמעותית נוספת, לכל $s > 1$ מתקיים
	\[
		\sum_{n = 1}^\infty \frac{1}{n^2} = \prod_{p \in P} {(1 - \frac{1}{ps})}^{-1} = \zeta(s)
	\]
	הערך ה־$s$ של פונקציית זטא של רימן, וזו זהות אוילר לפונקציית זטא.
\end{conclusion}
\begin{conclusion}
	מסקנה נוספת היא שבשל אי־רציונליות $\pi$ נוכל להסיק כי הטור הוא לא טור סופי, לכן יש אינסוף ראשוניים.
\end{conclusion}

\subsection{משתנים מקריים}
אנו רוצים להסתכל על משתנה מקרי כדרך להסתכל מחדש על מרחב ההסתברות ובפרט פונקציית ההסתברות באופן נוסף, זה בתורו יאפשר לנו לפתור בעיות בדרך חדשה ואולי אף פשוטה יותר, כפי שנראה בהמשך.
\begin{example}
	נגדיר $X$ להיות משתנה מקרי שמתאר סכום הטלת שתי קוביות הוגנות, דהינו נוכל להגדיר $\PP = {[6]}^2$ ו־$\PP$ אחידה. \\*
	בהתאם נגדיר $X : \Omega \to \RR$ על־ידי $(a, b) \mapsto a + b$. \\*
	לכן
	\[
		\rng(X) = \{2, \dots, 12 \}
	\]
	נעבור לחישוב הסתברויות
	\begin{align*}
		& \PP(X = 2) = \frac{1}{36}, \\
		& \PP(X = 3) = \frac{2}{36}, \\
		& \PP(X = 4) = \frac{3}{36}, \\
		& \PP(X = 5) = \frac{4}{36}, \\
		& \PP(X = 6) = \frac{5}{36}, \\
		& \PP(X = 7) = \frac{6}{36}, \\
		& \PP(X = 8) = \frac{5}{36}
	\end{align*}
	וכן הלאה, בהתאם נוכל להסיק
	\[
		\forall E \subseteq \RR,
		\PP(X \in E)
		= \PP_X(E \cap \rng(X))
		= \sum_{i \in E \cap \rng(X)} \PP(X = i)
	\]
	נסמן $X_i$ תוצאת ההטלה ה־$i$, ולכן $X = X_1 + X_2$, נחשב את $X$ ביחס ל־$X_i$.
	\begin{align*}
		\forall n \in \{2, \dots, 12\},
		\PP(X = n) 
		& = \sum_{i = 1}^{6} \PP(X_1 = i) \PP(X_2 = n - i) \\
		& = \sum_{i = 1}^{6} \frac{1}{6} \min\{6 - i, 0\} \\
		& = \frac{1}{36} |\{\{1, \dots, 6\} \cap \{n - 1, \dots, n - 6 \}\}|
	\end{align*}
	אם נגדיר $Y = X (\mod 6)$ אז $\rng(Y) = \{0, \dots, 5\}$ ונחשב
	\begin{align*}
		\forall n \in \{0, \dots, 5\},
		\PP(Y = n)
		& = \PP(X = n \lor X = n + 6 \lor X = n + 12) \\
		& = \frac{1}{36} \cdot |\{1, \dots, 6\} \cap \{n + 12, \dots, n - 6\}| \\
		& = \frac{6}{36} = \frac{1}{6}
	\end{align*}
\end{example}

\section{שיעור 8 --- 21.11.2024}

\subsection{משתנים מקריים --- המשך}
\begin{definition}[משתנה מקרי בדיד]\label{def_discrete_random_variable}
	$X$ משתנה מקרי נקרא \textbf{בדיד} אם $\PP_X$ הוא פונקציית הסתברות בדידה. \\*
	במקרה זה יש ל־$X$ התפלגות נקודתית $p_X : \RR \to [0, \infty)$.
\end{definition}
\begin{remark}
	נבחין כי גם אם מרחב ההסתברות הוא לא בדיד, נוכל להגדיר משתנה מקרי בדיד עליו.
\end{remark}
\begin{example}
	נגדיר $A \in \mathcal{F}$ ו־$X = 1^A$ ונניח $\PP(A) = p$. \\*
	אם $S \subseteq \mathcal{F}_\RR$ אז אם $\{0, 1 \} \in S$ אז $\Omega = X^{-1}(S)$ ואז $\PP_X(S) = \PP(\Omega) = 1$. \\*
	אם $1 \in S$ אבל $0 \notin S$ אז $A = X^{-1}(S)$ ואז $\PP_X(S) = \PP(A) = p$. \\*
	לבסוף אם $0 \in S$ ו־$1 \notin S$ אז $A^C = X^{-1}(S)$ אז $\emptyset = X^{-1}(S)$ ואז $\PP_X(S) = \PP(\emptyset) = 0$.

	אם נגדיר $p_X : \RR \to [0, \infty)$ על־ידי
	\[
		p_X(s) = \begin{cases}
			p & s = 1 \\
			1 - p & s = 0 \\
			0 & \text{else}
		\end{cases}
	\]
	אז מתקיים
	\[
		\PP_X(S) = \sum_{s \in S} p_X(s)
	\]
\end{example}
\begin{definition}[התפלגות ברנולי]
	משתנה מקרי $X$ מתפלג ברנולי עם פרמטר $p$ אם יש לו פונקציית התפלגות נקודתית
	\[
		p_X(s) = \begin{cases}
			p & s = 1 \\
			1 - p & s = 0 \\
			0 & \text{else}
		\end{cases}
	\]
	במקרה זה נסמן $X \sim \text{Ber}(p)$, סימון לא מאוד מועיל או מתכתב עם השימוש הסטנדרטי, אבל אלה הם החיים.
\end{definition}
נשאל את עצמנו את השאלה האם כל משתנה מקרי מתפלג ברנולי הוא מציין של מאורע.
אילו נגדיר $A = X^{-1}(1)$ אז מתקבל $X = 1_A$, אנו אומרים ש־$X$ שווה למציין של $1_A$ כמעט תמיד, נראה זאת בהמשך הפרק. \\*
נמשיך לעוד מקרים.
\begin{definition}[משתנה מקרי קבוע]
	משתנה מקרי $X$ הוא קבוע אם
	\[
		p_X(s) = \begin{cases}
			1 & s = c \\
			0 & \text{else}
		\end{cases}
	\]
	עבור $c$ קבוע כלשהו.
\end{definition}
\begin{definition}[משתנה מקרי אחיד]
	משתנה מקרי $X$ נקרא אחיד על $S$ תת־קבוצה סופית של $\RR$ אם
	\[
		p_X(s) = \begin{cases}
			\frac{1}{|S|} & s \in S \\
			0 & \text{else}
		\end{cases}
	\]
	במקרה זה נסמן $X \sim U(S)$.
\end{definition}
\begin{definition}[התפלגות גאומטרית]\label{geometric_distribution}
	$X$ מתפלג גאומטרית עם פרמטר $p$ אם
	\[
		p_X(s) = \begin{cases}
			{(1 - p)}^{s - 1} p & s \in \{1, 2, \dots \} \\
			0 & \text{else}
		\end{cases}
	\]
	ונסמן $X \sim \text{Geo}(p)$. \\*
	לפעמים הגדרה זו תסומן אחרת על־ידי מדידת המקרים שבהם יצאה ההסתברות למאורע הראשון בלבד. \\*
	התפלגות זו מתארת את המקרה שניסינו לקבל תוצאה בהסתברות בין שני מקרים וקיבלנו אותה בפעם ה־$s$.
\end{definition}
\begin{definition}[התפלגות בינומית]\label{binominal_distribution}
	$X$ מתפלג בינומית עם פרמטרים $n$ ו־$p$ אם
	\[
		p_X(s) = \begin{cases}
			\binom{n}{s} p^s {(1 - p)}^{n - s} & s \in \{1, 2, \dots\} \\
			0 & \text{else}
		\end{cases}
	\]
	ונסמן $X \sim \text{Bin}(n, p)$.
\end{definition}
מאפשר לנו לחשב את מספר המטבעות המוטים שיצאו על צד מסוים.
ולבסוף
\begin{definition}[התפלגות פואסונית]\label{poasson_distribution}
	$X$ מתפלג פואסונית עם פרמטר $\lambda$ אם
	\[
		p_X(s) = \begin{cases}
			e^{-\lambda} \frac{\lambda^s}{s!} & s \in \{0, 1, 2, \dots\} \\
			0 & \text{else}
		\end{cases}
	\]
	ונסמן $X \sim \text{Po}(\lambda)$.
\end{definition}
בפעם הראשונה ההתפלגות הזו הופיעה בהקשר של מספר החיילים שנהרגו מבעיטה מהסוס שלהם, התפלגות שהייתה מהותית עד מלחמת העולם הראשונה.

\subsection{קשרים בין משתנים־מקריים}
\begin{example}
	$\Omega = {[6]}^2$ מרחב אחיד להטלת שתי קוביות, ונגדיר שוב $Y = X_1 + X_2$ סכום הטלות שתי הקוביות, דהינו
	\[
		X_1(a, b) = a,
		X_2(a, b) = b,
		Y(a, b) = a + b
	\]
	בתרגול מצאנו את הערכים של $p_Y$ לכל ערך אפשרי. \\*
	נגדיר גם $Z = Y \mod 6$ (ומנוחות נגדיר $Z \in [6]$), ונשאל מה ההתפלגות של $Z$.
	\[
		p_Z(1) = \PP(Z = 1) = \PP(Y = 7) = \frac{1}{6}
	\]
	באופן דומה
	\[
		p_Z(2) = \PP(Z = 2) = \PP(Y = 2) + \PP(Y = 8) = \frac{1}{36} + \frac{5}{36} = \frac{1}{6}
	\]
	באופן כללי מתקיים מחישוב כזה ש־$p_Z(n) = \frac{1}{6}$ לכל $n \in [6]$, נסיק כי $Z \sim U([6])$.
\end{example}
\begin{definition}[הסתברות כמעט תמיד]
	אם $A$ מאורע עם הסתברות $1$ אז אומרים שהוא מתרחש כמעט תמיד.
\end{definition}
\begin{definition}[משתנים שווים שמעט תמיד]
	אם $X$ ו־$Y$ המקיימים ש־$X = Y$ כמעט תמיד אז נסמן $X \overset{a.s.}{=} Y$. \\*
	זה כמובן שקול להגדרה כי $\PP(\{ \omega \in \Omega \mid X(\omega) = Y(\omega) \}) = 1$ וזה נכון אם ורק אם $\PP(\{\omega \in \Omega \mid X(\omega) \ne Y(\omega) \}) = 0$.
\end{definition}
\begin{exercise}
	הוכיחו כי אם $X \overset{a.s.}{=} Y$ וגם $Y \overset{a.s.}{=} Z$ אז גם $X \overset{a.s.}{=} Z$, דהינו זהו יחס טרנזיטיבי.
\end{exercise}
\begin{proof}
	נשים לב לעובדה הבאה, אם $X(\omega) = Y(\omega)$ ו־$Y(\omega) = Z(\omega)$ עבור $\omega \in \Omega$ כלשהו, אז $X(\omega) = Z(\omega)$,
	כלומר
	\[
		\{ \omega \in \Omega \mid X(\omega) = Y(\omega) \} \cap \{ \omega \in \Omega \mid Y(\omega) = Z(\omega) \} \subseteq \{ \omega \in \Omega \mid X(\omega) = Z(\omega) \}
	\]
	ובהתאם גם
	\[
		\{ \omega \in \Omega \mid X(\omega) \ne Y(\omega) \} \cup \{ \omega \in \Omega \mid Y(\omega) \ne Z(\omega) \} \supseteq \{ \omega \in \Omega \mid X(\omega) \ne Z(\omega) \}
	\]
	אז מחסם האיחוד נקבל
	\[
		0 \le \{ \omega \in \Omega \mid X(\omega) \ne Z(\omega) \} \le 0 + 0
	\]
\end{proof}
\begin{proposition}
	$\overset{a.s.}{=}$ הוא יחס שקילות על מרחב כל המשתנים־המקריים על $\Omega$.
\end{proposition}
\begin{proof}
	ראינו עתה טרנזיטיביות, וסימטריה ורפלקסיביות נובעות ישירות מההגדרה.
\end{proof}
\begin{exercise}
	האם בדוגמה קודם מתקיים $X_1 \overset{a.s.}{=} X_2$?
\end{exercise}
\begin{solution}
	מחישוב מתקיים $\PP(X_1 = X_2) = \frac{1}{6}$ ולכן התשובה היא שלא. \\*
	נבחין כי גם $\PP(X_1 \ne Z) \ge \PP(X_1 = 2, Z = 3) = \PP(\{(2, 1)\}) = \frac{1}{36}$.
\end{solution}
באופן יותר כללי גם אם יש מאורעות שיש להם אותה ההסתברות, אין הכרח שיהיה קשר לשוויון שלהם כמעט תמיד.
\begin{definition}[משתנים  מקריים שווי התפלגות]
	אם למשתנים מקריים $X, Y$ יש אותה פונקציית התפלגות, דהינו $\PP_Y = \PP_X$, \\*
	דהינו מתקיים $\forall S \in \mathcal{F}_\RR, \PP_X(S) = \PP_Y(S) \iff \PP(X \in S) = \PP(Y \in S) \iff \PP(X^{-1}(S)) = \PP(Y^{-1}(S))$, \\*
	אז נאמר שהם שווי התפלגות ונסמן $X \overset{d}{=} Y$.
\end{definition}
ראינו שיש משתנים מקריים $X$ ו־$Y$ כך ש־$X \overset{a.s.}{\ne} Y$ אבל $X \overset{d}{=} Y$,
האם $X \overset{a.s.}{=} Y$ גורר $X \overset{d}{=} Y$?
התשובה היא שכן!
\begin{proposition}
	אם $X \overset{a.s.}{=} Y$ אז גם $X \overset{d}{=}$.
\end{proposition}
\begin{proof}
	נניח ש־$X \overset{a.s.}{=} Y$ ונרצה להוכיח ש־$\forall S \in \mathcal{F}_\RR, \PP(X \in S) = \PP(Y \in S)$. \\*
	לכל $S \in \mathcal{F}_\RR$ מתקיים
	\[
		0 \ne \PP(X \ne Y) \ge \PP(X \in S, Y \notin S) = 0
	\]
	ובהתאם
	\[
		\PP(X \in S) = \PP(X \in S, Y \in S) + \overbrace{\PP(X \in S, Y \notin S)}^{= 0} = \PP(X \in S, Y \in S)
	\]
	כמו־כן גם $\PP(Y \in S) = \PP(X \in S, Y \in S)$.
\end{proof}

\section{שיעור 9 --- 25.11.2024}

\subsection{וקטורים מקריים}
ניזכר בהגדרה\ \ref{def_discrete_random_variable}:
\begin{definition*}[משתנה מקרי בדיד]
	משתנה מקרי נקרא בדיד אם $\PP_X$ פונקציית הסתברות בדידה, כלומר
	\[
		\forall S \in \mathcal{F}_X, \PP_X(S) = \sum_{s \in S} p_X(s)
	\]
	כאשר $p_X(s) = \PP(X = s) = \PP(X^{-1}(s)) = \PP(\{\omega \in \Omega \mid X(\omega) = s \})$.
\end{definition*}
גם דיברנו על סוגים שונים של התפלגות, לדוגמה
\[
	\forall i \in [6], p_X(i) = \frac{1}{6} \iff X \sim U([6])
\]
או באופן דומה
\[
	\forall k \in \{0, \dots, n\}, p_X(k) = \binom{n}{k} p^k {(1 - p)}^{n - k}
\]
$A$ מתרחש כמעט תמיד אם $\PP(A) = 1$, לכן אם $\PP(X = Y) = 1$ אז נאמר ש־$X = Y$ כמעט תמיד, או נסמן $X \overset{a.s.}{=} Y$. \\*
באופן דומה אם $\PP_X = \PP_Y$ אז נסמן $X \overset{d}{=} Y$ או נאמר ש־$X$ ו־$Y$ שווי התפלגות, וראינו קשר בין שתי ההגדרות.
\begin{example}
	נגדיר הטלת מטבע, $\Omega = \{H, T\}$ ו־$X = 1_{\{H\}}, Y = 1_{\{T\}}$, אז $X \overset{d}{=} Y$ שכן
	\[
		p_X(s) = p_Y(s) = \begin{cases}
			\frac{1}{2} & s = 0 \\
			\frac{1}{2} & s = 1 \\
			0 & \text{else}
		\end{cases}
	\]
	אבל גם $\PP(X = Y) = 0$ ולכן $X \overset{a.s.}{\ne} Y$.
\end{example}
\begin{proposition}
	אם $X \overset{d}{=} Y$ ו־$f \in \mathcal{F}_{\RR \to \RR}$ אז $f(X) \overset{d}{=} f(Y)$.
\end{proposition}
\begin{proof}
	נגדיר $W = f(Y), Z = f(X)$,
	צריך להוכיח ש־$\forall S \in \mathcal{F}_\RR, \PP_Z(S) = \PP_W(S)$.
	\begin{align*}
		\PP_Z(S)
		& = \PP(Z \in S) \\
		& = \PP(\{\omega \in \omega \mid Z(\omega) \in S\}) \\
		& = \PP(\{\omega \in \omega \mid f(X(\omega)) \in S\}) \\
		& = \PP(\{\omega \in \omega \mid X(\omega) \in f^{-1}(S)\}) \\
		& = \PP(X \in f^{-1}(S)) \\
		& = \PP_X(f^{-1}(S)) \\
		& = \PP_Y(f^{-1}(S)) \\
		& = \PP(\{\omega \in \omega \mid Y(\omega) \in f^{-1}(S)\}) \\
		& = \PP(\{\omega \in \omega \mid f(Y(\omega)) \in S\}) \\
		& = \PP(W \in S) \\
		& = \PP_W(S)
	\end{align*}
\end{proof}
\begin{example}
	נניח ש־$X \sim Ber(\frac{1}{2})$ וגם $Y \sim Ber(\frac{1}{2})$, ונרצה לחשב את $\PP(X = Y)$. \\*
	אין לנו את היכולת לעשות זאת כי אין מספיק מידע.
\end{example}
\begin{definition}[וקטור מקרי]
	וקטור מקרי הוא משתנה מקרי לתוך $\RR^n$ במקום ל־$\RR$, $X : \Omega \to \RR^n$.
\end{definition}
כלל ההגדרות נשארות זהות פרט להגדרה זו, לדוגמה $\PP_X(S) = \PP(X \in S)$. \\*
המוטיב שלנו הוא היכולת לבנות כמה משתנים מקריים ולעבוד איתם כיציר בודד, לדוגמה $X = (X_1, X_2)$ עבור $X_1, X_2 : \Omega \to \RR$ משתנים מקריים. \\*
\begin{definition}[התפלגות משותפת והתפלגויות שוליות]
	אם $X_1, \dots, X_n$ משתנים מקריים המוגדרים על $\Omega$ יחיד אז $X = (X_1, \dots, X_n)$ הוא וקטור מקרי המוגדר על $\Omega$ וההתפלגות שלו נקראת ההתפלגות המשותפת של $X_1, \dots, X_n$. \\*
	ההתפלגויות של כל אחד מ־$X_1, \dots, X_n$ נקראות ההתפלגויות השוליות.
\end{definition}
השם הזה נובע מהגישה שבה נוכל להבין את ההסתברות של משתנה מקרי בודד מתוך הווקטור על־ידי, אם $X_1, X_2$ מרחבים מקריים אז
\[
	\PP_{X_1}(S) = \PP_{(X_1, X_2)}(S \times \RR) = \PP(\{\omega \in \in \Omega \mid (X_1(\omega), X_2(\omega)) \in S\})
\]
\begin{example}
	אם $\Omega = {[6]}^2$ ו־$X_1(a, b) = a, X_2(a, b) = b$ אז $X = (X_1, X_2)$ כאשר $X : \Omega \to \RR^2$ פונקציית הזהות.
\end{example}
\begin{example}
	נבחן עבור $E = \{(s, y) \in \RR^2 \mid s \le t\}$ את
	\[
		\PP_{(X, Y)}(E)
		= \PP(X \le Y)
	\]
\end{example}
\begin{definition}[התפלגות משותפת בדידה]
	אם לווקטור המקרי התפלגות בדידה, כלומר $\PP_{(X_1, \dots, X_n)}$ פונקציית הסתברות בדידה, \\*
	אז נאמר שההתפלגות המשותפת של $X_1, \dots, X_n$ בדידה.
\end{definition}
\begin{proposition}
	ההתפלגות המשותפת של $X_1, \dots, X_n$ בדידה אם ורק אם ההתפלגות של כל אחד מ־$X_1, \dots, X_n$ בדידה.
\end{proposition}
\begin{proof}
	נוכיח את הכיוון הראשון. \\*
	נניח $\PP_{X_1, \dots, X_n}$ בדידה, אך זה נכון אם ורק אם היא נתמכת על־ידי קבוצה בת־מניה, נבחר קבוצה $S \in \mathcal{F}_\RR$ כזו. \\*
	נסמן ב־$S_1$ את ההטלה של $S$ על הקורדינטה הראשונה, לכן $\PP_{X_1}(S_1) = \PP_{(X_1, X_2)}(S_1 \times \RR)$ אבל $S \subseteq S_1 \times \RR$. \\*
	לכן $X_1$ נתמך על־ידי קבוצה בת־מניה, $S_1$, ולכן הוא בדיד.

	נעבור לכיוון השני. \\*
	נניח ש־$X_1, X_2$ בדידים, לכן קיימות $S_1, S_2 \in \mathcal{F}_\RR$ בנות־מניה, \\*
	כך ש־$\PP(X_1 \in S_1) = \PP(X_2 \in S_2) = 1$. \\*
	לכן $\PP((X_1, X_2) \in S_1 \times S_2) = \PP(X_1 \in S_2, X_2 \in S_2) = 1$. \\*
	$S_1, S_2$ בנות־מניה ולכן נובע ש־$S_1 \times S_2$ בת־מניה.

	כמובן לווקטורים בגודל $n > 2$ ההוכחה דומה.
\end{proof}

\section{תרגול 5 --- 28.11.2024}

\subsection{משתנים מקריים}
בהרצאה זו נניח שכל המשתנים המקריים הם בדידים.
\begin{definition}[התניה במשתנים מקריים בדידים]
	אם $X : \Omega \to \RR^d$ ו־$A \subseteq \Omega$ כך ש־$\PP(A) > 0$ אז
	\[
		\forall S \subseteq \RR^d, \PP_{X \mid A}(S) = \PP(X \in S \mid A) = \PP_A(X \in S)
	\]
\end{definition}
\begin{definition}[אי־תלות במשתנים מקריים בדידים]
	אם $X, Y : \Omega \to \RR^d$ בלתי־תלויים אם לכל $S, T \subseteq \RR^d$ מתקיים
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \cdot \PP(Y \in T)
	\]
\end{definition}
\begin{exercise}
	יהיו $X_1, X_2 Geo(p)$ בלתי־תלויים ונגדיר גם $Z = X_1 + X_2$.
	\begin{enumerate}
		\item חשבו את ההתפלגות המשותפת של $X_1$ ו־$Z$.
		\item הראו ש־$X_1 \mid \{ Z = 1 \}$ מתפלג אחיד על $\{1, \dots, n - 1\}$.
	\end{enumerate}
\end{exercise}
\begin{solution}
	תחילה ניזכר שאם $W \sim Geo(p)$ אז $\supp(W) = \NN$ ו־$\PP(W = k) = {(1 - p)}^{k - 1} p$, שכן זוהי ההסתברות שלא הצלחנו $k - 1$ פעמים ובניסיון ה־$k$ הצלחנו, עבור איזושהי פעולה.
	\begin{enumerate}
		\item אנו מגדירים $X = (X_1, Z)$ וקטור מקרי ואנו רוצים לחשב את ההתפלגות שלו, נחשב את התומך
			\[
				\supp(X_1, Z) \subseteq \NN^2
			\]
			ישירות מההגדרה של הווקטור, אבל אנו יודעים כי תמיד $X_1 < Z$, וכן גם אם $m < n$
			\begin{align*}
				P_{(X_1, Z)}(m, n)
				& = \PP(X_1 = m, Z = n) \\
				& = \PP(X_1 = m, X_2 = n - m) \\
				& = \PP(X_1 = m) \cdot \PP(X_2 = m - n) \\
				& = {(1 - p)}^{m - 1} p {(1 - p)}^{n - m - 1} p \\
				& = p^2 {(1 - p)}^{n - 2}
			\end{align*}
			ולכן נסיק
			\[
				P_{(X_1, Z)}(n, n) = \begin{cases}
					0 & m \ge n \\
					p^2 {(1 - p)}^{n - 2} & m < n
				\end{cases}
			\]
		\item נבחן את $X_1 \mid \{ Z = n\}$ ונבין מה התומך.
			\[
				\supp(X_1 \mid \{Z = 1\})
				= \{1, \dots, n - 1\}
			\]
			שכן $Z$ מייצג סכום ולכן מהווה חסם ל־$X_1$ יחד עם $\supp(X_1) = \NN$. נעבור לחישוב ההתפלגות
			\[
				\PP(X = m \mid Z = n)
				= \frac{\PP(X = m, Z = n)}{\PP(Z = n)}
				= \frac{p^2 {(1 - p)}^{n - 2}}{\PP(Z = n)}
			\]
			אבל
			\[
				\PP(Z = n)
				= \PP(X_1 + X_2 = n)
				= \sum_{i = 1}^{n - 1}  \PP(X_1 = i, X_2 = n - i)
				= \sum_{i = 1}^{n - 1}  p^2 {(1 - p)}^{n - 2}
				= (n - 1) p^2 {(1 - p)}^{n - 2}
			\]
			זוהי קונבולוציה, לכן נוכל להסיק
			\[
				\PP(X_1 = m \mid Z = n)
				= \frac{p^2 {(1 - p)}^{n - 2}}{(n - 1) p^2 {(1 - p)}^{n - 2}}
				= \frac{1}{n - 1}
			\]
	\end{enumerate}
\end{solution}
\begin{exercise}
	מטילים מטבע הוגן, אם יצא 0 מטילים שוב מטבע הוגן ואם יצא 1 מטילים מטבע מוטה עם הסתברות $p$ ל־1. \\*
	נתחו את התפלגות ההטלה השנייה.
\end{exercise}
\begin{solution}
	נסמן $X$ תוצאת ההטלה הראשונה ו־$Y$ תוצאת ההטלה השנייה. \\*
	מהנתונים נסיק כי $X \sim Ber(\frac{1}{2})$. אנו גם יודעים כי גם $Y$ הוא בהתפלגות ברנולי כלשהי. \\*
	לבסוף אנו גם יודעים שמתקיים $Y \mid \{X = 0\} \sim Ber(\frac{1}{2})$ וגם ש־$Y \mid \{X = 1\} \sim Ber(p)$, לכן
	\[
		\PP(Y = 1)
		= \PP(Y = 1 \mid X = 0) \PP(X = 0) + \PP(Y = 1 \mid X = 1) \PP(X = 1)
		= \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot p
		= \frac{1}{4} + \frac{p}{2}
	\]
\end{solution}
\begin{exercise}
	יהיו $X \sim Ber(p)$ ו־$Y \sim Ber(q)$ בלתי־תלויים. \\*
	חשבו את ההתפלגות של $X \cdot Y$.
\end{exercise}
\begin{solution}
	נתחיל ונראה כי
	\[
		\supp(X Y) = \{0, 1\},
	\]
	וכן גם $XY$ בהתפלגות ברנולי כלשהי, אך
	\[
		\PP(XY = 1)
		= \PP(X = 1, Y = 1)
		= \PP(X = 1) \PP(Y = 1)
		= pq
	\]
	ולכן $XY \sim Ber(pq)$.
\end{solution}

\section{שיעור 10 --- 28.11.2024}
\subsection{התפלגות תחת התניה}
\begin{definition}[התפלגות משתנה מקרי בהינתן מאורע]
	יהי $X$ משתנה מקרי ו־$A$ מאורע כך ש־$\PP(A) > 0$ אז אפשר לדבר על התפלגות $X$ בהינתן $A$.
	זוהי ההתפלגות של $X$ תחת $\PP_A$ במקום $\PP$.
	במקרה זה
	\[
		\PP_{X \mid A}(S) = \PP_A(X \in S) = \PP(\{X \in S\} \mid A)
	\]
\end{definition}
\begin{proposition}
	אם $X \overset{d}{=} Y$ ו־$S \in \mathcal{F}_\RR$ כך ש־$\PP(X \in S) > 0$ וכן $\PP(Y \in S) > 0$, \\*
	אז $X \mid X \in S \overset{d}{=} Y \mid Y \in S$.
\end{proposition}
\begin{example}
	נניח ש־$X, Y \sim U([6])$ ו־$S = [3, \infty)$ אז
	\[
		X \mid X \in S \sim U(\{3, 4, 5, 6\}),
		\qquad
		Y \mid Y \in S \sim U(\{3, 4, 5, 6\})
	\]
\end{example}
\begin{definition}[אי־תלות משתנים מקריים]
	$X$ ו־$Y$ נקראים בלתי־תלויים אם לכל $S, T \in \mathcal{F}_\RR$ המאורעות $X \in S, Y \in T$ בלתי־תלויים. \\*
	הגדרה זו שקולה להגדרה שמתקיים
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \cdot \PP(Y \in T)
	\]
\end{definition}
\begin{proposition}
	אם $X$ ו־$Y$ משתנים מקריים בדידים אז $X$ ו־$Y$ בלתי־תלויים אם ורק אם לכל $s, t \in \RR$ מתקיים ש־$X = s$ ו־$Y = t$ בלתי־תלויים. \\*
	טענה זו שקולה לטענה שמתקיים
	\[
		\PP(X = s, Y = t) = \PP(X = s) \cdot \PP(Y = t)
	\]
\end{proposition}
\begin{proof}
	הכיוון הראשון הוא טריוויאלי מבחירת יחידונים ושימשו בהגדרה, לכן נניח את הכיוון השני ונראה כי מתקיים לכל $S, T \in \mathcal{F}_\RR$ מתקיים
	\[
		\PP(X \in S, Y \in T) = \PP(X \in S) \PP(Y \in T)
	\]
	נבחין כי
	\begin{align*}
		\PP(X \in S, Y \in T)
		& = \PP(X \in S \cap \supp(X), Y \in T \cap \supp(Y)) \\
		& = \sum_{\substack{s \in S \cap \supp(X) \\ t \in T \cap \supp(Y)}} \PP(X = s, Y = t) \\
		& = \sum_{\substack{s \in S \cap \supp(X) \\ t \in T \cap \supp(Y)}} \PP(X = s) \PP(Y = t) \\
		& = \sum_{s \in S \cap \supp(X)} \left( \sum_{t \in T \cap \supp(Y)} \PP(X = s) \PP(Y = t) \right) \\
		& = \left( \sum_{s \in S \cap \supp(X)} \PP(X = s) \right) \left( \sum_{t \in T \cap \supp(Y)} \PP(Y = t) \right) \\
		& = \PP(X \in S) \PP(Y \in T)
	\end{align*}
\end{proof}
\begin{proposition}
	התפלגות $X$ ו־$X + Y$ ו־$Y$ בלתי־תלויים קובע ביחידות את ההתפלגות המשותפת.
\end{proposition}
\begin{proof}[הוכחה עבור בדידים]
	$p_X$ ו־$p_Y$ בלתי־תלויים קובע את $p_{(X, Y)}(s, t) = p_X(s) p_Y(t)$.
\end{proof}
\begin{proposition}
	$X, Y, Z$ משתנים מקריים בדידים ונניח שלכל $s \in \supp(X)$ מתקיים $Y \mid X = s \overset{d}{=} Z$,
	אז $Y \overset{d}{=} Z$ ו־$X, Y$ בלתי־תלויים.
\end{proposition}
\begin{proof}
	מנוסחת ההסתברות השלמה נובע
	\begin{align*}
		\PP(Y = t)
		& = \sum_{s \in \supp(X)} \PP(X = s) \PP(Y = t \mid X = s) \\
		& = \sum_{s \in \supp(X)} \PP(X = s) \PP(Z = t) \\
		& = \PP(Z = t)
	\end{align*}
	עבור החלק השני נבחין כי
	\[
		\PP(X = s, Y = t)
		= \PP(X = s) \PP(Y = t \mid X = s)
		= \PP(X = s) \PP(Z = t)
		= \PP(X = s) \PP(Y = t)
	\]
\end{proof}
\begin{proposition}
	אם $X, Y$ משתנים מקריים בלתי־תלויים ו־$f, g \in \mathcal{F}_{\RR \to \RR}$ אז $f(X), g(Y)$ בלתי־תלויים.
\end{proposition}
\begin{proof}
	צריך להראות שלכל $S, T \in \mathcal{F}_{\RR}$ מתקיים
	\[
		\PP(f(X) \in S, g(Y) \in T) = \PP(f(X) \in S) \PP(g(Y) \in T)
	\]
	אבל ראינו כבר כי
	\[
		\PP(f(X) \in S, g(Y) \in T) = \PP(X \in f^{-1}(S), Y \in g^{-1}(T))
	\]
	אבל גם
	\[
		\PP(X \in f^{-1}(S), Y \in g^{-1}(T)) = \PP(f(X) \in S) \PP(g(Y) \in T)
	\]
\end{proof}
\begin{example}
	$X$ ו־$Y$ בלתי־תלויים אז $X^2, \frac{1}{Y}$ בלתי־תלויים, זאת שכן $\PP(X = 1, Y = 1) = \frac{1}{2} \ne \frac{1}{2} \cdot \frac{1}{2}$. \\*
	בכיוון ההפוך אם $X = Y \sim Ber(\frac{1}{2})$ לא בלתי־תלויים אבל אם $f(x) = g(y) = 6$ גוררים ש־$f(X)$ ו־$g(Y)$ הם כמעט תמיד 6 ולכן בלתי־תלויים.
\end{example}
\begin{definition}[קבוצת משתנים מקריים בלתי־תלויה]
	יהיו $X_1, \dots, X_n$ משתנים מקריים, אז הם יקראו בלתי־תלויים אם לכל $S_1, \dots, S_n \in \mathcal{F}_\RR$ המאורעות ${\{ X_i \in S_i \}}_{i \in [n]}$ הם בלתי־תלויים.
\end{definition}
\begin{example}
	אם $X, Y, Z$ הם בלתי־תלויים, האם גם $X + Y$ ו־$Z$ בלתי־תלויים?
	אנו צריכים להראות ש־$\PP(X + Y = s, Z = t) = \PP(X + Y = s) \PP(Z = t)$ עבור כל $s \in \{0, 1, 2\}, t \in \{0, 1\}$. \\*
	נבחר לדוגמה את $\PP(X + Y = 1, Z = 1) = \PP(X = 0, Y = 1, Z = 1) + \PP(X = 1, Y = 0, Z = 1) = \frac{1}{8} + \frac{1}{8}$ ונוכל להמשיך כך ולראות שהטענה אכן מתקיימת.
\end{example}
\begin{exercise}
	מאורעות $A_1, \dots, A_n$ הם בלתי־תלויים אם ורק אם $1_{A_1}, \dots, 1_{A_n}$ הם בלתי־תלויים.
\end{exercise}
\begin{proposition}
	$X_1, \dots, X_n$ משתנים מקריים בלתי־תלויים ונניח שיש אינדקסים $1 = i_0 < i_1 < \cdots < i_k = n$. \\*
	נגדיר $Y_0 = (X_{i_0}, \dots, X_{i - 1}), \dots. Y_k = (X_{i_{k - 1}}, \dots, X_{i_k})$.
\end{proposition}

\section{שיעור 11 --- 3.12.2024}
\subsection{אי־תלות משתנים מקריים}
נמשיך עם מהלך ההרצאה הקודמת.
\begin{proposition}
	יהיו $X_1, \dots, X_n$ משתנים מקריים (יכולים להיות גם וקטורים מקריים ללא השפעה על ההוכחה) בלתי־תלויים,
	ויהיו $1 = b_0 < b_1 < \cdots < b_k = n$ ונגדיר $Y_1 = (X_{b_0 + 1}, \dots, X_{b_1}), \dots, Y_k = (X_{b_{k - 1} + 1}, \dots, X_{b_k})$. \\*
	אז $Y_1, \dots, Y_k$ בלתי־תלויים. \\*
	כדוגמה, $X_1, \dots, X_7 \rightarrow \overbrace{(X_1, X_2, X_3)}^{Y_1}, \overbrace{(X_4, X_5)}^{Y_2}, \overbrace{(X_6, X_7)}^{Y_3}$.
\end{proposition}
\begin{proof}[הוכחה במקרה הבדיד]
	$Y_i$ הוא וקטור מקרי ממימד $b_i - b_{i - 1}$, צריך להוכיח שלכל $s_1, \dots, s_k$ כך ש$s_1 \in \RR^{b_i - b_{i - 1}}$ מתקיים
	\[
		\PP(\forall i \in k, Y_i = s_i) = \prod_{i = 1}^k \PP(Y_i = s_i)
	\]
	נניח ש־$s_i = (a_{i 1}, \dots, a_{i d_i})$ ולכן נסיק מחוסר התלות של $X_i$
	\[
		\prod_{i = 1}^k \PP(Y_i = s_i)
		= \prod_{i = 1}^k \PP(\forall 1 \le j \le d_i, X_{b_{i - 1} + j} = a_{i j})
		= \prod_{i = 1}^k \prod_{j = 1}^{d_i} \PP(X_{b_{i - 1} + j} = a_{i j})
	\]
	אבל
	\[
		PP(\forall i \in k, Y_i = s_i)
		= \PP(\forall j = X_j = c_j)
		= \prod_{j = 1}^h \PP(X_j = c_j)
		= \prod_{i = 1}^k \prod_{j = 1}^{d_i} \PP(X_{b_{i - 1} + j} = a_{i j})
	\]
	עבור
	\[
		c = (\overbrace{a_{11}, \dots, a_{1d_1}}^{s_1}, \dots, \overbrace{a_{k1}, \dots, a_{kd_1}}^{s_k})
	\]
	ומצאנו כי השוויון אכן מתקיים ו־$Y_1, \dots, Y_k$ בלתי־תלויים.
\end{proof}
\begin{conclusion}
	$X_1, \dots, X_n$ בלתי־תלויים ו־$0 = b_0 < b_1 < \cdots < b_k = n - 1$ ו־$d_i = b_i - b_{i - 1}$ כך ש־$Y_i = (X_{b_{i - 1} + 1}, \dots, X_{b_i})$ ו־$f_1, \dots, f_k$ כאשר $f_i : \RR^{d_i} \to \RR$,
	אז ${\{f_i(Y_i)\}}_{i = 1}^k$ בלתי־תלויים.
\end{conclusion}
\begin{example}
	אם $X_1, \dots, X_n$ בלתי־תלויים אז גם $X_1 + X_2, \dots, X_3 + X_4, \dots, X_{n - 1} + X_{n}$ כנביעה מהמסקנה, \\*
	באופן דומה גם $X_1 + X_2 + X_3, \dots$ בלתי־תלויים. \\*
	כרעיון אנו יכולים לחלק משתנים מקריים לווקטורים בלתי־תלויים, ואז להפעיל פונקציה, שלא משנה את חוסר התלות, על כל הקבוצה.
\end{example}
\begin{example}
	נניח ש־$A_1, \dots, A_5$ מאורעות בלתי־תלויים, אז המאורעות $(A_5 \cap A_4) \cup (A_1 \cap A_2) \cup A_3^C, A_4 \setminus A_5$ בלתי־תלויים,
	זאת אנו עושים על־ידי שימוש במשתנים המקריים האופייניים של $A_i$ ושימוש במסקנה.
\end{example}
נבחין כי דרישת סופיות קבוצת המשתנים המקריים היא לא תנאי הכרחי
\begin{definition}[קבוצה בת־מניה של משתנים מקריים בלתי־תלויים]
	$X_1, X_2, \dots$ משתנים מקריים הם בלתי־תלויים אם לכל $n \in \NN$ מתקיים $X_1, \dots, X_n$ בלתי־תלויים.
\end{definition}
\begin{proposition}
	אם ${\{X_n\}}_{n \in \NN}$ בלתי־תלויים ו־$S_n \in \mathcal{F}_\RR$ לכל $n \in \NN$ אז
	\[
		\PP(\forall n \in \NN, X_n \in S_n)
		= \prod_{n \in \NN} \PP(X_n \in S_n)
	\]
\end{proposition}
נשאל את עצמנו אם מצב זה בכלל אפשרי, נראה טענה ללא הוכחה שעונה על שאלה זו.
\begin{proposition}
	קיימת סדרת משתנים מקריים כזאת שכולם $Ber(\frac{1}{2})$.
\end{proposition}
\begin{proposition}
	סדרה כזו בהכרח לא מוגדרת על מרחב בדיד.
\end{proposition}
\begin{proof}
	נניח ש־$X_1, \dots$ סדרה כזו ונניח ש־$(\Omega, \mathcal{F}, \PP)$ בדיד. \\*
	נניח ש־$\omega_0 \in \Omega$ ו־$\PP(\{\omega_0\}) > 0$, נסמן $s_i = X_i(\omega_0)$, אז
	\[
		0 \xleftarrow[n \to 0]{} {\left(\frac{1}{2}\right)}^n = \PP(\forall i \le n, X_i = s_i) \ge \PP(\forall i \in \NN, X_i = s_i) \ge \PP(\{\omega_0\}) > 0
	\]
	וקיבלנו סתירה לקיום $\omega_0$ כזה.
\end{proof}

\subsection{התפלגות גאומטרית}
ניזכר בהגדרה\ \ref{geometric_distribution}, אשר מדברת על ניסוי שאנו עושים שוב ושוב עד שאנו מצליחים.
\begin{proposition}
	אם $X_1, X_2, \dots$ משתנים מקריים בלתי־תלויים המתפלגים $Ber(p)$ עבור $0 < p < 1$, \\*
	ונסמן $Y = \min \{ k \mid X_k = 1 \}$, אז $Y \sim Geo(p)$.
\end{proposition}
נבחין כי $Y$ מייצג בחירת המופע הראשון של $1$ בהתפלגות ברנולי, נזכיר כי היא מייצגת הגרלה יחידה, לדוגמה הטלת מטבע בודד. נעבור להוכחה.
\begin{proof}
	המאורע $Y = l$ הוא המאורע $X_1 = X_2 = \cdots = X_{l - 1} = 0$ ו־$X_l = 1$, אבל שלו הם משתנים בלתי־תלויים, לכן
	\[
		\PP(X_1 = \cdots = X_{l - 1} = 0, X_l = 1)
		= \PP(X_1 = 0) \cdots \PP(X_{l - 1}) \PP(X_l = 1)
		= {(1 - p)}^{l - 1} p
	\]
	זוהי התפלגות גאומטרית.
\end{proof}
\begin{remark}
	הסכום הוא
	\[
		\sum_{l = 1}^{\infty} {(1 - p)}^{l - 1} p = 1
	\]
	ולכן המקרה שבו אין מינימום כפי שהגדרנו לא רלוונטי להגדרה, וניתן להתעלם ממנו.
\end{remark}
מה יקרה אם נגדיר ככה $Y_1 = Y$ ו־$Y_2$ המשתנה המקרי שעבורו קיבלנו 1 בפעם השנייה וכן הלאה, אז $Y_2 - Y_1$ וסדרת החיסורים היא בלתי תלויה־אף היא, תוצאה אינטואיטיבית אך לא מובנת מאליו.

\section{תרגול 6 --- 5.12.2024}
\subsection{שאלות בנושאי משתנים מקריים בלתי־תלויים}
\begin{exercise}
	שתיים מטילות $n$ מטבעות הוגנים כל אחת באופן בלתי־תלוי. \\*
	מה ההסתברות שהן קיבלו אותו מספר תוצאות עץ?
\end{exercise}
\begin{solution}
	נגדיר $X_i \sim Ber(\frac{1}{2})$ ההטלה ה־$i$ של הראשונה, ו־$Y \sim Ber(\frac{1}{2})$ ההטלה ה־$i$ של השנייה, ונגדיר
	\[
		X = \sum_{i = 1}^{n} X_i,
		\qquad
		Y = \sum_{i = 1}^{n} Y_i
	\]
	משתנים המייצגים את מספר הטלות העץ של כל אחת מהשתיים. \\*
	אבל זאת דרך מורכבת לפתור את השאלה הזאת, נגדיר במקום זה $X, Y \sim Bin(n, \frac{1}{n})$, את ההוכחה לשקילות נראה בהרצאה הקרובה. \\*
	מהגדרה\ \ref{binominal_distribution} נוכל להסיק $\supp X = \{0, \dots, n\}$, ואנו מבקשים לחשב את $\PP(X = Y)$, נחשב על־ידי
	\[
		\PP(X = Y)
		= \sum_{k = 0}^{n} \PP(X = k = Y)
		= \sum_{k = 0}^{n} \PP(X = k) \PP(Y = k)
		= \sum_{k = 0}^{n} \binom{n}{k} \frac{1}{2^n} \binom{n}{k} \frac{1}{2^n}
		= \frac{1}{2^{2n}} \sum_{k = 0}^{n} {\binom{n}{k}}^2
	\]
\end{solution}
\begin{proposition}\label{binominal_distributions_summation_proposition}
	יהיו $X \sim Bin(n, p)$ ו־$Y \sim Bin(m, p)$ בלתי־תלויים, אז $X + Y \sim Bin(n + m, p)$.
\end{proposition}
\begin{proof}
	נבחין תחילה ש־$\supp X + Y = \{0, \dots, n + m\}$ וכן
	\begin{align*}
		\PP(X + Y = k)
		& = \sum_{i = 0}^{n + m} \PP(X = i, Y = k - i) \\
		& = \sum_{i = 0}^{n + m} \PP(X = i) \PP(Y = k - i) \\
		& = \sum_{i = 0}^{n + m} \binom{n}{i} p^i {(1 - p)}^{n - i} \binom{m}{k - i} p^{k - i} {(1 - p)}^{m - k + i} \\
		& = p^k {(1 - p)}^{n + m - k} \sum_{i = 0}^{n + m} \binom{n}{i} \binom{m}{k - i} \\
		& = p^k {(1 - p)}^{n + m - k} \binom{n + m}{k}
	\end{align*}
	כאשר עלינו להוכיח את השוויון האחרון, זאת נעשה בתרגיל הבא.
\end{proof}
\begin{exercise}[זהות ונדרמונדה]
	מתקיים
	\[
		\sum_{i = 0}^{n + m} \binom{n}{i} \binom{m}{k - i}
		= \binom{n + m}{k}
	\]
\end{exercise}
\begin{solution}
	נבחין כי $\binom{n + k}{k}$ הוא קומבינטורית שקול לבחירה $k$ ערכים מתוך קבוצה בגודל $m$ וקבוצה בגודל $n$. \\*
	במקביל $\binom{n}{i} \binom{m}{k - i}$ הוא היכולת לבחור $i$ מתוך $m$ ועוד השארית מ־$k$ מהקבוצה השנייה. \\*
	נבחין כי זוהי הוכחה קומבינטורית ואפשרי להוכיח גם אלגברית את השוויון הנתון.
\end{solution}
\begin{proposition}
	ניזכר בהגדרה $\ref{poasson_distribution}$ יהיו $X \sim Pois(\lambda)$ ו־$Y \sim Pois(\lambda + \eta)$ בלתי־תלויים, אז $X + Y \sim Pois(\lambda + \eta)$.
\end{proposition}
ניזכר בהתפלגות פואסון עם דוגמה.
אם מגיעים לבית־חולים בממוצע $\lambda = 10$ אנשים ביום, אז התפלגות פואסון היא השאלה כמה אנשים הגיעו ביום ספציפי לבית־החולים. \\*
בהתאם השאלה שאנו שואלים מדברת על מקרה שבו יש שני בתי־חולים ואנו שואלים על כמה אנשים הגיעו ביום. \\*
ברור לנו אם כן שטענה זו הגיונית, נעבור להוכחה.
\begin{proof}
	תחילה $\supp(X + Y) = \NN \cup \{0\}$ ונניח $k \in \supp(X + Y)$, ונחשב
	\begin{align*}
		\PP(X + Y = k)
		& = \sum_{n = 0}^{\infty} \PP(X = n) \PP(Y = k - n) \\
		& = \sum_{n = 0}^{k} \frac{e^{-\lambda} \lambda^n}{n!} \frac{e^{-\eta} \eta^{k - n}}{(k - n)!} \\
		& = e^{-(\lambda + \eta)} \sum_{n = 0}^{k} \frac{\lambda^n \eta^{k - n}}{n! (k - n)!} \\
		& = \frac{e^{-(\lambda + \eta)}}{k!} \sum_{n = 0}^{k} \frac{k!}{n! (k - n)!} \lambda^n \eta^{k - n} \\
		& = \frac{e^{-(\lambda + \eta)}}{k!} \sum_{n = 0}^{k} \binom{k}{n} \lambda^n \eta^{k - n} \\
		& = \frac{e^{-(\lambda + \eta)}}{k!} {(\lambda + \eta)}^k
	\end{align*}
\end{proof}
\begin{proposition}
	נניח $X \sim Pois(\lambda)$ ו־$Y$ משתנה מקרי המקיים
	\[
		\forall n \in \NN \cup \{0\},\ Y \mid \{X = n\} \sim Bin(n, p)
	\]
	אז $Y \sim Pois(\lambda p)$.
\end{proposition}
\begin{proof}
	הפעם $\supp Y = \NN \cup \{0\}$. \\*
	נניח ש־$k \in \NN \cup \{0\}$ ונחשב את $\PP(Y = k)$ על־ידי שימוש בנוסחת ההסתברות השלמה.
	\begin{align*}
		\PP(Y = k)
		& = \sum_{n = 0}^{\infty} \PP(X = n) \PP(Y = k \mid X = n) \\
		& = \sum_{n = 0}^{\infty} \frac{e^{-\lambda} \lambda^n}{n!} \binom{n}{k} p^k {(1 - p)}^{n - k} \\
		& = \sum_{n = k}^{\infty} \frac{e^{-\lambda} \lambda^n}{n!} \frac{n!}{k! (n - k)!} p^k {(1 - p)}^{n - k} \\
		& = \frac{e^{-\lambda} p^k}{k!} \sum_{n = k}^{\infty} \frac{\lambda^n {(1 - p)}^{n - k}}{(n - k)!} \\
		& = \frac{e^{-\lambda} p^k}{k!} \sum_{m = 0}^{\infty} \frac{\lambda^{m + k} {(1 - p)}^m}{m!} \\
		& = \frac{e^{-\lambda} p^k \lambda^k}{k!} \sum_{m = 0}^{\infty} \frac{\lambda^m {(1 - p)}^m}{m!} \\
		& = \frac{e^{-\lambda} p^k \lambda^k}{k!} e^{\lambda(1 - p)} \\
		& = \frac{e^{-\lambda p} p^k {(p \lambda)}^k}{k!}
	\end{align*}
\end{proof}

\section{שיעור 12 --- 5.12.2024}

\subsection{התפלגות גאומטרית}
\begin{proposition}
	אם $X_i \sim Ber(p)$ אז $Y = \min{k \mid X_k = 1}$ אז $Y \sim Geo(p)$.
\end{proposition}
\begin{theorem}[תכונת חוסר הזיכרון]
	$X$ משתנה מקרי הנתמך על $\NN$, ב־$\PP(X > 1) > 0$.
	אז התנאים הבאים שקולים:
	\begin{enumerate}
		\item $X \sim Geo(p)$ עבור $0 < p < 1$ כלשהו.
		\item לכל $l \in \NN$ מתקיים $X \overset{d}{=} X - l \mid X > l$. כלומר $\PP(X > l \mid X - l > 0) = \PP(X = k)$ לכל $l \in \NN$.
		\item $X \overset{d}{=} X - 1 \mid X > 1$. כלומר לכל $S \in \mathcal{F}_\RR$ מתקיים $\PP(X \in S) = \PP(X - 1 \in S \mid X > 1)$.
	\end{enumerate}
\end{theorem}
נראה טענה קודמת שתעזור לנו בהוכחת המשפט
\begin{proposition}
	אם $X$ משתנה מקרי שנתמך על $\NN$ אז התנאים הבאים שקולים:
	\begin{enumerate}
		\item $X \sim Geo(p)$
		\item $\PP(X > n) = {(1 - p)}^n$
	\end{enumerate}
\end{proposition}
\begin{proof}
	$1 \implies 2$:
	\[
		\PP(X > n)
		= \sum_{k = n + 1}^{\infty} \PP(X = k)
		= \sum_{k = n + 1}^{\infty} {(1 - p)}^{k - 1} p
		= {(1 - p)}^n \sum_{l = 1}^{\infty} {(1 - p)}^{l - 1} p
		= {(1 - p)}^n
	\]

	$2 \implies 1$:
	\[
		\forall n \in \NN,\ \PP(X = n)
		= \PP(X > n - 1) - \PP(X > n)
		= {(1 - p)}^{n - 1} - {(1 - p)}^n
		= {(1 - p)}^{n - 1} (1 - (1 - p))
	\]
\end{proof}
\begin{proof}[הוכחת המשפט]
	$1 \implies 2$:
	נניח $l, k \in \NN$
	\[
		{(1 - p)}^{k - 1} p
		= \PP(X = k)
		= \PP(X - l = k \mid X - l > 0)
		= \frac{\PP(X = l + k)}{\PP(X > l)}
		= \frac{{(1 - p)}^{l + k - 1} p}{{(1 - p)}^l}
		= {(1 - p)}^{l - 1} p
	\]

	$2 \implies 3$ מיידי.

	$3 \implies 1$:
	נסמן $p = \PP(X = 1)$ ונראה ש־$X \sim Geo(p)$ על־ידי כך שנראה $\forall n \in \NN,\ \PP(X < n) = {(1 - p)}^n$ והטענה. \\*
	נוכיח באינדוקציה. עבור $n = 1$ נובע $\PP(X > 1) = 1 - \PP(X = 1) = 1 - p$. \\*
	נניח שהטענה נכונה ל־$n$ ונראה
	\[
		\PP(X > n + 1)
		= \PP(X > 1) \PP(X > n + 1 \mid X > 1)
		= (1 - p) \PP(X - 1 > n \mid X > 1)
		= (1 - p) \PP(X > n)
		= (1 - p) {(1 - p)}^n
	\]
	והשלמנו את מהלך האינדוקציה.
\end{proof}

\subsection{התפלגות בינומית}
נעבור לדבר על התפלגויות בינומיות כפי שהגדרנו בהגדרה\ \ref{binominal_distribution}.
\begin{proposition}
	אם $X_1, \dots, X_n$ משתנים מקריים בלתי תלויים מתפלגים $Ber(p)$, אז $Y = \sum_{i = 1}^n X_i$ מתפלג $Bin(n, p)$.
\end{proposition}
\begin{proof}
	\begin{align*}
		\PP(Y = k)
		& = \sum_{\substack{v \in {\{0, 1\}}^n \\ \sum v_i = k}} \PP(X_1 = v_1, \dots, X_n = v_n) \\
		& = \sum_{\substack{v \in {\{0, 1\}}^n \\ \sum v_i = k}} \prod_{i = 1}^k \PP(X_i = v_i) \\
		& = \sum_{\substack{v \in {\{0, 1\}}^n \\ \sum v_i = k}} p^k {(1 - p)}^{n - k} \\
		& = \binom{n}{k} p^k {(1 - p)}^{n - k}
	\end{align*}
\end{proof}
ניזכר בטענה\ \ref{binominal_distributions_summation_proposition} ונוכיח אותה הפעם בדרך פורמלית ולא על־ידי אינטואיציה.
\begin{proof}
	נניח שיש $Z_1, \dots, Z_{n + m}$ משתנים מקריים בלתי־תלויים $Ber(p)$ כך שמתקיים
	\[
		X' = \sum_{i = 1}^n Z_i,
		\qquad
		Y' = \sum_{i = n + 1}^{n + m} Z_i
	\]
	אז $X' + Y' = \sum_{i = 1}^{n + m} Z_i$, לפי הטענה
	\[
		X' \sim Bin(n, p),
		\qquad
		Y' \sim Bin(m, p)
	\]
	וכן
	\[
		X' + Y' \sim Bin(n + m, p)
	\]
	לפי הטענה מההרצאה הקודמת $X'$ ו־$Y'$ בלתי־תלויים. \\*
	אבל $X \overset{d}{=} X'$ ו־$Y \overset{d}{=} Y'$ ו־$X$ ו־$Y$ בלתי־תלויים וגם $X', Y'$ בלתי־תלויים, אז $(X', Y') \overset{d}{=} (X, Y)$.
	לבסוף נובע $X + Y \overset{d}{=} X' + Y'$.
\end{proof}

\subsection{התפלגות פואסון}
נעבור להתפלגות\ \ref{poasson_distribution} ונבחן אותה
\begin{proposition}
	נניח $\lambda > 0$ ונגדיר $X_n \sim Bin(n, \frac{\lambda}{n})$ עבור $n > \lambda$, \\*
	אז לכל $k \in \NN$ מתקיים
	\[
		\PP(X_n = k) \xrightarrow[n \to \infty]{} \PP(Y = k)
	\]
	עבור $Y \sim Pois(\lambda)$.
\end{proposition}
\begin{proof}
	\[
		\PP(X_n = 0)
		= \binom{n}{0} {\left(\frac{\lambda}{n}\right)}^0 {\left(1 - \frac{\lambda}{n}\right)}^n \xrightarrow[n \to \infty]{} e^{-\lambda}
	\]
	באופן דומה
	\[
		\PP(X_n = 1)
		= \binom{n}{1} {\left(\frac{\lambda}{n}\right)}^1 {\left(1 - \frac{\lambda}{n}\right)}^n \cdot {\left(1 - \frac{\lambda}{n}\right)}^{-1} \xrightarrow[n \to \infty]{} e^{-\lambda} \cdot \lambda
	\]
	ונעבור למקרה הכללי
	\[
		\PP(X_n = k)
		= \overbrace{\binom{n}{k} {\left(\frac{\lambda}{n}\right)}^k}^{\frac{n!}{k! (n - k)!} \frac{\lambda^k}{n^k} \to \frac{\lambda^k}{k!}} {\left(1 - \frac{\lambda}{n}\right)}^n \cdot {\left(1 - \frac{\lambda}{n}\right)}^{-k}
		\xrightarrow[n \to \infty]{} \frac{\lambda^k}{k!} e^{-\lambda} \cdot 1
	\]
\end{proof}
נחזור לטענה שראינו בתרגיל הבית:
\begin{proposition}
	אם $X \sim Pois(\lambda_1)$ ו־$Y \sim Pois(\lambda_2)$ בלתי־תלויים אז $X + Y \sim Pois(\lambda_1 + \lambda_2)$.
\end{proposition}
הפעם אפשר יהיה להוכיחה על־ידי הטענה החדשה שראינו.
\begin{example}
	נניח ${\{X_n\}}_{n \in \NN}$ משתנים מקריים בלתי־תלויים $U([26])$ המייצגים את קבלת כל אחת מהאותיות באנגלית, אז
	\[
		\PP(X_1 = 1, \dots, X_{1000} = 1) = \frac{1}{26^{1000}} = \PP(X_{1001} = 1, \dots, X_{2000} = 1)
	\]
	ההסתברות שיצא טקסט שמורכב מהאות $a$ 1000 פעמים.
	הרעיון הוא שאין קשר בין המיקום שבו אנו שואלים עם הטקסט הופיע, אלא רק מהו אורך הטקסט, בהתאם
	\[
		\PP(\lnot \exists k,\ X_{1000 k + 1} = 1, \dots, X_{1000k + 1000} = 1)
		= {(1 - \frac{1}{26^{1000}})}^n
		\to 0
	\]
	ולכן בסופו של דבר הטקסט הזה בהכרח יופיע.
\end{example}

\section{שיעור 13 --- 10.12.2024}
\subsection{תוחלת}
\begin{definition}[תוחלת במשתנים מקריים בדידים]
	$X$ משתנה מקרי בדיד.
	ה\textbf{תוחלת} של $X$ היא
	\[
		\EE(X) = \sum_{s \in \RR} s \PP(X = s)
	\]
\end{definition}
\begin{remark}
	אם הטור $\sum_{s \in \RR} s \PP(X = s)$ לא מתכנס בהחלט, אז נאמר שאין תוחלת ל־$X$.
\end{remark}
\begin{example}
	נניח ש־$\Omega = [100]$ מרחב הסתברות אחיד, מייצג קבוצת תלמידים. \\*
	נגדיר $X(\omega)$ הציון של תלמיד $\omega$ במבחן. \\*
	אז $\EE(X)$ יהיה ממוצע הציונים בכיתה.
\end{example}
\begin{proposition}
	אם $X$ מוגדר על מרחב הסתברות בדידה, אז $\EE(X) = \sum_{\omega \in \Omega} X(\omega) \PP(\{\omega\})$.
\end{proposition}
\begin{proof}
	מההגדרה של מרחב הסתברות בדידה נוכל להשתמש בתומך ואז נובע
	\[
		\EE(X)
		= \sum_{s \in \RR} s \PP(X = s)
		= \sum_{s \in \RR} s \sum_{\substack{\omega \in \Omega \\ X(\omega) = s}} \PP(\{\omega\})
		= \sum_{s \in \RR} \sum_{\substack{\omega \in \Omega \\ X(\omega) = s}} X(\omega) \PP(\{\omega\})
		= \sum_{\omega \in \Omega} X(\omega) \PP(\{\omega\})
	\]
\end{proof}
\begin{proposition}
	אם $X_1, \dots, X_n$ משתנים מקריים בדידים ו־$f \in \mathcal{F}_{\RR^n \to \RR}$, $Y = f(X_1, \dots, X_n)$, אז
	\[
		\EE(Y) = \sum_{(s_1, \dots, s_n) \in \RR^n} f(s_1, \dots, s_n) \PP(X_1 = s_1, \dots, X_n = s_n)
	\]
\end{proposition}
\begin{proof}
	כמקודם נשתמש בתומך וכך נראה את השוויון.
	\begin{align*}
		\EE(Y)
		& = \sum_{s \in \RR} s \PP(Y = s) \\
		& = \sum_{s \in \RR} s \sum_{\substack{(s_1, \dots, s_n) \in \RR^n \\ f(s_1, \dots, s_n) = s}} \PP(X_1 = s_1, \dots, X_n = s_n) \\
		& = \sum_{s \in \RR} \sum_{\substack{(s_1, \dots, s_n) \in \RR^n \\ f(s_1, \dots, s_n) = s}} f(s_1, \dots, s_n) \PP(X_1 = s_1, \dots, X_n = s_n) \\
		& = \sum_{(s_1, \dots, s_n) \in \RR^n} f(s_1, \dots, s_n) \PP(X_1 = s_1, \dots, X_n = s_n)
	\end{align*}
\end{proof}
\begin{example}
	נניח $X \sim Ber(p)$ אז
	\[
		\EE(X) = 0 \cdot \PP(X = 0) + 1 \cdot \PP(X = 1) = p
	\]
\end{example}
\begin{example}
	אם $X \sim U([n])$ אז
	\[
		\EE(X) = \sum_{k = 1}^n k \cdot \PP(X = k)
		= \frac{1}{n} \sum_{k = 1}^{n} k
		= \frac{1}{n} \cdot \frac{n(n + 1)}{2}
		= \frac{n + 1}{2}
	\]
\end{example}
\begin{example}
	אם $X \sim Bin(n, p)$ אז
	\[
		\EE(X)
		= \sum_{k = 0}^{n} k \binom{n}{k} p^k {(1 - p)}^{n - k}
		= np
	\]
\end{example}
\begin{example}
	נניח $X \sim Poi(\lambda)$, אז
	\[
		\EE(X)
		= \sum_{k = 0}^\infty k e^{-\lambda} \frac{\lambda^k}{k!}
		= \sum_{k = 1}^\infty e^{-\lambda} \frac{\lambda^k}{(k - 1)!}
		= \lambda e^{-\lambda} \sum_{k = 1}^\infty \frac{\lambda^{k - 1}}{(k - 1)!}
		= \lambda e^{-\lambda} \sum_{k = 0}^\infty \frac{\lambda^k}{k!}
		= \lambda e^{-\lambda} e^\lambda
		= \lambda
	\]
\end{example}
ולבסוף גם
\begin{example}
	נניח $X \sim Geo(p)$ ולכן
	\[
		\EE(X)
		= \sum_{k = 1}^{\infty} k {(1 - p)}^{k - 1}
		= \frac{1}{p}
	\]
	את החישוב עצמו שמוכיח את הטענה הזאת נעשה בהמשך.
\end{example}
\begin{example}
	נניח $X \sim Geo(\frac{1}{2})$ ונגדיר $Y = 2^X$. \\*
	בהתאם $\PP(Y = 2) = \frac{1}{2}$ וכן $\PP(Y = 4) = \frac{1}{4}$ וכן הלאה, כך ש־$\PP(Y = 2^k) = \frac{1}{2^k}$.
	נחשב את התוחלת:
	\[
		\EE(Y)
		= \sum_{s \in \{2, 4, 8, \dots\}} 2^k \PP(Y = 2^k)
		= \sum_{k = 1}^{\infty} 2^k \frac{1}{2^k}
		= \sum_{k = 1}^{\infty} 1
		= \infty
	\]
	ולכן אין תוחלת.

	יכולנו להחליף את הגדרת $Y$ להיות $Y = {(-2)}^X$ והיינו מקבלים טור שלא מתכנס בכלל.
\end{example}
\begin{remark}
	אפשר להרחיב את התוחלת ותכונותיה למקרים אינסופיים, אנו לא נעשה זאת.
\end{remark}

\subsection{תכונות של תוחלת}
\begin{proposition}[תכונות של תוחלת]
	אם $X, Y$ משתנים מקריים בעלי תוחלת, אז:
	\begin{enumerate}
		\item אם $X \ge 0$ כמעט תמיד אז $\EE(X) \ge 0$.
			אם בנוסף $\PP(X > 0) > 0$ אז $\EE(X) > 0$.
		\item לינאריות: אם $a, b \in \RR$ אז אם $Z = aX + bY$ יש תוחלת והיא $\EE(Z) = a \EE(X) + b \EE(Y)$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item אם $X \ge 0$ כמעט תמיד אז כל המחוברים אי־שליליים ולכן $\EE(X) \ge 0$. \\*
			אם בנוסף $\PP(X > 0) > 0$ אז קיים $s > 0$ כך ש־$\PP(S = s) > 0$ ולכן $s \PP(X = s) > 0$ ולכן הסכום חיובי.
		\item נגדיר $f(x, y) = ax + by$ עבור $f \in \mathcal{F}_{\RR^2 \to \RR}$. אז
			\begin{align*}
				\EE(Z)
				& = \EE(f(x, y)) \\
				& = \sum_{(s, t) \in \RR^2} f(s, t) \PP(X = s, Y = t) \\
				& = \sum_{(s, t) \in \RR^2} (as + bt) \PP(X = s, Y = t) \\
				& = a \left(\sum_{(s, t) \in \RR^2} s \PP(X = s, Y = t)\right) + b \left(\sum_{(s, t) \in \RR^2} t \PP(X = s, Y = t)\right) \\
				& = a \left(\sum_{s \in \RR} \sum_{t \in \RR} s \PP(X = s, Y = t)\right) + b \left(\sum_{t \in \RR} \sum_{s \in \RR} t \PP(X = s, Y = t)\right) \\
				& = a \sum_{s \in \RR} s \PP(X = s) + b \sum_{t \in \RR} t \PP(Y = t) \\
				& = a \EE(X) + b \EE(Y)
			\end{align*}
	\end{enumerate}
\end{proof}
\begin{conclusion}
	אם $X$ ו־$Y$ משתנים מקריים בעלי תוחלת ו־$Y \le X$ כמעט תמיד, אז $\EE(Y) \le \EE(X)$.
\end{conclusion}
\begin{proof}
	נגדיר $Z = X - Y$ ואז $Z \ge 0$ כמעט תמיד ולכן $\EE(Z) \ge 0$ ואז $X = Y + Z$ ולכן $\EE(X) = \EE(Y) + \EE(Z) \ge \EE(Y)$.
\end{proof}
\begin{example}
	נעבור לראות את החישוב של תוחלת להתפלגות בינומית:
	נגדיר $X \sim Bin(n, p)$ ונגדיר $X_1, \dots, X_n$ משתנים מקריים $Ber(p)$, ויהי $X = \sum_{i = 1}^n X_i$, לכן $X \sim Bin(n, p)$ וכן
	\[
		\EE(X) = \sum_{i = 1}^n \EE(X_i) = np
	\]
\end{example}
\begin{definition}[תוחלת מותנית]
	$X$ משתנה מקרי ו־$A$ מאורע, כך ש־$\PP(A) > 0$, אז
	\[ 
		\EE(X \mid A)
		= \sum_{s \in \RR} s \PP(X = s \mid A)
	\]
\end{definition}
\begin{proposition}
	\[
		\EE(X \mid A) = \frac{\EE(X \cdot 1_A)}{\PP(A)}
	\]
\end{proposition}
\begin{proof}
	\[
		\EE(X \mid A)
		= \sum_{s \in \RR} s \PP(X = s \mid A)
		= \sum_{s \in \RR} s \frac{\PP(X = s, A)}{\PP(A)}
		= \sum_{s \in \RR} s \frac{\PP(X \cdot 1_A = s)}{\PP(A)}
	\]
	כאשר המעבר האחרון נובע מהגדרת המציין ובדיקה ידנית של המקרים בהם $s \in A, s \notin A$, כלומר
	\[
		\PP(X = s, A)
		= \PP(\{\omega \in \Omega \mid X(\omega) = s, \omega \in A\})
		= \PP(\{\omega \in \Omega \mid X(\omega) = s, 1_A(\omega) = 1\})
	\]
\end{proof}
\begin{proposition}
	אם $A_1, \dots, A_n$ חלוקה של $X$ משתנה מקרי בעל תוחלת, אז
	\[
		\EE(X) = \sum_{k = 1}^n \EE(X \cdot q_{A_k})
	\]
\end{proposition}
\begin{proof}
	מאותו מעבר כמו בהוכחה הקודמת נסיק
	\[
		X = \sum_{k = 1}^n X \cdot 1_{A_k}
	\]
	ואז משתמש בתכונת הלינאריות של תוחלות ונקבל את המבוקש.
\end{proof}
\begin{proposition}[נוסחת התוחלת השלמה]
	אם $A_1, \dots, A_n$ חלוקה ו־$\PP(A_k) > 0$ לכל $k$ אז
	\[
		\EE(X) = \sum_{k = 1}^n \PP(A_k) \EE(X \mid A_k)
	\]
\end{proposition}
\begin{proof}
	על־ידי הטענות הקודמות נובע
	\[
		\EE(X)
		= \sum_{k = 1}^n \EE(X \cdot 1_{A_k})
		= \sum_{k = 1}^n \PP(A_k) \EE(X \mid A_k)
	\]
\end{proof}
\begin{example}
	נניח $X \sim Geo(p)$ ו־$A_1 = \{X = 1\}$ וכן $A_2 = \{X > 1\}$, אז $\{A_1, A_2\}$ הם אכן חלוקה של $\Omega$. \\*
	נחשב את התוחלת:
	\[
		\EE(X)
		= \PP(A_1) \EE(X \mid A_1) + \PP(A_2) \EE(X \mid A_2)
		= p \cdot 1 + (1 - p) \cdot \EE(X \mid X > 1)
	\]
	אבל אז מתכונת חוסר הזיכרון
	\[
		p \cdot 1 + (1 - p) \cdot \EE(X \mid X > 1)
		= p + (1 - p) \cdot (\EE(X - 1 \mid X > 1) + \EE(1 \mid X > 1))
		= p + (1 - p) \cdot (\EE(X) + 1)
	\]
	ולכן קיבלנו את השוויון $\EE(X) = p + (1 - p)(\EE(X) + 1)$, ממנו נובע $0 = p + (1 - p) - p \EE(X)$, לכן $\EE(X) = \frac{1}{p}$.
\end{example}

\section{תרגול 7 --- 12.12.2024}
\subsection{שאלות ותכונות של תוחלות}
באנגלית תוחלת היא Expectancy, מילה שמתארת בצורה יותר נאמנה את מושג התוחלת.
\begin{example}
	נניח ש־$X \sim Geo(p)$, ונחשב את התוחלת של $X$. \\*
	יש להעריך את הטור $\sum_{n = 1}^\infty n \PP(X = n)$.
	\[
		\sum_{k = 1}^\infty k \PP(X = k)
		= \sum_{n = 1}^\infty k p {(1 - p)}^{k - 1}
		= p \sum_{n = 1}^\infty k {(1 - p)}^{k - 1}
	\]
	נגדיר $f \in (0, 1)$ את $f(q) = \frac{1}{1 - q} = \sum_{k = 0}^{\infty} q^k$ ולכן $f$ אנליטית ומתכנסת במידה שווה על תת־קבוצות של התחום שלה.
	אז
	\[
		\frac{1}{{(1 - q)}^2}
		= f'(q)
		= \sum_{k = 0}^{\infty} k q^{k - 1}
		= \sum_{k = 1}^{\infty} k q^{k - 1}
	\]
	אם נציב $q = 1 - p$ אז נובע
	\[
		p \sum_{n = 1}^\infty k {(1 - p)}^{k - 1}
		= p \cdot \frac{1}{{(1 - (1 - p))}^2}
		= \frac{1}{p}
	\]
\end{example}
\begin{example}
	אם $X \sim Bin(n, p)$ ואם נגדיר $Y \sim Bin(n - 1, p)$ אז
	\begin{align*}
		\EE(X)
		& = \sum_{k = 0}^{n} k \binom{n}{k} p^k {(1 - p)}^{n - k} \\
		& = \sum_{k = 1}^{n} \frac{k \cdot n!}{(n - k)! k!} p^k {(1 - p)}^{n - k} \\
		& = \sum_{k = 1}^{n} \frac{n!}{(n - k)! (k - 1)!} p^k {(1 - p)}^{n - k} \\
		& = \sum_{m = 0}^{n - 1} \frac{n!}{(n - m - 1)! m!} p^{m + 1} {(1 - p)}^{n - m - 1} \\
		& = n p \sum_{m = 0}^{n - 1} \frac{(n - 1)!}{(n - m - 1)! m!} p^m {(1 - p)}^{n - m - 1} \\
		& = n p \PP(y \in \supp Y) \\
		& = n p
	\end{align*}
\end{example}
\begin{example}[תוחלת של משתנה מקרי היפר־גאומטרי]
	ניזכר בשאלה: בכד יש $a$ כדורים אדומים ו־$b$ כדורים שחורים ושולפים $k$ כדורים ללא החזרה לכד. \\*
	$X$ משתנה מקרי שסופר את מהספר הכדורים האדומים. \\*
	נגדיר $X_i$ המשתנה המקרי שבשליפה ה־$i$ יצא כדור אדום, ו־$X = \sum_{i = 1}^k X_i$.
	נוכל להגדיר גם $\Omega = \{ (y_1, \dots, y_k) \mid i \ne j \implies y_i \ne y_j \}$, כאשר מסמנים את הכדורים האדומים ב־$\{1, \dots, a\}$ וכן את השחורים ב־$\{a + 1, \dots, b\}$.
	אז
	\[
		\PP(X_i = 1)
		= \PP(y_i \le a)
		= \sum_{j = 1}^{a} \PP(y_i = j)
		= \sum_{j = 1}^{a} \frac{1}{a + b}
		= \frac{a}{a + b}
	\]
	ולכן $\EE(X) = \frac{k \cdot a}{a + b}$.
\end{example}
נעבור לבחינת דוגמה לשימוש בנוסחת התוחלת השלמה, אותה ראינו בהרצאה האחרונה.
\begin{exercise}
	מטילים קובייה הוגנת שוב ושוב עד שיוצא $1$. \\*
	מה תוחלת סכום ערכי הקובייה?
\end{exercise}
\begin{solution}
	נגדיר $X$ משתנה מקרי שסוכם את מספר סיבובי המשחק, לכן $X \sim Geo(\frac{1}{6})$. \\*
	נגדיר גם $Y_i$ להיות תוצאת ההטלה ה־$i$, אם היא התקיימה. \\*
	בנוסף $Y = \sum_{i = 1}^{\infty} Y_i$, הערך המעניין אותנו, סכום הקובייה בסוף המשחק.
	\[
		Y_i \mid X = n
		\sim \begin{cases}
			U(2, \dots, 6) & i < n \\
			1 & i = n \\
			0 & i > n
		\end{cases}
	\]
	ולכן נשתמש בנוסחת התוחלת השלמה
	\begin{align*}
		\EE(Y)
		& = \sum_{n = 1}^\infty \EE(Y \mid X = n) \cdot \PP(X = n) \\
		& = \sum_{n = 1}^\infty (\sum_{i = 1}^n \EE(X_i \mid X = n)) \cdot \PP(X = n) \\
		& = \sum_{n = 1}^\infty (\sum_{i = 1}^n 4 + 1) \cdot {(\frac{5}{6})}^{n - 1} \\
		& = \frac{1}{6} \sum_{n = 1}^\infty (4n - 3) {(\frac{5}{6})}^{n - 1} \\
		& = \frac{1}{6} (4 \sum_{n = 1}^\infty n {(\frac{5}{6})}^{n - 1} - 3 \sum_{n = 1}^\infty n {(\frac{5}{6})}^{n - 1}) \\
		& = \frac{1}{6} (4 \cdot \frac{1}{{(\frac{1}{6})}^2} - 3 \cdot 6) \\
		& = 21
	\end{align*}
\end{solution}

\section{שיעור 14 --- 12.12.2024}
\subsection{תוחלת --- המשך}
נבחין כי מתקיימת הטענה הבאה, אך לא נוכיח אותה שכן אין בכך ערך לימודי:
\begin{proposition}[נוסחת התוחלת השלמה הבת־מניה]
	אם $X, Y$ משתנים מקריים בדידים, אז
	\[
		\EE(X) = \sum_{t \in \RR} \PP(Y = t) \EE(X \mid Y = t)
	\]
\end{proposition}
נבחן תכונה נוספת.
\begin{proposition}[תוחלת מכפלת משתנים מקריים בלתי־תלויים]
	אם $X, Y$ משתנים מקריים בלתי־תלויים ובעלי תוחלת, אז
	\[
		\EE(X Y) = \EE(X) \EE(Y)
	\]
\end{proposition}
נבחין שבשונה מלינאריות תוחלת, במקרה הזה אנו צריכים את חוסר־התלות.
\begin{proof}
	לפי נוסחת התוחלת השלמה
	\[
		\EE(XY) = \sum_{t \in \RR} \PP(Y = y) \EE(XY \mid Y = t)
	\]
	בהינתן $Y = t$ ההתפלגות של $Y$ מתרכזת כולה ב־$t$, כלומר
	\[
		\PP(Y = s \mid Y = t) = \begin{cases}
			1 & s = t \\
			0 & \text{else}
		\end{cases}
	\]
	לכן בהינתן $Y = t$ מתקבל $XY \overset{a.s.}{=} Xt$ ובהתאם
	\[
		XY \mid Y = t \overset{a.s.}{=} Xt \mid Y = t
	\]
	לכן
	\[
		\EE(XY \mid Y = t)
		= \EE(Xt \mid Y = t)
		= t \EE(X \mid Y = t)
		= t \EE(X)
	\]
	ומשילוב השוויונות שמצאנו נובע
	\[
		\EE(XY)
		= \sum_{t \in \RR} \PP(Y = y) t \EE(X)
		= \EE(X) \EE(Y)
	\]
\end{proof}
נעבור לדון במה בכלל המשמעות של תוחלת.
עד כה מצאנו תכונות שלה, ואף הגדרות שקולות, אך מה המשמעות של התוחלת בהקשר הסתברותי?
באיזה מובן עלינו להתחשב בתוחלת במקרה שבו אנו יודעים את ערכה, כשהיא חיובית?
נעבור להליך שנותן לנו מידע בהסתברות מתוך מידע על תוחלות.
\begin{theorem}[אי־שוויון מרקוב]\label{markov_inequality}
	יהי $X$ משתנה מקרי אי־שלילי (דהינו $X \overset{a.s.}{\ge} 0$) ובעל תוחלת. \\*
	אז לכל $a > 0$ מתקיים
	\[
		\PP(X \ge a) \le \frac{\EE(X)}{a}
	\]
\end{theorem}
\begin{proof}
	נבחן את החלוקה $\{X < 0\}, \{0 \le X < a\}, \{X \ge a\}$, זוהי חלוקה של $\Omega$, נסמן אותם גם ב־$A_0, A_1, A_2$ בהתאמה.
	\[
		\EE(X) = \overbrace{\EE(X 1_{A_0})}^{=0} + \EE(X 1_{A_1}) + \EE(X 1_{A_2})
	\]
	נוכל לקבל תוצאה דומה עם נוסחת התוחלת השלמה. \\*
	המחובר השני הוא אי־שלילי מההגדרות שהנחנו, והמחובר השלישי מקיים
	\[
		\PP(X \ge a \mid X \ge a) = 1
		\implies \EE(X \mid X \ge a) \ge \EE(a \mid X \ge a) = a
	\]
	ולכן חסום על־ידי $\PP(a \in X) a$.
\end{proof}

\subsection{שימושים של אי־שוויון מרקוב}
\begin{example}
	נניח ש־$X \sim Geo(\frac{1}{2})$, אז $\EE(X) = 2$, ובהתאם $\PP(X \ge 4) \le \frac{\EE(X)}{4} = \frac{2}{4} = \frac{1}{2}$. \\*
	נוכל לחשב את ההסתברות עצמה על־ידי $\PP(X \ge 4) = \sum_{k = 4}^{\infty} \PP(X = k) = \sum_{k = 4}^{\infty} \frac{1}{2^k} = \frac{1}{8}$. \\*
	קיבלנו שהחסם שנובע מאי־שוויון ברקוב לא מאוד מועיל לנו.

	אם $Y = X - 1$ ואנו מחפשים את $Y \ge 0$ אז $\EE(Y) = 1$. \\*
	במקרה זה $\PP(X \ge 4) = \PP(Y \ge 3) \le \frac{\EE(Y)}{3} = \frac{1}{3}$. \\*
	אז קיבלנו חסם יותר טוב לערך, זאת אומרת שיש לנו דרך נוספת להשתמש באי־השוויון.
\end{example}
\begin{example}
	אם $X \sim Po(\lambda)$ ואנו מחפשים את $\PP(X \ge 1)$. \\*
	אנו כבר יודעים ש־$\EE(X) = \lambda$, ולכן $\PP(X \ge 1) \le \frac{\EE(X)}{1} = \lambda$. \\*
	אם $\lambda \ge 1$ אז מצאנו שהחסם הוא 1, והוא לא ממש מועיל לנו. \\*
	מצד שני $\PP(X \ge 1) = 1 - \PP(X = 0) = 1 - e^{-\lambda} \frac{\lambda^k}{k!} = 1 - e^{-\lambda}$. \\*
	לכן $1 - e^{-\lambda} \le \lambda$ ו־$1 - \lambda \le e^{-\lambda}$.
	באופן כללי ראינו שהחסם אכן די מדויק עבור מקרים רבים של ערכי $\lambda$.
\end{example}
\begin{example}
	$n$ מכתבים מגיעים ל־$n$ תיבות דואר ובוחרים תמורה מקרית על $[n]$. \\*
	נבחן את $X$ מספר נקודות השבת של התמורה.
	\[
		X = \sum_{i = 1}^{n} 1_{A_i}
	\]
	כאשר $A_i$ המאורע של נקודת שבת של התמורה ב־$i$, כלומר $\sigma(i) = i$. \\*
	נחשב גם $\EE(X) = \sum_{i = 1}^{n} \EE(1_{A_i}) = \sum_{i = 1}^{n} \PP(A_i)$ ולכן $\PP(X \ge a) \le \frac{1}{a}$.
\end{example}
\begin{example}
	נחזור לפרדוקס יום ההולדת, $X_i \sim U([n])$ בלתי־תלויים עבור $i \in [k]$, כאשר המשתנה המקרי מייצג את הסיכוי שלאדם ה־$i$ יש יום הולדת. \\*
	$X$ הוא מספר ימי ההולדת המשותפים,
	\[
		X = \sum_{1 \le i < j \le k} 1_{\{X_i = X_j\}}
	\]
	ונוכל גם לכתוב
	\[
		\PP(X_i = X_j)
		= \sum_{l = 1}^m \PP(X_i = l, X_j = l)
		= \sum_{l = 1}^m \PP(X_i = l) \PP(X_j = l)
		= \sum_{l = 1}^m \frac{1}{m} \cdot \frac{1}{m}
		= \frac{1}{m}
	\]
	ונובע
	\[
		\EE(X)
		= \sum_{1 \le i < j \le k} \PP(X_i = X_j)
		= \binom{k}{2} \frac{1}{m}
	\]
	ולבסוף
	\[
		\PP(X \ge 1) \le \frac{\EE(X)}{1} = \binom{k}{2} \frac{1}{m}
	\]
	זוהי הכללה של חסם האיחוד.
\end{example}
\begin{example}
	יהיו $A_1, \dots, A_N$ קבוצות, $|A_i| \ge n$ ו־$N < 2^{n - 1}$, אז קיימת קבוצה $B$ כך ש־$B \cap A_i \ne \emptyset$ ו־$\forall 1 \le i \le N, A_i \not\subseteq B$.

	נגדיר $A = \bigcup_{i = 1}^N A_i$.
	יהיו $X_a \sim Ber(\frac{1}{2})$ בלתי תלויים לכל $a \in A$ ונגדיר $B = \{ a \mid X_a = 1 \}$. \\*
	נחשב את ההסתברות $\PP(A_i \subseteq B) = \PP(\forall a \in A_i, X_a = 1) = {(\frac{1}{2})}^{|A_i|} \le \frac{1}{2^n}$. \\*
	מצד שני $\PP(A_i \cap B \ne \emptyset) = \PP(\forall a \in A_i, X_a = 0) = {(\frac{1}{2})}^{|A_i|} \le \frac{1}{2^n}$.
	אז
	\[
		\PP(\exists 1 \le i \le N, A_i \subseteq B \lor A_i \cap B = \emptyset)
		\le \sum_{i = 1}^{N} \PP(A_i \subseteq B) + \PP(A_i \cap B = \emptyset)
		\le N \cdot (\frac{1}{2^n} + \frac{1}{2^n})
		= \frac{N}{2^{n - 1}}
		< 1
	\]
	ולכן קיימת $B$ כזאת.
\end{example}

\section{שיעור 15 --- 17.12.2024}

\subsection{נוסחה לתוחלות}
נתחיל בנוסחה קטנה שתעזור לנו לפתח אינטואיציה, אך לא נשתמש בה רבות.
\begin{proposition}[נוסחת הזנב לתוחלת]
	אם $X$ משתנה מקרי שנתמך על־ידי $\NN \cup \{0\}$ אז
	\[
		\EE(X) = \sum_{n \in \NN} \PP(X \ge n)
	\]
\end{proposition}
\begin{proof}
	ממשפט פוביני לסכומים מרובים
	\[
		\EE(X)
		= \sum_{n = 1}^\infty n \PP(X = n)
		= \sum_{n = 1}^\infty \left( \sum_{k = 1}^n \PP(X = n) \right)
		= \sum_{\substack{n, k \in \NN \\ k \le n}} \PP(X = n)
		= \sum_{k = 1}^\infty \sum_{n = k}^\infty \PP(X = n)
		= \sum_{k = 1}^\infty \PP(X \ge k)
	\]
\end{proof}

\subsection{שונות}
באנגלית Variance.
אנו רוצים לשאול את השאלה כמה הסתברות רחוקה בעצם מהתוחלת, כך שנוכל לאפיין את שתי התכונות באופן מוצלח יותר אחת על־ידי השנייה.
לדוגמה אם $\EE(X) = \EE(Y) = 5$ אבל $X = 5$ כמעט תמיד ומצד שני $\PP(Y = 5000000) = \frac{1}{10^6}$ ו־$\PP(Y = 0) = 1 - \frac{1}{10^6}$ בעלות תוחלת זהה אבל בבירור שונות בתכלית.
\begin{definition}[שונות]
	$X$ משתנה מקרי בעל תוחלת $\mu = \EE(X)$, אז ה\textbf{שונות} של $X$ היא
	\[
		\var(X) = \EE({(X - \mu)}^2)
	\]
\end{definition}
\begin{example}
	במקרה שראינו זה עתה
	\[
		\var(X) = \EE({(X - 5)}^2) = 0
	\]
	בעוד שמתקיים
	\[
		\var(Y) = \EE({(Y - 5)}^2) = {(5000000 - 5)}^2 \cdot \frac{1}{10^6} + {(0 - 5)}^2 \cdot (1 - \frac{1}{10^6}) \approx 25000000
	\]
	כפי שאנו רואים, הפעם השונות מייצגת את ההבדל המשמעותי שבין שני המשתנים המקריים.
\end{example}
נוסיף הגדרה שלא נעסוק בה אך שרבים מאיתנו שמעו בעבר, והוא מושג סטיית התקן, מושג שמשמש רבות בסטטיסטיקה.
\begin{definition}[סטיית תקן]
	סטיית התקן של $X$ היא $\sigma(X) = \sqrt{\var(X)}$.
\end{definition}
נראה הגדרה נוספת לשונות שמשומשת אף היא, הגדרה זו שקולה להגדרה שראינו
\begin{definition}[הגדרה שקולה לשונות]
	נגדיר את השונות להיות
	\[
		\var(X) = \EE(X^2) - {(\EE(X))}^2
	\]
\end{definition}
\begin{proof}[הוכחת השקילות]
	נסמן $\mu = \EE(X)$ ולכן מתכונות התוחלת
	\[
		\EE({(X - \mu)}^2)
		= \EE(X^2 - 2X\mu + \mu^2)
		= \EE(X^2) - \EE(2X\mu) + \EE(\mu^2)
		= \EE(X^2) - 2\mu \EE(X) + \mu^2
		= \EE(X^2) - 2\mu^2 + \mu^2
	\]
	ומצאנו כי מתקיים השוויון שחיפשנו.
\end{proof}
\begin{proposition}[תכונות של שונות]
	כלל התכונות הבאות מתקיימות עבור $X$ משתנה מקרי בעל תוחלת:
	\begin{enumerate}
		\item $\var(X) \ge 0$ ו־$\var(X) = 0$ אם $X$ קבוע כמעט תמיד. השונות היא חיובית ואם המשתנה המקרי קבוע, אז השינוי שהיא מייצגת הוא אפס סביב התוחלת, היא גם ההסתברות.
		\item $\var(X) = \var(X + a)$ לכל $a \in \RR$. השונות לא מושפעת מהזהה, היא תכונה כללית.
		\item $\var(aX) = a^2 \var(X)$. מתיחה של המשתנה המקרי מגדילה אפילו יותר את השונות, נבחין כי השונות מייצגת את הטווח סביב התוחלת, ונוכל להסתכל עליה כשטח של איזשהו רדיוס סביב התוחלת, ככה נקבל את הריבוע.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item ${(X - \mu)}^2 \ge 0$ ולכן גם $\EE({(X - \mu)}^2) \ge 0$. גם
			\[
				X - \mu \overset{a.s.}{=} 0 \iff {(X - \mu)}^2 \overset{a.s.}{=} 0 \iff \EE({(X - \mu)}^2) = 0
			\]
		\item נגדיר $Y = X + a$ ולכן $\EE(Y) = \mu + a$, ואז
			\[
				\var(Y) = \EE({(Y - (\mu + a))}^2) = \EE({((X + a) - (\mu + a))}^2) = \EE({(X - \mu)}^2) = \var(X)
			\]
		\item נגדיר $Y = aX$ ובהתאם $\EE(Y) = \EE(aX) = a \EE(X) = a \mu$, ולכן
			\[
				\var(Y) = \EE({(Y - a \mu)}^2) = \EE({(aX - a\mu)}^2) = a^2 \EE({(X - \mu)}^2) = a^2 \var(X)
			\]
	\end{enumerate}
\end{proof}
נבחין כי בעוד שאנו יודעים כי תוחלות הן לינאריות, זהו לא המקרה עבור שונות, ננסה לחשב שונות של סכום משתנים מקריים.
נחשב את $\var(X + Y)$ עבור $\mu = \EE(X), \nu = \EE(Y)$, אז $\EE(X + Y) = \mu + \nu$. נעבור לחישוב השונות
\begin{align*}
	\var(X + Y)
	& = \EE({((X + Y) - (\mu + \nu))}^2) \\
	& = \EE({((X - \mu) + (Y - \nu))}^2) \\
	& = \EE({(X - \mu)}^2 + 2 (X - \mu)(Y - \nu) + {(Y - \nu)}^2) \\
	& = \EE({(X - \mu)}^2) + 2 \EE((X - \mu)(Y - \nu)) + \EE({(Y - \nu)}^2) \\
\end{align*}
ניתן שם לביטוי לחלק הביטוי שיצא לנו, ונגדיר
\begin{definition}[שונות משותפת]
	נגדיר
	\[
		\cov(X, Y) = \EE((X - \mu)(Y - \nu))
	\]
	עבור $\EE(X) = \mu, \EE(Y) = \nu$.
\end{definition}
כאשר $\cov$ הוא קיצור ל־Covariance, הוא בתורו קיצור למילה Cooperative Variance.
ולכן נוכל לקבל
\begin{conclusion}
	עבור משתנים מקריים $X, Y$ בעלי תוחלת, מתקיים
	\[
		\var(X + Y) = \var(X) + 2 \cov(X, Y) + \var(Y)
	\]
\end{conclusion}
\begin{proposition}
	אם $X$ ו־$Y$ בלתי־תלויים ובעלי שונות, אז $\cov(X, Y) = 0$.
\end{proposition}
\begin{proof}
	נראה בהמשך שאם ל־$X$ ול־$Y$ יש שונות אז $\cov(X, Y)$ מוגדר (כלומר הטור מתכנס בהחלט). \\*
	נניח כרגע שזה נכון ולכן
	\[
		\cov(X, Y)
		= \EE((X - \mu)(Y - \nu))
		\overset{(1)}{=} \EE(X - \mu) \EE(Y - \nu)
		= 0 \cdot 0
		= 0
	\]
	כאשר
	\begin{enumerate}
		\item $X - \mu$ ו־$Y - \nu$ בלתי־תלויים, מאי־התלות של $X, Y$ עצמם.
	\end{enumerate}
\end{proof}
\begin{example}
	אם $X = c$ כמעט תמיד אז $\EE(X) = c$ ו־$\var(X) = 0$.
\end{example}
\begin{example}
	נניח $X \sim Ber(p)$ ולכן $\var(X) = \EE(X^2) - {\EE(X)}^2 = p - p^2 = p(1 - p)$. \\*
	אם $X \sim Ber(p)$ אז $1 - X \sim Ber(1 - p)$ ואז $\var(1 - X) = \var(-X) = {(-1)}^2 \var(X) = \var(X)$. \\*
	זאת אומרת, לא מפתיע שהשונות היא סימטרית במקרה זה עבור שני המשתנים.
\end{example}
\begin{example}
	אם $X \sim Bin(n, p)$, אז נוכל להשתמש ישירות בהגדרת השונות, אבל נשתמש בעובדה שמשתנה בינומי הוא סכום של משתנים ברנולי,
	כלומר $X_i \sim Ber(p)$ בלתי־תלויים עבור $1 \le i \le n$ אז אם $X = \sum_{i = 1}^{n} X_i$ גם $X \sim Bin(n, p)$, ולכן
	\[
		\var(X)
		= \var(\sum_{i = 1}^{n} X_i)
		= \sum_{i = 1}^{n} \var(X_i)
		= n p(1 - p)
	\]
\end{example}
\begin{example}
	נניח עתה $X \sim Poi(\lambda)$, ונחשב על־ידי ההגדרה השקולה והעובדה שאנו כבר יודעים ש־$\EE(X) = \lambda$:
	\begin{align*}
		\EE(X^2)
		& = \sum_{k = 0}^{\infty} k^2 \PP(X = k) \\
		& = \sum_{k = 1}^{\infty} k^2 \frac{e^{-\lambda} \lambda^k}{k!} \\
		& = e^{-\lambda} \sum_{k = 1}^{\infty} \frac{k \lambda^k}{(k - 1)!} \\
		& = e^{-\lambda} \sum_{k = 1}^{\infty} \frac{((k - 1) + 1) \lambda^k}{(k - 1)!} \\
		& = e^{-\lambda} \sum_{k = 1}^{\infty} \frac{(k - 1) \lambda^k}{(k - 1)!} + e^{-\lambda} \sum_{k = 1}^{\infty} \frac{\lambda^k}{(k - 1)!} \\
		& = e^{-\lambda} \lambda^2 \sum_{k = 2}^{\infty} \frac{\lambda^{k - 2}}{(k - 2)!} + e^{-\lambda} \lambda \sum_{k = 1}^{\infty} \frac{\lambda^{k - 1}}{(k - 1)!} \\
		& = \lambda^2 + \lambda
	\end{align*}
	ולכן נסיק $\var(X) = \EE(X^2) - {\EE(X)}^2 = \lambda$.
\end{example}

\section{תרגול 8 --- 19.12.2024}
\subsection{שימושים למשפט מרקוב}
\begin{exercise}
	מטילים מטבע מוטה עם הסתברות $p$ לעץ, 20 פעמים,
	חסמו את ההסתברות שהרצף עץ עץ יצא פחות מפעמיים.
\end{exercise}
\begin{solution}
	נגדיר $\Omega = {\{0, 1\}}^{20}$ וכן $X = 1_{\{\omega_i = \omega_{i + 1} = 1\}}$, וגם $X = \sum X_i$, אנו רוצים לחשב את $\PP(X \le 1)$. \\*
	נבחין כי $X_i \sim Ber(p^2)$, וכן $\EE(X) = \sum \EE(X_i)  = 19 p^2$, שכן $0 \le X \le 19$.
	\[
		\PP(X \le 1)
		= \PP(19 - X \ge 18)
		\le \frac{\EE(19 - X)}{18}
		= \frac{19 - 19p^2}{18}
	\]
\end{solution}

\subsection{שאלות נבחרות בנושא שונות}
נתחיל ונבחין ששונות היא תבנית בי־לינארית (תבנית ריבועית), ובשל כך היא מקיימת את הטענה שהיא אי־שלילית, היא אדישה להזזות קבועות ויש לה כיול ריבועי,
\begin{example}
	אם $X \sim U([6])$ משתנה מקרי להטלת קובייה הוגנת, אז $\EE(X) = 3 \frac{1}{2}$ וכן
	\[
		\var(X) = \frac{6^2 - 1}{12} = \frac{35}{12} \approx 3
	\]
	זאת־אומרת שהשונות באמת מתכתבת עם המרחק של הערכים מהתוחלת.
\end{example}
\begin{exercise}
	יהי $X \sim Geo(q)$, חשבו את $\var(X)$.
\end{exercise}
\begin{solution}
	אנו יודעים ש־$\EE(X) = \frac{1}{q}$. \\*
	עוד אנו יודעים מאנליטיות $\frac{1}{1 - x}$ ופיתוח טיילור, מתקיים
	\[
		\sum_{n = 0}^{\infty} n (n - 1) x^{n - 2} = \frac{2}{{(1 - x)}^3}
	\]
	נשתמש בנוסחה זו ונובע
	\[
		\EE(X(X - 1))
		= \sum_{n = 0}^{\infty} n (n - 1) q {(1 - q)}^{n - 1}
		= q (1 - q) \sum_{n = 0}^{\infty} n (n - 1) {(1 - q)}^{n - 2}
		= q (1 - q) \frac{2}{q^3}
	\]
	ולכן $\EE(X^2) = \frac{2(1 - q)}{q^2} + \frac{1}{q}$.
	נעבור לחישוב השונות
	\[
		\var(X)
		= \EE(X^2) - {\EE(X)}^2
		= \frac{2(1 - q)}{q^2} + \frac{1}{q} - \frac{1}{q^2}
		= \frac{2(1 - q) + q - 1}{q^2}
		= \frac{1 - q}{q^2}
	\]
\end{solution}
ניזכר בשונות משותפת, נבחין כי מבי־לינאריות השונות מתקיים
\begin{align*}
	\var(X + Y)
	& = \cov(X + Y, X + Y) \\
	& = \cov(X, X) + \cov(X, Y) + \cov(Y, X) + \cov(Y, Y) \\
	& = \var(X) + \var(Y) + 2 \cov(X, Y)
\end{align*}
בהתאם גם
\[
	\var(X)
	= \var(\sum_{i = 1}^{n} X_i)
	= \sum_{i, j = 1}^{n} \cov(X_i, X_j)
	= \sum_{i = 1}^{n} \cov(X_i) + 2 \sum_{1 = i < j = n} \cov(X_i, X_j)
\]
\begin{exercise}
	חשבו את השונות ואת התוחלת של מספר נקודות השבת בתמורה מקרית על $\{1, \dots, n\}$.
\end{exercise}
\begin{solution}
	נגדיר $X_i$ נקודת השבת במקום ה־$i$.
	$X_i \sim Ber(\frac{1}{n})$ ו־$X = \sum X_i$.
	בהתאם
	\[
		\EE(X) = n \frac{1}{n} = 1
	\]
	אם $i \ne j$ אז $\PP(X_i, X_j = 1) = \frac{1}{n} \cdot \frac{1}{n - 1} \ne \frac{1}{n^2} = \PP(X_i = 1) \PP(X_j = 1)$, כלומר אנו רואים כי משתנים אלה תלויים.
	\[
		\var(X_i) = \frac{1}{n} (1 - \frac{1}{n})
	\]
	וכן
	\[
		i \ne j
		\implies \cov(X_i, X_j)
		= \EE(X_i X_j) - \EE(X_i) \EE(X_j)
		= \frac{1}{n} \cdot \frac{1}{n - 1} - \frac{1}{n^2}
	\]
	לכן
	\begin{align*}
		\var(X)
		& = \sum_{i = 1}^{n} \frac{1}{n}(1 - \frac{1}{n}) + 2 \sum_{1 = i < j = n} (\frac{1}{n} \cdot \frac{1}{n - 1} - \frac{1}{n^2}) \\
		& = (1 - \frac{1}{n}) + 2 \binom{n}{2} (\frac{1}{n(n - 1)} - \frac{1}{n^2}) \\
		& = (1 - \frac{1}{n}) + 2 \frac{n!}{2! (n - 2)!} (\frac{1}{n(n - 1)} - \frac{1}{n^2}) \\
		& = (1 - \frac{1}{n}) + \frac{n(n - 1)}{1} (\frac{1}{n(n - 1)} - \frac{1}{n^2}) \\
		& = 1 - \frac{1}{n} + 1 - \frac{n - 1}{n} \\
		& = 1
	\end{align*}
\end{solution}

\section{שיעור 16 --- 19.12.2024}
\subsection{שונות --- המשך}
נרחיב את הטענה מההרצאה הקודמת.
\begin{proposition}
	עבור משתנים מקריים $X, Y$ בעלי תוחלת מתקיים
	\[
		\cov(X, Y)
		= \EE((X - \EE(X))(Y - \EE(Y)))
		= \EE(XY) - \EE(X) \EE(Y)
	\]
\end{proposition}
\begin{proof}
	\[
		\EE((X - \EE(X))(Y - \EE(Y)))
		= \EE(XY - \EE(X) Y - \EE(Y) X + \EE(X) \EE(Y))
		= \EE(XY) - \EE(X) \EE(Y) - \EE(Y) \EE(X) + \EE(X) \EE(Y)
	\]
\end{proof}
\begin{remark}
	אם $X$ ו־$Y$ בלתי־תלויים אז $\cov(X, Y) = 0$.
\end{remark}
\begin{definition}[משתנים מקריים בלתי־מתואמים]
	אם מתקיים $\cov(X, Y) = 0$ אז נאמר ש־$X$ ו־$Y$ \textbf{בלתי־מתואמים}.
\end{definition}
נבחין כי זהו תנאי הרבה יותר חלש מאי־תלות.
\begin{proposition}[תכונות של שונות משותפת]
	לכל שני משתנים מקריים $X, Y$ בעלי־תוחלת מתקיימות התכונות הבאת
	\begin{enumerate}
		\item $\cov(X, Y) = \cov(Y, X)$
		\item $\cov(a + X, Y) = \cov(X, Y)$ 
		\item $\cov(aX, Y) = a \cdot \cov(X, Y)$
		\item $\var(X) = \cov(X, X)$
		\item $\cov(X + Y, Z) = \cov(X, Z) + \cov(Y, Z)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	התכונה הראשונה טריוויאלית ונובעת מההגדרה, ולכן נוכיח את התכונה השנייה.
	\[
		\EE(((a + X) - \EE(a + X))(Y - \EE(Y)))
		= \EE((X - \EE(X))(Y - \EE(Y)))
	\]

	נוכיח את התכונה החמישית.
	\begin{align*}
		\EE(((X + Y) - \EE(X + Y))(Z - \EE(Z)))
		& = \EE(((X - \EE(X)) + (Y - \EE(Y)))(Z - \EE(Z))) \\
		& = \EE((X - \EE(X))(Z - \EE(Z))) + \EE((Y - \EE(Y))(Z - \EE(Z)))
	\end{align*}
\end{proof}
\begin{proposition}
	נניח ש־$X_1, \dots, X_n$ משתנים מקריים בעלי תוחלת, אז
	\[
		\var(\sum_{i = 1}^{n} X_i)
		= \sum_{i = 1}^{n} \var(X_i) + 2 \sum_{1 \le i < j \le n} \cov(X_i, Y_i)
	\]
\end{proposition}
\begin{proof}
	\begin{align*}
		\var(\sum_{i = 1}^{n} X_i)
		& = \cov(\sum_{i = 1}^{n} X_i, \sum_{i = 1}^{n} X_i) \\
		& = \sum_{i = 1}^n \cov(X_i, \sum_{i = 1}^{n} X_i) \\
		& = \sum_{i = 1}^n \sum_{j = 1}^n \cov(X_i, X_j) \\
		& = \sum_{i = 1}^{n} \var(X_i) + 2 \sum_{1 \le i < j \le n} \cov(X_i, Y_i)
	\end{align*}
\end{proof}
\begin{example}
	מטילים מטבע הוגן $n$ פעמים וסופרים את מספר ה־$HH$, נחשב את התוחלת ואת השונות.

	נגדיר ${\{X_i\}}_{i = 1}^n$ משתנים מקריים $Ber(\frac{1}{2})$ בלתי־תלויים, ונגדיר $Y_i = X_i + X_{i + 1}$ עבור $i = 1, \dots, n - 1$.
	נגדיר גם $Y = \sum_{i = 1}^{n - 1} Y_i$.
	\[
		\EE(Y_i) = \EE(X_i) \EE(X_{i + 1}) = \frac{1}{4},
		\qquad
		\EE(Y) = \sum_{i = 1}^{n - 1} \EE(Y_i) = \frac{n - 1}{4}
	\]
	וכן
	\[
		\var(Y)
		= \var(\sum_{i = 1}^{n - 1} Y_i)
		= \sum_{n = 1}^{n - 1} \var(Y_i) + 2 \sum_{1 \le i < j \le n - 1} \cov(Y_i, Y_j)
	\]
	ונעבור לחישוב אלה האחרונים.
	\[
		\var(Y_i) = \frac{3}{16}
	\]
	נבחן דוגמה ספציפית למקרה שהמשתנים שונים,
	\[
		\cov(Y_1, Y_7)
		= \cov(X_1 X_2, X_7 X_8)
	\]
	לכל $i + 1 < j$ המשתנים $Y_i = X_i X_{i + 1}$ ו־$Y_j = X_j X_{j + 1}$ בלתי־תלויים (בתור פונקציות מעל משתנים מקריים בקבוצות זרות), לכן $\cov(Y_i, Y_j) = 0$. \\*
	נשאר המקרה $j = i + 1$.
	\[
		\forall 1 \le i \le n - 2, \cov(Y_i, Y_{i + 1})
		= \cov(X_i X_{i + 1}, X_{i + 1} X_{i + 2})
		= \EE(X_i X_{i + 1}^2 X_{i + 2}) - \EE(Y_i)\EE(Y_{i + 1})
		= \frac{1}{8} - \frac{1}{16}
		= \frac{1}{16}
	\]
	לבסוף $\var(Y) = (n - 1) \frac{3}{16} + 2(n - 2) \frac{1}{16} = o(n)$.
\end{example}
ניזכר באי־שוויון מרקוב\ \ref{markov_inequality} ונגדיר אי־שוויון חדש
\begin{theorem}[אי־שוויון צ'בישב]
	נניח ש־$X$ משתנה מקרי בעל שונות (ולכן בעל תוחלת), אז
	\[
		\forall \lambda > 0,\ 
		\PP(|X - \EE(X)| \ge \lambda) \le \frac{\var(X)}{\lambda^2}
	\]
\end{theorem}
\begin{proof}
	נשים לב ש־$\{ |X - \EE(X)| \ge \lambda \} = \{ {(X - \EE(X))}^2 \ge \lambda^2 \}$. \\*
	נגדיר $Y = {(X - \EE(X))}^2$ ואז $0 \le Y$ וכן $\EE(Y) = \EE({(X - \EE(X))}^2) = \var(X)$, ולכן מ־\ref{markov_inequality}
	\[
		\PP(Y \ge \lambda^2) \le \frac{\var(X)}{\lambda^2}
	\]
\end{proof}
\begin{example}
	מטילים מטבע הוגן $10^6$ פעמים.
	$X$ מספר העצים ואנו רוצים לדעת מה ההסתברות ש־$495000 < X < 505000$. אנו יודעים ש־$X \sim Bin(10^6, \frac{1}{2})$.
	\begin{align*}
		\PP(495000 < X < 505000)
		& = 1 - \PP(X \le 495000 \lor X \ge 505000) \\
		& = 1 - \PP(|X - 500000| \ge 5000) \\
		& \ge \frac{\var(X)}{5000^2}
	\end{align*}
	אנו גם יודעים שמתקיים
	\[
		X = \sum_{i = 1}^{10^6} X_i
		\implies
		\var(X) = \sum_{i = 1}^{10^6} \var(X_i) = 10^6 \cdot \frac{1}{4}
	\]
	ולכן
	\[
		\PP(|X - 500000| \ge 5000)
		\le \frac{\var(X)}{5000^2}
		= \frac{10^6 \cdot \frac{1}{4}}{5000^2}
		= \frac{1}{100}
	\]
	ולכן ההסתברות הזאת גדולה מ־$0.99$, זאת־אומרת שמצאנו חסם מאוד טוב למספר ההטלות שקיבלו עץ באופן יחסי.
\end{example}
\begin{example}
	אם נחזור לדוגמה איתה פתחנו את ההרצאה, אז נוכל לקבוע
	\[
		\var(Y_n) < 100n
		\qquad
		\EE(Y_n) = \frac{n - 1}{4}
	\]
	אז
	\[
		\PP\left(\left\lvert\frac{Y_n}{\frac{n - 1}{4}} - 1\right\rvert \ge \epsilon\right)
		\le \frac{\var(\frac{Y_n}{\frac{n - 1}{4}})}{\epsilon^2}
		= \frac{\var(Y_n)}{\epsilon^2 {(\frac{n - 1}{4})}^2}
		\le \frac{o(n)}{\frac{\epsilon^2}{16} {(n - 1)}^2}
		= o(\frac{1}{n})
	\]
\end{example}
\begin{theorem}[החוק החלש של המספרים הגדולים]
	תהי $X_1, X_2, \dots$ סדרת משתנים מקריים בלתי־תלויים שווי התפלגות ובעלי תוחלת $\mu$. \\*
	אם $Y_n = \frac{\sum_{i = 1}^{n} X_i}{n}$, אז לכל $\epsilon > 0$
	\[
		\PP(|Y_n - \mu| \ge \epsilon) \xrightarrow[n \to \infty]{} 0
	\]
\end{theorem}
\begin{proof}
	נוכיח בהנחת קיום שונות, כאשר ניתן להוכיח גם ללא הנחה זו.
	\[
		\EE(Y_n)
		= \frac{\EE(\sum_{i = 1}^{n} X_i)}{n}
		= \frac{\sum_{i = 1}^{n} \EE(X_i)}{n}
		= \mu
	\]
	ולכן
	\[
		\PP(|Y_n - \mu|)
		\le \frac{\var(Y_n)}{\epsilon^2}
		= \frac{\var(\frac{\sum_{i = 1}^{n} X_i}{n})}{\epsilon^2}
		= \frac{\var(\sum_{i = 1}^{n} X_i)}{n^2 \epsilon^2}
		= \frac{n\var(X_1)}{n^2 \epsilon^2}
		\xrightarrow[n \to \infty]{} 0
	\]
\end{proof}

\section{שיעור 17 --- 31.12.2024}
\subsection{בעיית אספן הקופונים}
\begin{exercise}[אספן הקופונים]
	יהי אספן קופונים אשר מקבל כל יום קופון כלשהו מבין מספר קופונים אפשריים, מה החסם שמעיד שהאספן השיג את כל הקופונים? 
\end{exercise}
\begin{solution}
	נגדיר $X_1, \dots, X_m$ משתנים מקריים בלתי־תלויים המתפלגים אחיד $U([n])$, כלומר $m$ מספר סוגי הקופונים ו־$n$ מספר השליפות, ואנו רוצים לחסום את $\PP(\forall k \in [n], \exists i \in [m], X_i = k)$.
	נבחן את המאורע המשלים, $\PP(\exists k \in [m], \forall i \in [n], X_i \ne k) = \PP(\bigcup_{k = 1}^n A_k)$ כאשר האיחוד איננו זר ו־$A_k = \forall i \in [m], X_i \ne k$, אפשר לחסום זאת עם חסם האיחוד,
	\[
		\PP(\bigcup_{k = 1}^n A_k)
		\le \sum_{k = 1}^{n} \PP(A_k)
		= \sum_{k = 1}^{n} {(1 - \frac{1}{n})}^m
		= n {(1 - \frac{1}{n})}^m
	\]
	אנו רוצים להבין מתי הביטוי שהתקבל שואף ל־$0$, נשים לב שידוע כי $\forall x \in \RR, 1 + x \le e^x$, ולכן
	\[
		n {(1 - \frac{1}{n})}^m
		\le n {(e^{-\frac{1}{n}})}^m
		= n e^{-\frac{m}{n}}
	\]
	אז אם $m = \lceil cn \log n \rceil$ ו־$c > 1$ אז
	\[
		n e^{-\frac{m}{n}}
		= n e^{-c \log n}
		= n n^{-c}
		= n^{1 - c}
		\xrightarrow[n \to \infty]{} 0
	\]
	נבחין כי חסם האיחוד הוא מקרה פרטי של אי־שוויון מרקוב.
	נעבור לחישוב של תוחלת ושונות של כמה קופונים האספן לא השיג,
	נסמן ב־$Y$ את מספר הקופונים החסרים, וכן $Y = \sum_{k = 1}^{n} Y_n$ כאשר $Y_k = 1_{A_k}$.
	מהחישובים שעשינו עד כה נוכל להסיק
	\[
		\EE(Y) = n {(1 - \frac{1}{n})}^m
	\]
	שכן מאי־שוויון מרקוב $\PP(Y \ge 1) \le \frac{\EE(Y)}{1}$.
	נראה שבהצבה של $m$ שקטן ממה שמצאנו (יחד עם $c < 1$) חסמנו מלמעלה את $\PP(Y = 0)$. נעבור לחישוב השונות של $Y$,
	\[
		\var(Y_k)
		= {(1 - \frac{1}{n})}^m (1 - {(1 - \frac{1}{n})}^m)
		\le {(1 - \frac{1}{n})}^m
		= \EE(Y_k)
	\]
	וכן
	\begin{align*}
		k \le l,
		\cov(Y_k, Y_l)
		& = \EE(Y_k \cdot Y_l) - \EE(Y_k) \EE(Y_l) \\
		& = {(1 - \frac{2}{n})}^m - {(1 - \frac{1}{n})}^{2m} \\
		& = {(1 - \frac{2}{n})}^m - {({(1 - \frac{1}{n})}^{2})}m \\
		& = {(1 - \frac{2}{n})}^m - {({(1 - \frac{2}{n} + \frac{1}{n^2})}^{2})}m \\
		& \le 0
	\end{align*}
	ולכן $\var(Y) \le n {(1 - \frac{1}{n})}^m = \EE(Y)$ (מחקנו את האיברים השליליים) ובהתאם
	\[
		\frac{\var(Y)}{{(\EE(Y))}^2}
		\le \frac{1}{\EE(Y)}
	\]
	נשאר למצוא מתי $EE(Y) \to \infty$,
	עבור $m = cn \log n$ כאשר $1 > c$ נובע
	\[
		\log(n {(1 - \frac{1}{n})}^m)
		= \log(n) + m \log(1 + \frac{1}{n})
		= \log(n) + cn \log(1 - \frac{1}{n}) \to \infty
	\]
	כאשר את המקרה $c = 1$ אין לנו היכולת להראות.
\end{solution}

\subsection{בין הסתברות ללינארית}
\begin{proposition}
	$X$ משתנה מקרי בעל שונות ו־$f(a) = \EE({(X - a)}^2)$, אז $f$ מקבלת מינימום ב־$a = \EE(X)$.
\end{proposition}
\begin{proof}
	\[
		f(a)
		= \EE({(X - a)}^2)
		= \EE(X^2 0 2a X + a^2)
		= \EE(X^2) - 2a \EE(X) + a^2
	\]
	ולכן
	\[
		f'(a)
		= -2 \EE(X) + 2a
	\]
	ונובע ש־$f'(\EE(X)) = 0$.
\end{proof}
אנו נתקלים בקושי של הקשר בין משתנים מקריים, תוחלת ושונות עם אלגברה לינארית.
\begin{definition}[קבוצת כל המשתנים המקריים]
	נגדיר את $L_2$ להיות קבוצת כל המשתנים המקריים (במרחב הסתברות כלשהו) בעלי שונות,
	כאשר אנו מזהים משתנים מקריים ששווים כמעט תמיד.
\end{definition}
\begin{proposition}
	$L_2$ הוא מרחב מכפחה פנימית עם $\langle X, Y \rangle = \EE(XY)$.
\end{proposition}
לפני שניגש להוכחה נבחן דוגמה שתבהיר לנו את הטענה.
\begin{example}
	נגדיר $\Omega = [n]$ עם הסתברות אחידה, אז אם $X$ משתנה מקרי אז $X : \Omega \to \Omega$, כלומר $L_2 = \RR^\Omega$.
	מתקיים
	\[
		\langle X, Y \rangle
		= \sum_{\omega \in \Omega} \PP(\{\omega\}) X(\omega) Y(\omega)
		= \sum_{k = 1}^{n} \frac{1}{n} X(k) Y(k)
	\]
	וזו אכן מכפלה פנימית.
\end{example}
נעבור להוכחה.
\begin{proof}
	מהגדרת התוחלת של $\EE(XY)$ קל להוכיח כי $\langle aX, Y \rangle = a \langle X, Y \rangle$ וכן ש־$\langle X + Y, Z \rangle = \langle X, Z \rangle + \langle Y, Z \rangle$,
	ואף $\EE(X^2) = \langle X, X \rangle \iff X = 0$.
	אם מצטמצמים לתת־מרחב של המשתנים המקריים של $\EE(X) = 0$ אז $\langle X, Y \rangle = \cov(X, Y)$ ו־$\lVert X \rVert = \var(X)$.
	נשים לב כי הפעולה של התוחלת היא פונקציה ולכן $\EE : L_2 \to \RR$ לינארית ונחשוב על $\RR$ כ־$K$ כאשר $K$ כל המשתנים המקריים הקבועים.
	נזהה את התוחלת על־ידי הטלה אורתוגונלית. כלומר $\EE(X)$ אורתוגונלי ל־$X - \EE(X)$ וזה ברור כי $\langle X, X - \EE(X) \rangle = \EE((X - \EE(X)) \EE(X)) = 0$. \\
	את ההוכחה הפורמלית נעשה עם משפט קושי־שוורץ.
	לכל $X, Y \in L_2$, נראה
	\[
		|\EE(XY)|
		\le \sqrt{\EE(X^2) \EE(Y^2)}
	\]
	נגדיר
	\[
		\overline{X} = \frac{X}{\sqrt{\EE(X^2)}},
		\qquad
		\overline{Y} = \frac{Y}{\sqrt{\EE(X^2)}}
	\]
	ואז $\EE(\overline{X}^2) = \EE(\overline{Y}^2) = 0$.
	ניזכר שאם $X, Y$ אי־שליליים אז $0 \le XY \le \frac{X^2 + Y^2}{2}$ ונניח שהמשתנים המקריים שלנו אי־שליליים.
	אז
	\[
		\EE(\overline{X} \overline{Y})
		\le \EE(\frac{\overline{X}^2 + \overline{Y}^2}{2})
		= \frac{1}{2}(\EE(\overline{X}^2) + \EE(\overline{Y}^2))
		= 1
	\]
\end{proof}

\section{תרגול 9 --- 2.1.2025}
\subsection{תרגילים שונים בנושא שונות}
\begin{example}
	סופר שואל סטודנטים בקמפוס אם הם קראו ספר שלו או לא. \\
	אחוז הסטודנטים שקראו את הספר הוא $q$.
	אנו רוצים למצוא את $q$ על־ידי שאילת רק חלק מהסטודנטים. \\
	נניח שדגמנו $n$ סטודנטים (מספר הסטודנטים הכללי לא ידוע), נסמן ב־$X$ את מספר הסטודנטים שענו שהם קראו.
	נמצא כמה סטודנטים אנו צריכים לשאול כדי שיתקיים
	\[
		\PP(| \frac{X}{n} - q| \ge 0.1) < 0.05
	\]
	נניח ש־$X \sim Bin(n, q)$ (זוהי הנחה מסטטיסטיקה שלא נעסוק בה).
	לכן
	\[
		\EE(X) = nq,
		\EE(\frac{X}{n}) = q,
		\var(X) = nq(1 - q) \le n \frac{1}{4}
	\]
	בהתאם
	\[
		\var(\frac{X}{n}) \le \frac{1}{4n}
	\]
	מאי־שוויון צ'בישב
	\[
		\PP(|\frac{X}{n} - q| \ge 0.1) <\frac{4n}{0.01} = \frac{25}{n} < \frac{5}{100}
		\iff 500 < n
	\]
\end{example}
\begin{exercise}
	הזמן (בחודשים) עד שתנור חדש מתקלקל מתפלג $X$ עם תוחלת $\mu$ ושונות $\sigma^2$. \\
	חברת ביטוח רוצה להציע אחריות לתנורים, היא חישבה שעל־מנת להרוויח היא צריכה שיחס מימושי הביטוח תהיה לכל היותר $p$. \\
	כמה חודשי ביטוח על החברה להציע?
\end{exercise}
\begin{solution}
	אנו מחפשים את $\PP(X < M) < p$ עבור $M$ מספר חודשים. נעשה שינוי ונכתוב $\{X < \mu - k\} \subseteq \{ |X - \mu| > k \}$.
	צ'בישב
	\[
		\PP(X < \mu - k)
		\le \PP(|X - \mu| \ge k)
		< \frac{\var(X)}{k^2}
		= \frac{\sigma^2}{k^2}
		< p
	\]
	ולכן
	\[
		k > \frac{\sigma}{\sqrt{p}}
	\]
	ועל החברה להציע ביטוח לכל היותר ל־$\mu - \frac{\sigma}{\sqrt{p}}$ חודשים.
\end{solution}
\begin{exercise}
	מגרילים מטריצה $n \times n$ ממקדמים ב־$\ZZ_{/2}$ על־ידי זה שמגרילים מטבע בכל כניסה באופן בלתי תלוי. \\
	חשבו את התוחלת והשונות של מספר תתי־המטריצות $2 \times 2$ מהצורה $\begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$.
\end{exercise}
\begin{solution}
	נסמן $X_{i, j}$ משתנה מקרי ברנולי לפי האם הכניסה ה־$i, j$ היא $1$ ו־$Y_{i, j}$ ברנולי של האם התת־מטריצה בגודל שתיים במקום ה־$i, j$ היא מטריצת יחידות כרצוי.
	נבחין שמתקיים
	\[
		Y_{i, j} = X_{i, j} X_{i + 1, j} X_{i, j + 1} X_{i + 1, j + 1}
	\]
	נסמן
	\[
		Y = \sum_{1 \le i, j \le n - 1} Y_{i, j}
	\]
	ונעבור לחישוב הערכים,
	\[
		\EE(Y)
		= {(n - 1)}^2 \frac{1}{2^2}
	\]
	וכן עבור השונות
	\begin{align*}
		\var(Y)
		& = \cov(Y, Y) \\
		& = \sum_{1 \le i, j \le n - 1} \cov(Y_{i, j}, Y_{i, j}) \\
		& = \sum_{1 \le i, j \le n - 1} \var(Y_{i, j})
		+ \sum_{\substack{1 \le i \le n - 1 \\ 1 \le j \le n - 2}} \cov(Y_{i, j}, Y_{i + 1, j})
		+ \cdots
		+ \sum_{1 \le j \le n - 2} \cov(Y_{i, j}, Y_{i + 1, j + 1})
		+ \sum_{\substack{1 \le i, j \le n - 1 \\ |i - j| \ge 2}} 0
	\end{align*}
	ויש לנו שלושה סוגי חפיפה שנוספים אף הם ולא כתבנו, עתה נוכל לעבור לחישוב החלקים השונים ביתר קלות.
	לדוגמה
	\[
		\cov(Y_{i, j}, Y_{i, j + 1})
		= \EE(X_{i, j} X_{i + 1, j} X_{i, j + 1} X_{i + 1, j + 1} X_{i + 1, j + 1} X_{i + 2, j + 1} X_{i + 1, j + 2} X_{i + 2, j + 2}) - \EE(Y_{i, j}) \EE(Y_{i, j + 1})
		= \frac{1}{2^6} - \frac{1}{2^8}
	\]
	ולכן
	\[
		\var(Y)
		= {(n - 1)}^2 (\frac{1}{2^4} - \frac{1}{2^8})
		+ 4(n - 1)(n - 2) (\frac{1}{2^6} - \frac{1}{2^8})
		+ 4 {(n - 2)}^2 {(n - 1)}^2 (\frac{1}{2^7} - \frac{1}{2^8})
	\]
\end{solution}

\section{שיעור 18 --- 2.1.2025}
\subsection{מומנטים גבוהים}
\begin{definition}[מומנט]
	המומנט ה־$k$ של $X$ הוא $\EE(X^k)$.
\end{definition}
\begin{definition}[מומנט מרכזי]
	המומנט המרכזי הוא $\EE({(X - \EE(X))}^k)$.
\end{definition}
\begin{proposition}[צ'בישב מוכלל]
	\[
		\PP(|X - \EE(X)| \ge \lambda) \le \frac{\EE({(X - \EE(X)))}^k}{\lambda^k}
	\]
	לכל $k$ זוגי.
\end{proposition}
ההוכחה מאוד דומה להוכחה של אי־השוויון במקרה הרגיל.
\begin{example}
	מטילים מטבע הוגן $n$ פעמים ואנו רוצים לחסום את ההסתברות שקיבלנו יותר מ $\frac{3}{4}n$ עצים. \\
	מגדירים $X_1, \dots, X_n \sim Ber(\frac{1}{2})$ וכן $X = \sum_{i = 1}^n X_i$ ולכן
	\[
		\PP(X \ge \frac{3}{4}n)
		\le \PP(|X - \frac{n}{2}| \ge \frac{n}{4})
		\le \frac{\var(X)}{{(\frac{n}{4})}^2}
	\]
	אבל מ־$X \sim Bin$ נסיק
	\[
		\var(X) = \frac{n}{4}
	\]
	ולכן
	\[
		\frac{\var(X)}{{(\frac{n}{4})}^2}
		= \frac{\frac{n}{4}}{{(\frac{n}{4})}^2}
		= \frac{4}{n}
	\]
	ננסה להשתמש בנוסחה החדשה.
	\[
		\EE({(X - \frac{n}{2})}^4)
		= \EE({(\sum_{i = 1}^{n}  (X_i - \frac{1}{2}))}^4)
	\]
	נגדיר $Y_i = X_i - \frac{1}{2}$ ולכן
	\[
		\EE({(X - \frac{n}{2})}^4)
		= \EE({(\sum_{i = 1}^{n} Y_i)}^4)
		= \EE(\sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k = 1}^{n} \sum_{l = 1}^{n} Y_i Y_j Y_k Y_l)
		= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{k = 1}^{n} \sum_{l = 1}^{n} \EE(Y_i Y_j Y_k Y_l)
	\]
	במצב הרגיל אנו יכולים לחלק למקרים עבור אינקסים זהים ושונים, הפעם יש לנו סוגי התלכדות שונים, נחשב לדוגמה את המקרה הזר, נניח ש$i, j, k, l$ שונים ולכן
	\[
		\EE(Y_i Y_j Y_k Y_l) = 0
	\]
	למעשה מספיק שאחד מהם יהיה שונה מכל השאר, לדוגמה $i \ne j, k, l$ אז
	\[
		\EE(Y_i Y_j Y_k Y_l)
		= \EE(Y_i) \EE(Y_j Y_k Y_l)
		= 0
	\]
	ולכן
	\[
		\EE({(X - \frac{n}{2})}^4)
		= \sum_{i = 1}^{n} \EE(Y_i^4)
		+ \binom{4}{2} \sum_{i = 1}^{n} \sum_{j = 1, j \ne i}^{n} \EE(Y_i^2) \EE(Y_j^2)
		\le K n^2
	\]
	ומאי־שוויון צבישב המוכלל נקבל
	\[
		\PP(|X - \frac{n}{2}| \ge \frac{3n}{4})
		\le o(\frac{1}{n^2})
	\]
	במקום לעבוד עם פולינומים נעבוד עם משתנים מערכיים על־ידי ההגדרה
	\[
		Z = 2^X
	\]
	ולכן גם $Z - 2^X = 2^{\sum_{i = 1}^{n} X_i} = \prod_{i = 1}^n 2^{X_i}$, נגדיר את מכפלות אלה כ־$Z_i$ ואז
	\[
		\PP(X \ge \frac{3n}{4})
		= \PP(Z \ge 2^{\frac{3n}{4}})
		\le \frac{\EE(Z)}{2^{\frac{3n}{4}}}
		= \frac{\prod_{i = 1}^n \EE(Z_i)}{2^{\frac{3n}{4}}}
	\]
	וגם
	\[
		\EE(Z_i) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot 2 = \frac{3}{2}
	\]
	ולכן
	\[
		\EE(Z) = {\left(\frac{3}{2}\right)}^n
	\]
	וכן
	\[
		\frac{\prod_{i = 1}^n \EE(Z_i)}{2^{\frac{3n}{4}}}
		= \frac{\frac{3^n}{2^n}}{2^{\frac{3n}{4}}}
		= {(\frac{\frac{3}{2}}{2^{\frac{3}{4}}})}^n
		\xrightarrow[n \to \infty]{} 0
	\]
\end{example}
\begin{definition}[פונקציה יוצרת מומנטים]
	$X$ משתנה מקרי, אז הפונקציה יוצרת המומנטים שלו היא
	\[
		M_X(t) = \EE(e^{tX})
	\]
	והיא מוגדרת עבור חלק מערכי $t \in \RR$.
\end{definition}
\begin{proposition}[אי־שוויון צ'רנוף]
	נניח $X$ משתנה מקרי ו־$\lambda \in \RR$ אז
	\[
		\PP(X \ge \lambda)
		\le \frac{M_X(t)}{e^{t\lambda}}
	\]
	לכל $t > 0$ עבורו $M_X(t)$ מוגדרת.
\end{proposition}
\begin{proof}
	\[
		\PP(X \ge \lambda)
		= \PP(e^{tX} \ge e^{t\lambda})
		\le \frac{\EE(e^{tX})}{e^{t\lambda}}
	\]
\end{proof}
\begin{proposition}[כפליות פונקציה יוצרת מומנטים]
	אם $X, Y$ בלתי־תלויים אז
	\[
		M_{X + Y}(t) = M_X(t) \cdot M_Y(t)
	\]
	לכל $t$ בתחום ההגדרה של $X, Y$.
\end{proposition}
\begin{proof}
	\[
		\EE(e^{t(X + Y)})
		= \EE(e^{tX} \cdot e^{tY})
		= \EE(e^{tX}) \EE(e^{tY})
	\]
\end{proof}
\begin{example}
	נניח ש־$X \sim Ber(p)$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		= (1 - p) e^{t \cdot 0} + p e^{t \cdot 1}
		= 1 + p(e^t - 1)
	\]
	ועל־ידי הטענה האחרונה אם $X \sim Bin(n, p)$ אז
	\[
		M_X(t)
		= {(1 + p(e^t - 1))}^n
	\]
\end{example}
\begin{example}
	נניח ש־$X \sim Poi(\lambda)$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		= \sum_{k = 0}^{\infty} e^{tk} e^{-\lambda} \frac{\lambda^k}{k!}
		= e^{-\lambda} \sum_{k = 0}^{\infty} \frac{{(\lambda \cdot e^t)}^k}{k!}
		= e^{-\lambda} e^{e^t \lambda}
		= e^{\lambda(e^t - 1)}
	\]
\end{example}
\begin{remark}
	אם ניקח $X_n \sim Bin(n, \frac{\lambda}{n})$ אז $M_{X_n}(t) = {(1 + \frac{\lambda(e^t - 1)}{n})}^n \xrightarrow[n \to \infty]{} e^{\lambda(e^t - 1)} = M_X(t)$.
\end{remark}
\begin{example}
	נניח ש־$X \sim Geo(p)$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		= \sum_{k = 1}^{\infty} e^{tk} {(1 - p)}^{k - 1} p
		= p e^t \sum_{k = 1}^{\infty} {(e^t (1 - p))}^{k - 1}
		= \frac{p e^t}{1 - e^t(1 - p)}
	\]
	וזה מוגדר רק כאשר $e^t(1 - p) < 1$.
\end{example}
\begin{theorem}[אי־שוויון הופדינג]\label{hofding_inequality_theorem}
	יהיו $X_1, \dots, X_n$ משתנים מקריים בלתי־תלויים כך ש־$\EE(X_i) = 0$, ו־$|X_i| \le 1$ כמעט תמיד, אז
	\[
		\PP(\sum_{i = 1}^{n} X_i \ge \lambda)
		\le e^{-\frac{\lambda^2}{2n}}
	\]
\end{theorem}
את ההוכחה נראה בהרצאה הבאה, אבל כן נראה דוגמה
\begin{example}
	מטילים $n = 10^6$ מטבעות הוגנים באופן בלתי־תלוי,
	\[
		0.99 \le \PP(495000 \le X \le 505000)
	\]
	ונמצא חסם נוסף להסתברות זו.
	נגדיר $Y_i = 2(X_i - \frac{1}{2})$ כדי למרכז אותם, ואז
	\[
		\EE(Y_i) = 0,
		|Y_i| \le 1
	\]
	אז אפשר להשתמש באי־שוויון הופדינג על $Y = \sum_{i = 1}^{n} Y_i$.
	\[
		\PP(Y \ge \lambda)
		\le e^{-\frac{\lambda^2}{2n}}
	\]
	רוצים לחסום את $|X - \frac{n}{2}| \ge 5000$ אז
	\[
		Y
		= \sum_{i = 1}^{n} Y_i
		= \sum_{i = 1}^{n} 2X_i - 1
		= 2X - n
		= 2 (X - \frac{n}{2})
	\]
	ולכן במקרה של $Y$ אנו מחפשים את
	\[
		|Y| \ge 10000
	\]
	ועתה נוכל מטעמי סימטריה לבחון רק את המקרה החיובי,
	\[
		\PP(|X - \frac{n}{2}| \ge 5000)
		\le 2 \PP(Y \ge 10000)
		\le e^{\frac{10000^2}{2n}}
		= e^{-50}
	\]
\end{example}

\section{שיעור 19 --- 7.1.2024}
\subsection{פונקציה יוצרת מומנטים --- המשך}
בהרצאה הקודמת ראינו את אי־שוויון הופדינג\ \ref{hofding_inequality_theorem}, אי־שוויון שימושי במיוחד עבור חסמים, עתה נראה את ההוכחה שלו ודוגמות נוספות.
\begin{example}
	אם $X_i \sim U(\{-1, 1\})$ מקיימים את תנאי \ \ref{hofding_inequality_theorem}. \\
	מאי־שוויון צ'בישב נקבל את החסם
	\[
		\PP(| \sum_{i = 1}^{n} X_i | \ge \lambda)
		\le \frac{\var(\sum_{i = 1}^{n} X_i)}{\lambda^2}
		= \frac{n}{\lambda^2}
	\]
	בזמן שמאי־שוויון הופדינג נובע
	\[
		\PP(| \sum_{i = 1}^{n} X_i | \ge \lambda)
		\le 2 \exp(- \frac{\lambda^2}{2n})
	\]
\end{example}
נראה עתה למה שנצטרך להוכחת אי־השוויון.
\begin{lemma}[הלמה של הופדינג]
	אם $X$ משתמה מקרי כך ש־$\EE(X) = 0$ ו־$|X| \le 1$ אז
	\[
		M_X(t)
		= \EE(e^{tX})
		\le e^{\frac{t^2}{2}}
	\]
	לכל $t \in \RR$.
\end{lemma}
\begin{proof}
	יהי $t \in \RR$.
	נגדיר את הפונקציה הקווית $L(x) = \frac{e^t - e^{-t}}{2} x + \frac{e^t + e^{-t}}{2}$ על־ידי שימוש בשיפוע על גרף הפונקציה $e^{tX}$. \\
	לכל $x \in [-1, 1]$ נובע $e^{tx} \le L(x)$.
	עבור $X$ שמקיים $|X| \le 1$ מתקיים $e^{tX} \le L(X)$.
	לכן מתכונות התוחלת נובע $\EE(e^{tX}) \le \EE(L(X))$.
	נותר אם כן לחשב את הערך האחרון,
	\[
		\EE(L(X))
		= \EE(\frac{e^t - e^{-t}}{2} X + \frac{e^t + e^{-t}}{2})
		= \frac{e^t - e^{-t}}{2} \EE(X) + \frac{e^t + e^{-t}}{2}
		= \frac{e^t + e^{-t}}{2}
	\]
	ומצאנו חסם, אך לא האחד שרצינו, נמשיך ונראה כי החסם המבוקש מתקיים אף הוא.
	מפיתוח טיילור נקבל
	\[
		\frac{e^t + e^{-t}}{2}
		= \frac{1}{2} \left(\sum_{k = 0}^{\infty} \frac{t^k}{k!} + \sum_{k = 0}^{\infty} \frac{{(-t)}^k}{k!} \right)
		= \frac{1}{2} \left(\sum_{k = 0}^{\infty} \frac{t^k + {(-t)}^k}{k!}\right)
		= \frac{1}{2} \left(\sum_{l = 0}^{\infty} \frac{t^{2l}}{(2l)!}\right)
	\]
	מהצד השני
	\[
		e^{\frac{t^2}{2}}
		= \sum_{l = 0}^{\infty} \frac{{\left(\frac{t^2}{2}\right)}^l}{l!}
		= \sum_{l = 0}^{\infty} \frac{t^{2l}}{2^l l!}
	\]
	אבל לכל $l \ge 0$ מתקיים
	\[
		2^l l! = 2l (2l - 2) \cdots 2 \le 2 l (2l - 1) \cdots 1 = (2l)!
	\]
	ולכן מצאנו את החסם הרצוי בדיוק.
\end{proof}
נעבור להוכחת אי־השוויון\ \ref{hofding_inequality_theorem}.
\begin{proof}
	לפי הלמה לכל $i$,
	\[
		M_{X_i}(t) \le e^{\frac{t^2}{2}}
	\]
	אנו גם יודעים ש־$X_i$ בלתי־תלויים ולפי כפליות
	\[
		M_{\sum_{i = 1}^{n} X_i}(t)
		= \prod_{i = 1}^n M_{X_i}(t)
		\le {(e^{\frac{t^2}{2}})}^n
		= e^{\frac{nt^2}{2}}
	\]
	ולכן לפי צ'רנוף
	\[
		\PP(\sum_{i = 1}^{n} X_i \ge \lambda)
		\le \frac{M_{\sum_{i = 1}^{n} X_i}(t)}{e^{\lambda t}}
		\le e^{\frac{nt^2}{2} - \lambda t}
	\]
	נמצא את הערך הקן ביותר עבור חסם זה על־ידי מציאת ערך קטן ביותר לביטוי $\frac{n t^2}{2} - \lambda t$, נגזור את הביטוי ונקבל $nt - \lambda$ ולכן נבחר $t = \frac{\lambda}{n}$ ונקבל
	\[
		e^{\frac{n {(\frac{\lambda}{n})}^2}{2} - \lambda \frac{\lambda}{n}}
		= e^{\frac{\lambda^2}{2n} - 2 \frac{1}{2}\frac{\lambda^2}{n}}
		= e^{-\frac{\lambda^2}{2n}}
	\]
	כפי שרצינו להראות.
\end{proof}
\begin{example}
	$X_i \sim U([6])$ עבור $i \in [100]$ ונרצה לחשב את ההסתברות שממוצע מאה ההטלות הוא לפחות 4.
	\[
		\PP(\frac{\sum_{i = 1}^{100} X_i}{100} \ge 4)
	\]
	נמרכז את המשתנים המקריים על־ידי הגדרת $Y_i = \frac{X_i - \frac{7}{2}}{\frac{5}{2}}$ כדי לעמוד בדרישות אי־השוויון\ \ref{hofding_inequality_theorem}. \\
	מההגדרה אכן $\EE(Y_i) = 0$ וגם $|Y_i| \le 1$.
	לכן נובע מאי־שוויון הופדינג
	\[
		\PP(\sum_{i = 1}^{100} Y_i \ge \lambda)
		\le e^{- \frac{\lambda^2}{100}}
	\]
	ואנו רוצים למצוא את ערך $\lambda$ המתאים כדי לתרגם את אי־השוויון הזה לאחד ששואל על $\frac{\sum_{i = 1}^{100} X_i}{100} \ge 4$.
	מהצבה
	\[
		X_i = \frac{5}{2} Y_i + \frac{7}{2}
	\]
	ולכן
	\[
		\frac{\sum_{i = 1}^{100} \frac{5}{2} Y_i + \frac{7}{2}}{100} \ge 4
		\iff
		\frac{\frac{5}{2} \sum_{i = 1}^{100} Y_i}{100} + \frac{7}{2} \ge 4
		\iff
		\frac{5}{2} \sum_{i = 1}^{100} Y_i \ge 50
		\iff
		\sum_{i = 1}^{100} Y_i \ge 20
	\]
	ולכן נקבל
	\[
		\PP(\frac{\sum_{i = 1}^{100} X_i}{100} \ge 4)
		\le e^{- \frac{20^2}{200}}
		= e^{-2}
	\]
\end{example}
לבסוף נעיר הערה על השם פונקציה יוצרת מומנטים, הסיבה שאנו קוראים לה ככה הוא שהנגזרות שלה הם המומנטים, כלומר $\EE(X^2)$ יהיה ערך הנגזרת השנייה של $M_X(t)$ וכן הלאה, כך נראה עתה.
\begin{proposition}
	אם $M_X(t)$ מוגדרת בסביבת $0$ אז היא חלקה בסביבת $0$ ונגזרותיה הן המומנטים של $X$.
\end{proposition}
\begin{proof}[הוכחה במקרה של תומך סופי]
	\[
		M_X(t)
		= \sum_{s \in \supp X} e^{ts} \PP(X = s)
	\]
	ולכן
	\[
		M_X'(t)
		= \sum_{s \in \supp X} s e^{ts} \PP(X = s)
	\]
	ובאופן כללי
	\[
		M_X^{(k)}(t)
		= \sum_{s \in \supp X} s^k e^{ts} \PP(X = s)
	\]
	ולכן
	\[
		M_X^{(k)}(0)
		= \sum_{s \in \supp X} s^k \PP(X = s)
		= \EE(X^k)
	\]
\end{proof}

\subsection{מבוא למרחבי הסתברות רציפים}
ניזכר שהגדרנו $(\Omega, \mathcal{F}, \PP)$ עבור מרחב הסתברות, אבל לא דיברנו על המשמעות של $\mathcal{F}$ כדי להבין מה היכולות האמיתיות של ההגדרה שלנו.
ננסה עתה להגדיר מרחב הסתברות לא בדיד בצורה נאיבית. למען הפשטות נבחן את $\Omega = [0, 1]$.
אנו רוצים ש־$\PP([0, \frac{1}{2}]) = \frac{1}{2}$ וכן ש־$\PP([\frac{1}{2}, 1]) = \frac{1}{2}$ אבל אז נובע ישירות ש־$\PP(\{\frac{1}{2}\}) = 0$.
ככלל נגדיר ש־$\PP([a, b]) = b - a$.
אבל בהגדרה זו אנו נתקלים בפרדוקס בשם בנך־טרסקי.
אפשר לחלק את כדור היחידה ב־$\RR^3$ למספר סופי של חלקים זרים ולהזיז ולסובב את החלקים ולהרכיב מהם שני כדורי יחידה.
כדי לא להיתקל בפרדוקס הזה אנו הולכים להשתמש במידה ולהגביל את $\mathcal{F}$.

\listoftheorems[title=הגדרות ומשפטים,ignoreall,show={theorem,definition},swapnumber,onlynamed={proposition}]
\end{document}
